<!doctype html>
<html lang="en">
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="author" content="Adam Novotny">
    <meta name="description" content="Collection of AI, ML, and data resources I've found useful. Contents Algorithms Bayes Explainability MLOps Model Evaluation Preprocessing...">
    <title>Machine Learning Notes | ANotes</title>
    <link rel="icon" type="image/png" sizes="32x32" href="https://adamnovotny.com/theme/images/favicon.png">
<link rel="canonical" href="https://adamnovotny.com/blog/machine-learning-notes.html">
    <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;lang=en">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">
    <link rel="stylesheet" href="https://adamnovotny.com/theme/css/blog.css">
</head>

<body>
    <nav class="navbar navbar-expand-lg navbar-light">
        <div class="container-fluid">
          <a class="navbar-brand" href="https://adamnovotny.com">
            <img src="https://adamnovotny.com/theme/images/adamnovotny_logo.png" alt="">
          </a>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarSupportedContent" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
          </button>
          <div class="collapse navbar-collapse" id="navbarSupportedContent">
            <ul class="navbar-nav me-auto mb-2 mb-lg-0">
              <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                  Articles
                </a>
                <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <li><a class="dropdown-item" href="https://adamnovotny.com/category/aviation.html">Aviation <i class="fas fa-plane fa-md"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/category/finance.html">Finance <i class="fas fa-chart-line fa-md"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/category/machine-learning.html">Machine Learning <i class="fas fa-laptop fa-md"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com">All articles</a></li>
                </ul>
              </li>
              <li class="nav-item dropdown">
                <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
                  Resources
                </a>
                <ul class="dropdown-menu" aria-labelledby="navbarDropdown">
                  <li><a class="dropdown-item" href="https://adamnovotny.com/pages/bookshelf.html">Bookshelf <i class="fas fa-book fa-sm"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/pages/wishlist.html">Wishlist <i class="fas fa-book fa-sm"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/lucky.html">Lucky <i class="fas fa-quote-left fa-sm"></i></a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/feeds/atom.xml">Feed Atom</a></li>
                  <li><a class="dropdown-item" href="https://adamnovotny.com/feeds/rss.xml">Feed RSS <i class="fas fa-rss-square fa-sm"></i></a></li>
                </ul>
              </li>
              <li>
                <a class="dropdown-item" href="https://github.com/adamnovotnycom">Github <i class="fab fa-github fa-md"></i></a>
              </li>
              <li>
                <a class="dropdown-item" href="https://www.linkedin.com/in/adamnovotnycom">LinkedIn <i class="fab fa-linkedin fa-md"></i></a>
              </li>
            </ul>
          </div>
        </div>
      </nav>
      

<main class="container">
    <div class="row">
        <div class="col"></div>
        <div class="col-md-9">
    
            <div class="blog-post">
            <h2 class="blog-post-title">Machine Learning Notes</h2>
            <p class="blog-post-meta" >
                Published: 2022-05-07
                . <a href="https://medium.com/@adamnovotnycom/machine-learning-notes-8dfcb983ff01/" target="_blank"><i class="fab fa-medium fa-lg"></i></a>
            </p>
            <p>Collection of AI, ML, and data resources I've found useful.</p>
<h4>Contents</h4>
<ul>
<li><a href="#algorithms">Algorithms</a></li>
<li><a href="#bayes">Bayes</a></li>
<li><a href="#explainability">Explainability</a></li>
<li><a href="#mlops">MLOps</a></li>
<li><a href="#model_evaluation">Model Evaluation</a></li>
<li><a href="#preprocessing">Preprocessing</a></li>
<li><a href="#reinforcement_learning">Reinforcement Learning</a></li>
<li><a href="#sql">SQL</a></li>
<li><a href="#statistics">Statistics</a></li>
</ul>
<h4>Algorithms <span id="algorithms"></span></h4>
<ul>
<li>Expectation-maximization (EM): <a href="https://scikit-learn.org/stable/modules/mixture.html#estimation-algorithm-expectation-maximization">algo</a> assumes random components and computes for each point a probability of being generated by each component of the model. Then iteratively tweaks the parameters to maximize the likelihood of the data given those assignments. Example: <a href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture">Gaussian Mixture</a></li>
<li>Gaussian Mixtures: <a href="https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b">anomaly detection example</a>: future examples may look nothing like the past. This is where supervised learning differs because it assumes that future examples fall within the range of the training data</li>
<li><a href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting">Gradient Boosting</a>: optimization of arbitrary differentiable loss functions.</li>
<li>K-means: aims to choose centroids that minimize the inertia, or within-cluster sum-of-squares criterion. Use the <a href="https://www.scikit-yb.org/en/latest/api/cluster/elbow.html">“elbow” method</a> to identify the right number of means. <a href="https://scikit-learn.org/stable/modules/clustering.html#k-means">scikit tutorial</a></li>
<li>KNN: Simple, flexible, naturally handles multiple classes. Slow at scale, sensitive to feature scaling and irrelevant features. <a href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification">scikit tutorial</a></li>
<li>Linear Discriminant Analysis (LDA): A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. <a href="https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers">scikit tutorial</a></li>
<li>Linear regression<ul>
<li>assumptions (LINE) <a href="https://online.stat.psu.edu/stat500/lesson/9/9.2/9.2.3#paragraph--3265">source</a><ul>
<li>Linearity</li>
<li>Independence of errors</li>
<li>Normality of errors</li>
<li>Equal variances</li>
<li>Tests of assumptions: i) plot each feature on x-axis vs y_error, ii) plot y_predicted on x-axis vs y_error, iii) histogram of errors.</li>
</ul>
</li>
<li>Overspecified model can be used for prediction of the label, but should not be used to ascribe the effect of a feature on the label.</li>
<li><a href="http://cecas.clemson.edu/~ahoover/ece854/lecture-notes/lecture-normeqs.pdf">Linear algebra solution</a><a href="/theme/images/1*i0ylsCBDeVY5rFlGa9AYWg.png.png"><img src="/theme/images/1*i0ylsCBDeVY5rFlGa9AYWg.png.png" alt="Normal equation" style="width: 100%" loading="lazy"></a></li>
</ul>
</li>
<li>Naive Bayes: uses naive conditional independence assumption of features. <a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes">scikit</a></li>
<li>PCA: transform data using k vectors that minimize the perpendicular distance to points. PCA can be also thought of as an <a href="https://online.stat.psu.edu/stat505/lesson/11/11.2">eigenvalue/engenvector decomposition</a>. <a href="https://scikit-learn.org/stable/modules/decomposition.html#pca">scikit</a>. <a href="https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf">Intuition paper</a></li>
<li>Pearson’s correlation coefficient**. <a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient">wiki</a>. <a href="/theme/images/1*qtdPV-XQhTYACKS7beLDpg.jpeg.png"><img src="/theme/images/1*qtdPV-XQhTYACKS7beLDpg.jpeg.png" alt="Correlation formula" style="width: 100%" loading="lazy"></a></li>
<li>Random Forests: each tree is built using a sample of rows (with replacement) from training set. Less prone to overfitting. <a href="https://scikit-learn.org/stable/modules/ensemble.html#random-forests">scikit</a></li>
<li>RNN: <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">Karpathy tutorial</a></li>
<li>Sorting <a href="https://lamfo-unb.github.io/2019/04/21/Sorting-algorithms">tutorial</a>. <a href="/theme/images/rO1H18bCodMa.png"><img src="/theme/images/rO1H18bCodMa.png" alt="Ridge Regression" style="width: 100%" loading="lazy"></a></li>
<li>Stochastic gradient descent <a href=" https://realpython.com/gradient-descent-algorithm-python/#basic-gradient-descent-algorithm">tutorial</a>. Calculus solution: <a href="/theme/images/1*_6C1R-IamnPtIo0jLOoblw.png.png"><img src="/theme/images/1*_6C1R-IamnPtIo0jLOoblw.png.png" alt="Stochastic gradient descent cost function" style="width: 100%" loading="lazy"></a></li>
<li>SVD: Singular Value Decomposition <a href="https://towardsdatascience.com/svd-8c2f72e264f"> intuition with PCA use case</a></li>
<li>SVM: Effective in high dimensional spaces (or when number of dimensions &gt; number of examples). SVMs do not directly provide probability estimates. <a href="https://scikit-learn.org/stable/modules/svm.html#svm-classification">scikit</a></li>
<li>Transformers <a href="https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/">tutorial</a><a href="/theme/images/1232021073114943.png"><img src="/theme/images/1232021073114943.png" alt="Original transformer architecture" style="width: 100%" loading="lazy"></a></li>
</ul>
<h4>Bayes <span id="bayes"></span></h4>
<ul>
<li><a href="https://www.nature.com/articles/s43586-020-00001-2">Nature article overview</a></li>
<li><a href="https://towardsdatascience.com/bayesian-a-b-testing-in-pymc3-54dceb87af74">Bayesian A/B Testing in PyMC3</a></li>
<li><a href="https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6">Inference — Intuition and Example (Beta &amp; Binomial)</a></li>
</ul>
<h4>Explainability <span id="explainability"></span></h4>
<ul>
<li>Books: <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning</a></li>
<li>Tutorials: <a href="https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/">twosigma: a brief survey</a></li>
<li>EthicalML tools <a href="https://github.com/EthicalML/awesome-production-machine-learning#explaining-black-box-models-and-datasets">EthicalML github</a></li>
<li>Partial dependence plots (PDP): x-axis = value of a single feature, y-axis = label. <a href="https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence-plots">scikit</a></li>
<li>Individual conditional expectation (ICE): x-axis = value of a single feature, y-axis = label. <a href="https://scikit-learn.org/stable/modules/partial_dependence.html#individual-conditional-expectation-ice-plot">scikit</a></li>
<li>Permutation feature importance: Randomly shuffle features and calculate impact on model metrics such as F1. <a href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance">scikit</a></li>
<li>Global surrogate: train an easily interpretable model (such as liner regression) on the predictions made by a black box model</li>
<li>Local Surrogate: LIME (for Local Interpretable Model-agnostic Explanations). Train individual models to approximate an individual prediction by removing features to learn their impact on the prediction</li>
<li>Shapley Value (SHAP): The contribution of each feature is measured by adding and removing it from all other feature subsets. The Shapley Value for one feature is the weighted sum of all its contributions</li>
</ul>
<h4>MLOps <span id="mlops"></span></h4>
<ul>
<li><a href="https://github.com/bentoml/BentoML">BentoML</a>: open platform that simplifies ML model deployment by saving models in a standard format, defining a web service with pre/post processing, deploying the web service in a container</li>
<li>Data <a href="https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/">a16z</a>
<a href="/theme/images/1*LYBSxf0MPcERPzkEJlk9Cw.png.png"><img src="/theme/images/1*LYBSxf0MPcERPzkEJlk9Cw.png.png" alt="A Unified Data Infra" style="width: 100%" loading="lazy"></a></li>
<li>ML Blueprint <a href="https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/">a16z</a>
<a href="/theme/images/1*MqMX4k5IupAK9T9vKs5h8g.png.png"><img src="/theme/images/1*MqMX4k5IupAK9T9vKs5h8g.png.png" alt="AI and ML Blueprint" style="width: 100%" loading="lazy"></a></li>
<li>Lifecycle <a href="https://aws.amazon.com/blogs/machine-learning/architect-and-build-the-full-machine-learning-lifecycle-with-amazon-sagemaker/">AWS blog</a> <a href="/theme/images/mvaymymdlhxpalecniyphkibwaqhmboz.jpg"><img src="/theme/images/mvaymymdlhxpalecniyphkibwaqhmboz.jpg" alt="ML lifecycle" style="width: 100%" loading="lazy"></a></li>
<li>EthicalML/awesome-production-machine-learning <a href="https://github.com/EthicalML/awesome-production-machine-learning">EthicalML github</a></li>
<li>Pipeline tools <a href="https://github.com/EthicalML/awesome-production-machine-learning#data-pipeline-etl-frameworks">EthicalML/data-pipeline-etl-frameworks</a></li>
<li>MLOps Google <a href="https://cloud.google.com/architecture/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning#mlops_level_2_cicd_pipeline_automation">Google</a>
<a href="/theme/images/20220115104222.png"><img src="/theme/images/20220115104222.png" alt="A Unified Data Infra" style="width: 100%" loading="lazy"></a></li>
</ul>
<h4>Model evaluation <span id="model_evaluation"></span></h4>
<ul>
<li>Classification:<ul>
<li>Recall: <a href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall">wiki</a></li>
<li>Receiver operating characteristic (ROC): relates true positive rate (y-axis) and false positive rate (x-axis). TPR = TP / (TP + FN) and FPR = FP / (FP + TN). <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc">scikit</a></li>
</ul>
</li>
<li>Regression<ul>
<li>R2: strength of a linear relationship. Could be 0 for nonlinear relationships. Never worsens with more features. <a href="https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score">scikit</a></li>
</ul>
</li>
<li>Learning curves <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#plotting-learning-curves">scikit tutorial</a> <a href="/theme/images/1*fz1sqw361u7Y_D1G-aDEmw.png.png"><img src="/theme/images/1*fz1sqw361u7Y_D1G-aDEmw.png.png" alt="Learning Curve example" style="width: 100%" loading="lazy"></a></li>
<li>Overfitting and regularization<ul>
<li>Overfitting (high variance) options: more data, increase regularization, or decrease model complexity. <a href="https://rmartinshort.jimdofree.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/">tutorial</a></li>
<li>Underfitting (high bias) options: decrease regularization, increase model complexity</li>
<li>Lasso regression: linear model regularization technique with tendency to prefer solutions with fewer non-zero coefficients. <a href="https://scikit-learn.org/stable/modules/linear_model.html#lasso">scikit tutorial</a>. <a href="/theme/images/1*bvk1Esh-TGPCIub2ggNzQg.png.png"><img src="/theme/images/1*bvk1Esh-TGPCIub2ggNzQg.png.png" alt="Lasso equation" style="width: 100%" loading="lazy"></a></li>
<li>Ridge regression: imposes a penalty on the size of the coefficients
<a href="/theme/images/1*fekJIBmDHMoU2zQ6wVmQkA.png.png"><img src="/theme/images/1*fekJIBmDHMoU2zQ6wVmQkA.png.png" alt="Ridge Regression" style="width: 100%" loading="lazy"></a><a href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification">scikit</a></li>
<li>Validation curve: <a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#plotting-validation-curves">scikit</a><a href="/theme/images/1*HVM4sFhGDTNE40xr5aVCiQ.png.png"><img src="/theme/images/1*HVM4sFhGDTNE40xr5aVCiQ.png.png" alt="validation curve example" style="width: 100%" loading="lazy"></a></li>
</ul>
</li>
</ul>
<h4>Preprocessing <span id="preprocessing"></span></h4>
<ul>
<li><a href="https://scikit-learn.org/stable/modules/preprocessing.html">scikit</a></li>
<li>Analysis<ol>
<li>Remove duplicates</li>
<li>SOCS of each feature: Shape (skew), Outliers, Center, Spread</li>
<li>Feature correlation</li>
</ol>
</li>
<li>Production pipeline<ol>
<li>Outliers: remove or apply non-linear transformations</li>
<li>Missing values<ul>
<li>SMOTE: Generate and place a new point on the vector between a minority class point and one of its nearest neighbors, located [0, 1] percent of the way from the original point. Algorithm is parameterized with k_neighbors. <a href="https://www.kaggle.com/residentmario/oversampling-with-smote-and-adasyn">tutorial</a></li>
</ul>
</li>
<li>Standardization</li>
<li>Discretization</li>
<li>Encoding categorical features</li>
<li>Generating polynomial features</li>
<li>Dimensionality reduction</li>
</ol>
</li>
</ul>
<h4>Reinforcement Learning <span id="reinforcement_learning"></span></h4>
<ul>
<li><a href="/theme/images/1*TMQs5IMfL3k9OZwy1cck_A.png"><img src="/theme/images/1*TMQs5IMfL3k9OZwy1cck_A.png" alt="Reinforcement learning" style="width: 100%" loading="lazy"></a></li>
</ul>
<h4>SQL <span id="sql"></span></h4>
<ul>
<li>window functions, row_number() and partition(): <a href="https://docs.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql?view=sql-server-ver15#d-using-row_number-with-partition">tutorial</a></li>
<li>COALESCE(): evaluates the arguments in order and returns the current value of the first expression that initially doesn’t evaluate to NULL. <a href="https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql?view=sql-server-ver15">tutorial</a></li>
</ul>
<h4>Statistics <span id="statistics"></span></h4>
<ul>
<li><a href="https://www.statology.org/tutorials/">Statology tutorial</a></li>
<li>Means<ul>
<li>Arithmetic: <a href="https://mathworld.wolfram.com/ArithmeticMean.html">wolfram</a></li>
<li>Geometric: used in finance to calculate average growth rates and is referred to as the compounded annual growth rate. <a href="https://mathworld.wolfram.com/GeometricMean.html">wolfram</a></li>
<li>Harmonic: used in finance to average multiples like the price-earnings ratio because it gives equal weight to each data point. Using a weighted arithmetic mean to average these ratios would give greater weight to high data points than low data points because price-earnings ratios aren't price-normalized while the earnings are equalized. <a href="https://mathworld.wolfram.com/HarmonicMean.html">wolfram</a></li>
</ul>
</li>
<li>Probability distributions <a href="https://www.statology.org/statistics-socs/">Description acronym SOCS</a>: shape, outliers, center, spread. <a href="https://medium.com/@srowen/common-probability-distributions-347e6b945ce4">Comparison article</a>. <a href="/theme/images/Bf8a4LtHWOrJ.png"><img src="/theme/images/Bf8a4LtHWOrJ.png" alt="Correlation formula" style="width: 100%" loading="lazy"></a><ul>
<li>Beta: probability distribution on probabilities bounded [0, 1]. <a href="https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af">tutorial</a></li>
<li>Binomial: probability of obtaining k successes in n binomial experiments with probability p. <a href="https://www.statology.org/binomial-distribution/">tutorial</a></li>
<li>Normal: empirical rule is sometimes called the 68-95-99.7 rule</li>
<li>Poisson: the probability of obtaining k successes during a given time interval. <a href="https://www.statology.org/poisson-distribution/">Statology tutorial</a>. <a href="https://builtin.com/data-science/poisson-process">tutorial 2</a>.<a href="https://timeseriesreasoning.com/contents/zero-inflated-poisson-regression-model/">Zero Inflated Poisson Regression Model</a></li>
</ul>
</li>
<li>Sample variance: divided by n-1 to achieve an unbiased estimator because 1 degree of freedom is used to estimate b0. <a href="https://online.stat.psu.edu/stat500/lesson/1/1.5/1.5.3#paragraph--3051">tutorial</a></li>
<li>Tests <a href="/theme/images/1*ShYx679GlV5WVL8ukd2j2w.png"><img src="/theme/images/1*ShYx679GlV5WVL8ukd2j2w.png" alt="Selecting statistical test. Source: Statistical Rethinking 2. Free Chapter 1" style="width: 100%" loading="lazy"></a><ul>
<li>ANOVA: Analysis of variance compares the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means. <a href="https://www.statology.org/one-way-anova/">Statology tutorial</a></li>
<li>F-statistic: determines whether to reject a full model (F) in favor of a reduced (R) model. Reject full model if F is large — or equivalently if its associated p-value is small. <a href="https://online.stat.psu.edu/stat501/lesson/6/6.2#paragraph--785">tutorial</a><a href="/theme/images/1*7Vz6m3tqtLAvxF_Xqe2JAQ.png.png"><img src="/theme/images/1*7Vz6m3tqtLAvxF_Xqe2JAQ.png.png" alt="F-statistic" style="width: 100%" loading="lazy"></a></li>
<li>Linear regression coefficient CI: <a href="https://online.stat.psu.edu/stat501/node/644">tutorial</a><a href="/theme/images/1*hQ5pabjmSByQSC5O4r_uRw.png.png"><img src="/theme/images/1*hQ5pabjmSByQSC5O4r_uRw.png.png" alt="t-interval for slope parameter beta_1" style="width: 100%" loading="lazy"></a></li>
<li>T-test: <a href="https://online.stat.psu.edu/stat555/node/36/">tutorial</a><a href="/theme/images/1*R1ysZ-ofSr5wXwE0_emXiQ.png.png"><img src="/theme/images/1*R1ysZ-ofSr5wXwE0_emXiQ.png.png" alt="T-test formula" style="width: 100%" loading="lazy"></a></li>
</ul>
</li>
</ul>
<iframe style="width:100%; height: 4000px" src="https://docs.google.com/spreadsheets/d/e/2PACX-1vQdwyyqdYlU6DxxlnTJ07gFbQkwZSArRde6jylWVLBGNVsOc0Z3Gq8FfmEagJRUZOnjgafCd-eA_Fql/pubhtml?gid=634223602&amp;single=true&amp;widget=true&amp;headers=false"></iframe>
            </div>
    
        </div>
        <div class="col"></div>
    </div>
</main>

    
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
    <!-- dev Vue -->
    <!-- <script src="https://unpkg.com/vue@3.2"></script>  -->
    <!-- prod Vue -->
    <script src="https://unpkg.com/vue@3.2/dist/vue.global.prod.js"></script>
    
</body>


</html>