<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>ANotes</title><link href="https://adamnovotny.com/" rel="alternate"></link><link href="https://adamnovotny.com/feeds/atom.xml" rel="self"></link><id>https://adamnovotny.com/</id><updated>2021-11-13T00:00:00-05:00</updated><entry><title>Keras LSTM Forecasting Using Synthetic Data</title><link href="https://adamnovotny.com/blog/lstm-forecast-synthetic-data.html" rel="alternate"></link><published>2021-11-13T00:00:00-05:00</published><updated>2021-11-13T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-11-13:/blog/lstm-forecast-synthetic-data.html</id><summary type="html">&lt;h3&gt;Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#notebook"&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary &lt;span id="summary"&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Keras LSTM can be a powerful tool for forecasting. Below is a simple template notebook showing how to setup a data science forecasting experiment.&lt;/p&gt;
&lt;h4&gt;Dataset&lt;/h4&gt;
&lt;p&gt;A synthetic dataset was generated using a scikit-learn regression generator &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1" target="_blank"&gt;make_friedman1&lt;/a&gt;. The dataset is nonlinear, with noise, and some features are …&lt;/p&gt;</summary><content type="html">&lt;h3&gt;Contents&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#notebook"&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary &lt;span id="summary"&gt;&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Keras LSTM can be a powerful tool for forecasting. Below is a simple template notebook showing how to setup a data science forecasting experiment.&lt;/p&gt;
&lt;h4&gt;Dataset&lt;/h4&gt;
&lt;p&gt;A synthetic dataset was generated using a scikit-learn regression generator &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html#sklearn.datasets.make_friedman1" target="_blank"&gt;make_friedman1&lt;/a&gt;. The dataset is nonlinear, with noise, and some features are manually scaled to make the deep learning task more challenging. Time series dependence is created by making each label a weighted average of the &lt;i&gt;make_friedman1&lt;/i&gt; generated values and previous labels. For details see notebook function &lt;i&gt;generate_data()&lt;/i&gt;.&lt;/p&gt;
&lt;p&gt;The image below shows correlations between the generated features, the y label generated for the same time period, and actual future_label we are trying to forecast. Features x_0 - x_4 are the only informative features as can be verified from the bottom row showing meaningful but not very strong correlations:
&lt;a href="/theme/images/lstm_oin235no.png"&gt;&lt;img src="/theme/images/lstm_oin235no.png" alt="Validation loss" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Model training&lt;/h4&gt;
&lt;p&gt;The model is a simple NN with a single hidden layer defined as 
&lt;i&gt;keras.layers.LSTM(32)&lt;/i&gt;. The generated dataset is split into training, validation, and test sets, each honoring time series nature of the data. Validation set is used to stop training early to prevent overfitting. However, this is not a concern for our synthetic dataset as can be seen from following chart. The validation curve never starts increasing as training epochs continue:
&lt;a href="/theme/images/lstm_synthetic_data_9827345.png"&gt;&lt;img src="/theme/images/lstm_synthetic_data_9827345.png" alt="Validation loss" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Model evalution&lt;/h4&gt;
&lt;p&gt;Comparing predictions and actual labels for the validation set shows strong performance even though there are clear optimizations that can be made near extreme values:
&lt;a href="/theme/images/lstm_synthetic_data_val_q4598.png"&gt;&lt;img src="/theme/images/lstm_synthetic_data_val_q4598.png" alt="Validation loss" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, the validation set was already used during training for early stopping purposes. This is why we set aside a test dataset the model has never seen during training. The test dataset is the only true evaluation of the expected performance of the model and in this case it confirms that the model performs well for the synthetic dataset:
&lt;a href="/theme/images/lstm_synthetic_data_test_234897f.png"&gt;&lt;img src="/theme/images/lstm_synthetic_data_test_234897f.png" alt="Validation loss" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Notebook &lt;span id="notebook"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/blog/lstm-forecast-synthetic-data.html#notebook"&gt;embedded in blog post&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="/notebooks/lstm_synthetic_data.html" target="_blank"&gt;as html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://gist.github.com/adamnovotnycom/36af4c4400a7f970982685472661eba1" target="_blank"&gt;as Github Gist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- notebook.html will be appended here per theme/article.html --&gt;</content><category term="Machine Learning"></category></entry><entry><title>Scikit-learn Pipeline with Feature Engineering</title><link href="https://adamnovotny.com/blog/custom-scikit-learn-pipeline.html" rel="alternate"></link><published>2021-08-30T00:00:00-05:00</published><updated>2021-08-30T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-08-30:/blog/custom-scikit-learn-pipeline.html</id><summary type="html">&lt;h4&gt;Contents&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#notebook"&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="summary"&gt;Summary&lt;/h4&gt;
&lt;p&gt;In general, a machine learning pipeline should have the following characteristics:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*8PUAA9DjMv6CMsPWhbayIQ.png.png"&gt;&lt;img src="/theme/images/1*8PUAA9DjMv6CMsPWhbayIQ.png.png" alt="scikit-learn logo" style="width: 50%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;To ensure data consistency, the pipeline should include every step (such as feature engineering) required to train and score training and testing datasets, and score real time requests. The pipeline does not need to include …&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;h4&gt;Contents&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="#summary"&gt;Summary&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#notebook"&gt;Notebook&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="summary"&gt;Summary&lt;/h4&gt;
&lt;p&gt;In general, a machine learning pipeline should have the following characteristics:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*8PUAA9DjMv6CMsPWhbayIQ.png.png"&gt;&lt;img src="/theme/images/1*8PUAA9DjMv6CMsPWhbayIQ.png.png" alt="scikit-learn logo" style="width: 50%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
    &lt;li&gt;To ensure data consistency, the pipeline should include every step (such as feature engineering) required to train and score training and testing datasets, and score real time requests. The pipeline does not need to include one-off steps such as removing duplicates.&lt;/li&gt;
    &lt;li&gt;Numerical features are transformed using scikit-learn classes. SimpleImputer is used to fill missing values and StandardScaler for scaling.&lt;/li&gt;
    &lt;li&gt;Categorical columns are similarly transformed. OneHotEncoder is applied transforming columns containing categorical values. Importantly, I like to define the categories argument to prevent the &lt;a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank"&gt;Curse of dimensionality&lt;/a&gt; that might occur when too many categories are present.&lt;/li&gt;
    &lt;li&gt;An example custom feature engineering class DailyTrendFeature is included in the pipeline for illustration.&lt;/li&gt;
    &lt;li&gt;The pipeline allows for parallel preprocessing subject to the limits of the computing environment. For example, the preprocessing of categorical and numerical features can take place in parallel because the transformation steps are independent of each other. This is accomplished using scikit-learn's &lt;pre&gt;FeatureUnion(n_jobs=-1, ...)&lt;/pre&gt; class that combines other pipeline steps.&lt;/li&gt;
    &lt;li&gt;&lt;a href="https://gist.github.com/adamnovotnycom/a09294f179d8e483d5411eb5c8c4e00f" target="_blank"&gt;Notebook as Github Gist&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id="notebook"&gt;Notebook&lt;/h4&gt;
&lt;!-- notebook.html will be appended here per theme/article.html --&gt;</content><category term="Machine Learning"></category></entry><entry><title>Global Temperature Forecast Using Prophet and CO2</title><link href="https://adamnovotny.com/blog/berkeley-global-temperature-forecast-prophet.html" rel="alternate"></link><published>2021-05-16T00:00:00-05:00</published><updated>2021-05-16T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-05-16:/blog/berkeley-global-temperature-forecast-prophet.html</id><summary type="html">&lt;p&gt;
In this article I will leverage the global &lt;a href="https://adamnovotny.com/blog/berkeley-earth-global-temperature-data2.html"&gt;temperate dataset I discussed previously&lt;/a&gt; to make a temperature forecast using &lt;a href="https://facebook.github.io/prophet/"&gt;Facebook Prophet&lt;/a&gt; for the next 50 years. Note: the temperature dataset serves ONLY as a vehicle to learn how to do forecasting using Prophet. In general, climate and other complex sciences …&lt;/p&gt;</summary><content type="html">&lt;p&gt;
In this article I will leverage the global &lt;a href="https://adamnovotny.com/blog/berkeley-earth-global-temperature-data2.html"&gt;temperate dataset I discussed previously&lt;/a&gt; to make a temperature forecast using &lt;a href="https://facebook.github.io/prophet/"&gt;Facebook Prophet&lt;/a&gt; for the next 50 years. Note: the temperature dataset serves ONLY as a vehicle to learn how to do forecasting using Prophet. In general, climate and other complex sciences cannot be solved using a simple tool such ash Prophet.&lt;/p&gt;

&lt;p&gt; All code can be found in this &lt;a href="https://gist.github.com/adamnovotnycom/8752aa0732576eac32de4e0b9fbda601"&gt;gist&lt;/a&gt;.
&lt;/p&gt;

&lt;section&gt;
&lt;h4&gt;Data&lt;/h4&gt;
&lt;p&gt;
    To review, the temperature dataset covers monthly data since 1850 including 95% confidence intervals (high CI - blue, low CI - red):
&lt;/p&gt;
&lt;a href="/theme/images/bx08l40tssl2p9wv.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="temperature dataset" src="/theme/images/bx08l40tssl2p9wv.png"&gt;
&lt;/a&gt;
&lt;p&gt;
    In addition, I will use the CO2 emmissions data from &lt;a href="https://ourworldindata.org/grapher/annual-co2-emissions-per-country?tab=chart&amp;time=1924..latest&amp;country=~OWID_WRL"&gt;ourworldindata.org&lt;/a&gt;:
&lt;/p&gt;
&lt;a href="/theme/images/4ob68f4dlgcu1r4z.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="CO2 emissions dataset" src="/theme/images/4ob68f4dlgcu1r4z.png"&gt;
&lt;/a&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;h4&gt;Forecast&lt;/h4&gt;
&lt;p&gt;
I will only highlight here how the Prophet API works (specifically when we want include an additional regressor such as CO2). First, we need to format the training dataset such that the label column is &lt;i&gt;y&lt;/i&gt; and date is &lt;i&gt;ds&lt;/i&gt;
&lt;/p&gt;
&lt;a href="/theme/images/4na1arghpydyjqu6.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="Prophet training dataset" src="/theme/images/4na1arghpydyjqu6.png"&gt;
&lt;/a&gt;
&lt;p&gt;
Next, we train the Prophet model and add the custom regressor (CO2):
&lt;/p&gt;
&lt;pre&gt;
m = Prophet()
m.add_regressor("co2_monthly_bn_tons", 
                prior_scale=0.5, 
                mode="multiplicative",
                standardize=True)
m.fit(prophet_train_set)
&lt;/pre&gt;
&lt;p&gt;
Then we need to create a forecast dataset that includes the dates to be forecasted and assumptions for the custom regressor. In the temperature forecasting dataset, I created timestamps for the next 50 years. Last 3 rows of the forecast dataset ("prophet_forecast_set"):
&lt;/p&gt;
&lt;a href="/theme/images/h12zg3yiwmdk3pa1.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="Prophet forecast dataset" src="/theme/images/h12zg3yiwmdk3pa1.png"&gt;
&lt;/a&gt;
&lt;p&gt;
In order to create the dataset above, I had to make an assumption about CO2 growth. I assumed that monthly growth over the next 50 years will continue at the same pace as it has between 2000-2020:
&lt;/p&gt;
&lt;a href="/theme/images/9p5vaqrkoh3u5bgr.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="CO2 growth assumptions" src="/theme/images/9p5vaqrkoh3u5bgr.png"&gt;
&lt;/a&gt;
&lt;p&gt;
In reality, the value of the temperature forecast comes from the data scientist's background knowledge of the field. In this example, in order for the temperature forecast to be valuable, we have to be able to forecast CO2 emissions (and other regressors) with high confidence.
&lt;/p&gt;
&lt;p&gt;
Performing the actual forecast using Prophet is very simple:
&lt;/p&gt;
&lt;pre&gt;
forecast_prophet = m.predict(prophet_forecast_set)
forecast_prophet.head(5)
&lt;/pre&gt;
&lt;p&gt;
Prophet generates valuable confidence intervals for its forecast. These confidence bars are more valuable than the point forecast itself. In the chart below, the point forecast in 2070 is 16.1C. However, the forecast ranges widely from nearly 17C to 15.2C.
&lt;/p&gt;
&lt;a href="/theme/images/66bdwdd86os7jq45.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="Temperature forecast" src="/theme/images/66bdwdd86os7jq45.png"&gt;
&lt;/a&gt;
&lt;/section&gt;

&lt;section&gt;
&lt;h4&gt;Validation&lt;/h4&gt;
&lt;p&gt;
The step that many people doing forecasts "conveniently" skip is validation. In other words, if we approached the problem the same way in the past, how incorrect would we turn out to be today.
&lt;/p&gt;
&lt;p&gt;
Let's assume that we are standing in 1970, and we apply the exact same methodology as above to forecast the next 50 years (so we are forecasting 1970-2020). What would the forecasting graphs look like compared to the reality we've already experienced? First, our hypothetical CO2 assumption would match reality reasonably nicely:
&lt;/p&gt;
&lt;a href="/theme/images/4npp2fdw5x5cinm3.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="Hypothetical CO2 forecast since 1970" src="/theme/images/4npp2fdw5x5cinm3.png"&gt;
&lt;/a&gt;
&lt;p&gt;
However, our temperature point forecast would underestimate reality. Our forecast is still within confidence intervals because it nearly perfectly aligns with the upper bound. However, the behavior of the forecast doesn't appear to reflect the upward slope we've experienced historically:
&lt;/p&gt;
&lt;a href="/theme/images/b74bpz4kt579ecxe.png"&gt;
    &lt;img style="width: 100%" loading="lazy"  alt="Hypothetical temperature forecast since 1970" src="/theme/images/b74bpz4kt579ecxe.png"&gt;
&lt;/a&gt;
&lt;p&gt;
This is an example of why confidence intervals are more important than point estimates. Also, it reflects how important it is to be intellectually honest when forecasting and performing historical validation. The takeaway here might be that we are missing additional regressors to be able to properly forecast the temperature physical process.
&lt;/p&gt;

&lt;/section&gt;</content><category term="Machine Learning"></category></entry><entry><title>Berkeley Earth Global Temperature Data</title><link href="https://adamnovotny.com/blog/berkeley-earth-global-temperature-data2.html" rel="alternate"></link><published>2021-05-14T00:00:00-05:00</published><updated>2021-05-14T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-05-14:/blog/berkeley-earth-global-temperature-data2.html</id><summary type="html">&lt;p&gt;&lt;a href="http://berkeleyearth.org/data/"&gt;Berkeley Earth&lt;/a&gt; publishes an unique dataset with global temperature measurements. Below is a guide to the download the data and start analyzing it using Python. All code can be found in this &lt;a href="https://gist.github.com/adamnovotnycom/e844fbfdbcc563123cbbfcd96604bb7b"&gt;gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/theme/images/pztgdtmbiuigjqxlyuejjkjmprukqdkbjqvcbdc.png"&gt;&lt;img style="width: 100%" loading="lazy"  alt="Berkeley Earth air temperature measurements above sea ice" src="/theme/images/pztgdtmbiuigjqxlyuejjkjmprukqdkbjqvcbdc.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Download .txt file from &lt;a href="http://berkeleyearth.org/data/"&gt;Berkeley Earth&lt;/a&gt; data website section "Land + Ocean (1850 — Recent)" and read it using …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://berkeleyearth.org/data/"&gt;Berkeley Earth&lt;/a&gt; publishes an unique dataset with global temperature measurements. Below is a guide to the download the data and start analyzing it using Python. All code can be found in this &lt;a href="https://gist.github.com/adamnovotnycom/e844fbfdbcc563123cbbfcd96604bb7b"&gt;gist&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href="/theme/images/pztgdtmbiuigjqxlyuejjkjmprukqdkbjqvcbdc.png"&gt;&lt;img style="width: 100%" loading="lazy"  alt="Berkeley Earth air temperature measurements above sea ice" src="/theme/images/pztgdtmbiuigjqxlyuejjkjmprukqdkbjqvcbdc.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Download .txt file from &lt;a href="http://berkeleyearth.org/data/"&gt;Berkeley Earth&lt;/a&gt; data website section "Land + Ocean (1850 — Recent)" and read it using the following Python command:&lt;/p&gt;

&lt;pre&gt;
colspecs = [(2, 6), (10, 12), (14, 22), (24, 29)]
df = pd.read_fwf(
    "/content/drive/My Drive/Colab Notebooks/berkeley_earth/data/Land_and_Ocean_complete.txt",
    colspecs=colspecs, 
    header=85
)
df.columns = ["year", "month", "anomaly_C", "confidence_95_C"]
df.head(12)
&lt;/pre&gt;

&lt;p&gt;colspecs defines the column indexes so (2, 6) represents year in the source text file.&lt;/p&gt;

&lt;p&gt;&lt;a href="/theme/images/0dpug746gz4r84cl.png"&gt;&lt;img style="width: 100%" loading="lazy"  alt="/theme/images/" src="/theme/images/0dpug746gz4r84cl.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The data documentation explains that &lt;i&gt;anomaly_C&lt;/i&gt; is the recorded temperature anomaly in Celsius relative to estimated Jan 1951-Dec 1980 global mean temperature of 14.108 +/- 0.02. The chart below shows the absolute air temperatures along with 95% uncertainty intervals (in green) recorded during the 2000s.&lt;/p&gt;</content><category term="Random"></category></entry><entry><title>Dynamic HTML with Python, AWS Lambda, and Containers</title><link href="https://adamnovotny.com/blog/dynamic-html-with-python-aws-lambda-and-containers.html" rel="alternate"></link><published>2021-03-27T00:00:00-05:00</published><updated>2021-03-27T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-03-27:/blog/dynamic-html-with-python-aws-lambda-and-containers.html</id><summary type="html">&lt;p&gt;This article is an extension of my previous article describing a similar &lt;a href="https://adamnovotny.com/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html" target="_blank"&gt;deployment process using native AWS Lambda tools&lt;/a&gt;. However, Amazon since started &lt;a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/" target="_blank"&gt;supporting container images&lt;/a&gt; and updated it’s pricing policy to &lt;a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/" target="_blank"&gt;1ms granularity&lt;/a&gt;. Both are major developments improving tooling and making small deployments cost effective.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*WSpeFmskKx0xiwx-WRRJ6A.jpeg.png"&gt;&lt;img src="/theme/images/1*WSpeFmskKx0xiwx-WRRJ6A.jpeg.png" alt="Deploying AWS Lambda using a container" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My &lt;a href="https://adamnovotny.com/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html" target="_blank"&gt;previous&lt;/a&gt; article …&lt;/p&gt;</summary><content type="html">&lt;p&gt;This article is an extension of my previous article describing a similar &lt;a href="https://adamnovotny.com/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html" target="_blank"&gt;deployment process using native AWS Lambda tools&lt;/a&gt;. However, Amazon since started &lt;a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-container-image-support/" target="_blank"&gt;supporting container images&lt;/a&gt; and updated it’s pricing policy to &lt;a href="https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/" target="_blank"&gt;1ms granularity&lt;/a&gt;. Both are major developments improving tooling and making small deployments cost effective.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*WSpeFmskKx0xiwx-WRRJ6A.jpeg.png"&gt;&lt;img src="/theme/images/1*WSpeFmskKx0xiwx-WRRJ6A.jpeg.png" alt="Deploying AWS Lambda using a container" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;My &lt;a href="https://adamnovotny.com/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html" target="_blank"&gt;previous&lt;/a&gt; article focused on the logic of the code and didn’t address how to actually deploy the function because that was well covered by AWS in its many tutorials. Here I explore the new the container deployment options while keeping all business logic untouched. Please review the AWS tutorial on deploying a &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/python-image.html" target="_blank"&gt;generic Python Lambda code using containers&lt;/a&gt; which I leveraged below.&lt;/p&gt;
&lt;h4&gt;1. Dockerfile&lt;/h4&gt;
&lt;pre&gt;FROM public.ecr.aws/lambda/python:3.8
RUN mkdir -p /mnt/app
ADD app.py /mnt/app
ADD index.html /mnt/app
WORKDIR /mnt/app
RUN pip install --upgrade pip
RUN pip install Jinja2==2.11.*
CMD ["/mnt/app/app.handler"]&lt;/pre&gt;
&lt;p&gt;I am using the AWS base image because it is packaged with a very nice mini server that simulates function responses when developing locally. This is extremely useful because we can call the function with 100s of arguments and verify that it behaves as expected before deployed.&lt;/p&gt;
&lt;h4&gt;App code&lt;/h4&gt;
&lt;p&gt;From the Dockerfile, we can see that all application code is contained in two files:&lt;/p&gt;
&lt;p&gt;1) app.py:&lt;/p&gt;
&lt;pre&gt;import os
from jinja2 import Environment, FileSystemLoader&lt;/pre&gt;
&lt;pre&gt;def lambda_handler(event, context):
    env = Environment(loader=FileSystemLoader(os.path.join(os.path.dirname(__file__), "."), encoding="utf8"))
    my_name_from_query = False
    if event["queryStringParameters"] and "my_name" in event["queryStringParameters"]:
        my_name_from_query = event["queryStringParameters"]["my_name"]
    template = env.get_template("index.html")
    html = template.render(
        my_name=my_name_from_query
    )
    return {
        "statusCode": 200,
        "body": html,
        "headers": {
            "Content-Type": "text/html",
        }
    }&lt;/pre&gt;
&lt;p&gt;2) index.html:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*aJfoRhYbqPXONxww3qyYeA.png.png"&gt;&lt;img src="/theme/images/1*aJfoRhYbqPXONxww3qyYeA.png.png" alt="index.html" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;app.py simply parses one argument named “my_name” from the Lambda query string and passes it to the html template as variable named “my_name”. Jinja2 then parses the variable and returns the final template.&lt;/p&gt;
&lt;h4&gt;Calling and testing the app locally&lt;/h4&gt;
&lt;p&gt;Testing the app locally is very simple thanks to the new container packaging. Simply run docker-compose -f docker-compose.yml up, where docker-compose.yml file is defined as:&lt;/p&gt;
&lt;pre&gt;version: '3'
services:
  cont_name:
    container_name: cont_name
    image: cont_name_img
    build:
      context: .
      dockerfile: Dockerfile
    volumes:
      - .:/mnt/app
    ports:
      - "9000:8080"
    stdin_open: true
    tty: true
    restart: always&lt;/pre&gt;
&lt;p&gt;This stands up the function locally on a simple AWS-provided server. We can send requests and monitor responses using Python code such as:&lt;/p&gt;
&lt;pre&gt;import requests
r = requests.get(
    "http://localhost:9000/2015-03-31/functions/function/invocations", 
    data=open("event.json", "rb")
)
print(r.json())&lt;/pre&gt;
&lt;p&gt;where “event.json” is any .json file we wish to send to the lambda function as arguments. In the example case above, we would send something like:&lt;/p&gt;
&lt;pre&gt;{
  "queryStringParameters": {
    "my_name": "Adam"
  }
}&lt;/pre&gt;
&lt;h4&gt;Cost&lt;/h4&gt;
&lt;p&gt;The simple AWS base server returns responses such as the one below. This is where we can see the significant impact of the new 1ms pricing update. The cost of running this example code is about 9ms which is very small considering that we are returning a full html template to browsers. However, previously AWS would charge for the full 100ms because that was the minimum charge defined. Now, this function could cost nearly 90% less!&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*5Xq3l1IxQmCWQxIkSg4wLw.png.png"&gt;&lt;img src="/theme/images/1*5Xq3l1IxQmCWQxIkSg4wLw.png.png" alt="Lambda duration" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category></entry><entry><title>Google Colab and Auto-sklearn with Profiling</title><link href="https://adamnovotny.com/blog/google-colab-and-auto-sklearn-with-profiling.html" rel="alternate"></link><published>2021-03-20T00:00:00-05:00</published><updated>2021-03-20T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2021-03-20:/blog/google-colab-and-auto-sklearn-with-profiling.html</id><summary type="html">&lt;p&gt;This article is a follow up to my previous tutorial on how to &lt;a href="https://adamnovotny.com/blog/google-colab-and-automl-auto-sklearn-setup.html" target="_blank"&gt;setup Google Colab and auto-sklean&lt;/a&gt;. Here, I will go into more detail that shows auto-sklearn performance on an artificially created dataset. The full notebook gist can be found &lt;a href="https://gist.github.com/adamnovotnycom/ffe8e3961fe0207c64a1b9a074883e51" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, I generated a regression dataset using &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" target="_blank"&gt;scikit …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;This article is a follow up to my previous tutorial on how to &lt;a href="https://adamnovotny.com/blog/google-colab-and-automl-auto-sklearn-setup.html" target="_blank"&gt;setup Google Colab and auto-sklean&lt;/a&gt;. Here, I will go into more detail that shows auto-sklearn performance on an artificially created dataset. The full notebook gist can be found &lt;a href="https://gist.github.com/adamnovotnycom/ffe8e3961fe0207c64a1b9a074883e51" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, I generated a regression dataset using &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html" target="_blank"&gt;scikit learn&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;X, y, coeff = make_regression(
    n_samples=1000,
    n_features=100,
    n_informative=5,
    noise=0,
    shuffle=False,
    coef=True
)&lt;/pre&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Nv5JrZA6e7M9-K_gPxsspg.jpeg.png"&gt;&lt;img src="/theme/images/1*Nv5JrZA6e7M9-K_gPxsspg.jpeg.png" alt="Subset of 100 generated features" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This generates a dataset with 100 numerical features where the first 5 features are informative (these are labeled as “feat_0” to “feat_4”). The rest (“feat_5” to “feat_99”) are random noise. We can see this in the scatter matrix above where only the first 5 features show a correlation with the label.&lt;/p&gt;
&lt;p&gt;We know that this is a simple regression problem which could be solved using a linear regression perfectly. However, knowing what to expect helps us to verify the performance of auto-sklearn which trains its ensemble model using the following steps:&lt;/p&gt;
&lt;pre&gt;import autosklearn.regressionautoml = autosklearn.regression.AutoSklearnRegressor(
    time_left_for_this_task=300,
    n_jobs=-1
)
automl.fit(
    X_train_transformed,
    df_train["label"]
)&lt;/pre&gt;
&lt;p&gt;I also created random categorical features which are then one-hot-encoded into a feature set “X_train_transformed“. Running the AutoSklearnRegressor for 5 minutes (time_left_for_this_task=300) produced the following expected results:&lt;/p&gt;
&lt;pre&gt;predictions = automl.predict(X_train_transformed)
r2_score(df_train["label"], predictions)
&gt;&gt; 0.999
predictions = automl.predict(X_test_transformed)
r2_score(df_test["label"], predictions)
&gt;&gt; 0.999&lt;/pre&gt;
&lt;p&gt;A separate pip package &lt;a href="https://github.com/VIDA-NYU/PipelineVis" target="_blank"&gt;PipelineProfiler&lt;/a&gt; helps us visualize the steps auto-sklearn took to achieve the result:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*9ZWW9HeGqTjkan4qtd4mDQ.jpeg.png"&gt;&lt;img src="/theme/images/1*9ZWW9HeGqTjkan4qtd4mDQ.jpeg.png" alt="PipelineProfiler output" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Above we can see the attempts auto-sklearn made to generate the best emsemble of models within the 5 minute constraint I set. The best model found was &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html" target="_blank"&gt;Liblinear SVM&lt;/a&gt;, which produced R2 of nearly 1.0. As a result, this toy ensemble model gives weight of 1.0 to just one algorithm. Libsvm Svr and Gradient boosting scored between 0.9–0.96.&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Machine Learning Notes</title><link href="https://adamnovotny.com/blog/machine-learning-notes.html" rel="alternate"></link><published>2020-12-23T00:00:00-05:00</published><updated>2020-12-23T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-12-23:/blog/machine-learning-notes.html</id><summary type="html">&lt;p&gt;Collection of AI, ML, and data resources I've found useful.&lt;/p&gt;
&lt;h4&gt;Contents&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#algorithms"&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bayes"&gt;Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#explainability"&gt;Explainability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#infrastructure"&gt;Infrastructure / tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model_evaluation"&gt;Model Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preprocessing"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement_learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sql"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Algorithms &lt;span id="algorithms"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/theme/images/1*mobq_8Cwq6J59oN07-aKwA.png.png"&gt;&lt;img src="/theme/images/1*mobq_8Cwq6J59oN07-aKwA.png.png" alt="ML breakdown: Supervised + Unsupervised + RL" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdaBoost: Fits a sequence of weak learners on repeatadly modified data. The modifications are based on errors made by previous learners. &lt;a href="https://scikit-learn.org/stable/modules/ensemble.html#adaboost" target="_blank"&gt;scikit tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Classification: &lt;a href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html" target="_blank"&gt;scikit comparison …&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Collection of AI, ML, and data resources I've found useful.&lt;/p&gt;
&lt;h4&gt;Contents&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="#algorithms"&gt;Algorithms&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#bayes"&gt;Bayes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#explainability"&gt;Explainability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#infrastructure"&gt;Infrastructure / tools&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#model_evaluation"&gt;Model Evaluation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#preprocessing"&gt;Preprocessing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#reinforcement_learning"&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#sql"&gt;SQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="#statistics"&gt;Statistics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Algorithms &lt;span id="algorithms"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/theme/images/1*mobq_8Cwq6J59oN07-aKwA.png.png"&gt;&lt;img src="/theme/images/1*mobq_8Cwq6J59oN07-aKwA.png.png" alt="ML breakdown: Supervised + Unsupervised + RL" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;AdaBoost: Fits a sequence of weak learners on repeatadly modified data. The modifications are based on errors made by previous learners. &lt;a href="https://scikit-learn.org/stable/modules/ensemble.html#adaboost" target="_blank"&gt;scikit tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Classification: &lt;a href="https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html" target="_blank"&gt;scikit comparison&lt;/a&gt;
&lt;a href="/theme/images/1*GkUw2SIWy0Fl2rKd6cIPmg.png.png"&gt;&lt;img src="/theme/images/1*GkUw2SIWy0Fl2rKd6cIPmg.png.png" alt="Classifier comparison: scikit-learn.org" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Expectation-maximization (EM): &lt;a href="https://scikit-learn.org/stable/modules/mixture.html#estimation-algorithm-expectation-maximization" target="_blank"&gt;algo&lt;/a&gt; assumes random components and computes for each point a probability of being generated by each component of the model. Then iteratively tweaks the parameters to maximize the likelihood of the data given those assignments. Example: &lt;a href="https://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture" target="_blank"&gt;Gaussian Mixture&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Gaussian Mixtures: &lt;a href="https://towardsdatascience.com/understanding-anomaly-detection-in-python-using-gaussian-mixture-model-e26e5d06094b" target="_blank"&gt;anomaly detection example&lt;/a&gt;: future examples may look nothing like the past. This is where supervised learning differs because it assumes that future examples fall within the range of the training data&lt;/li&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting" target="_blank"&gt;Gradient Boosting&lt;/a&gt;: optimization of arbitrary differentiable loss functions.&lt;/li&gt;
&lt;li&gt;K-means: aims to choose centroids that minimize the inertia, or within-cluster sum-of-squares criterion. Use the &lt;a href="https://www.scikit-yb.org/en/latest/api/cluster/elbow.html" target="_blank"&gt;“elbow” method&lt;/a&gt; to identify the right number of means. &lt;a href="https://scikit-learn.org/stable/modules/clustering.html#k-means" target="_blank"&gt;scikit tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;KNN: Simple, flexible, naturally handles multiple classes. Slow at scale, sensitive to feature scaling and irrelevant features. &lt;a href="https://scikit-learn.org/stable/modules/neighbors.html#nearest-neighbors-classification" target="_blank"&gt;scikit tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear Discriminant Analysis (LDA): A classifier with a linear decision boundary, generated by fitting class conditional densities to the data and using Bayes’ rule. The model fits a Gaussian density to each class, assuming that all classes share the same covariance matrix. &lt;a href="https://scikit-learn.org/stable/modules/lda_qda.html#mathematical-formulation-of-the-lda-and-qda-classifiers" target="_blank"&gt;scikit tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear regression&lt;ul&gt;
&lt;li&gt;assumptions (LINE) &lt;a href="https://online.stat.psu.edu/stat500/lesson/9/9.2/9.2.3#paragraph--3265" target="_blank"&gt;source&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Linearity&lt;/li&gt;
&lt;li&gt;Independence of errors&lt;/li&gt;
&lt;li&gt;Normality of errors&lt;/li&gt;
&lt;li&gt;Equal variances&lt;/li&gt;
&lt;li&gt;Tests of assumptions: i) plot each feature on x-axis vs y_error, ii) plot y_predicted on x-axis vs y_error, iii) histogram of errors.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Overspecified model can be used for prediction of the label, but should not be used to ascribe the effect of a feature on the label.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://cecas.clemson.edu/~ahoover/ece854/lecture-notes/lecture-normeqs.pdf" target="_blank"&gt;Linear algebra solution&lt;/a&gt;&lt;a href="/theme/images/1*i0ylsCBDeVY5rFlGa9AYWg.png.png"&gt;&lt;img src="/theme/images/1*i0ylsCBDeVY5rFlGa9AYWg.png.png" alt="Normal equation" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Naive Bayes: uses naive conditional independence assumption of features. &lt;a href="https://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;PCA: transform data using k vectors that minimize the perpendicular distance to points. PCA can be also thought of as an &lt;a href="https://online.stat.psu.edu/stat505/lesson/11/11.2" target="_blank"&gt;eigenvalue/engenvector decomposition&lt;/a&gt;. &lt;a href="https://scikit-learn.org/stable/modules/decomposition.html#pca" target="_blank"&gt;scikit&lt;/a&gt;. &lt;a href="https://www.cs.princeton.edu/picasso/mats/PCA-Tutorial-Intuition_jp.pdf" target="_blank"&gt;Intuition paper&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pearson’s correlation coefficient**. &lt;a href="https://en.wikipedia.org/wiki/Pearson_correlation_coefficient" target="_blank"&gt;wiki&lt;/a&gt;. &lt;a href="/theme/images/1*qtdPV-XQhTYACKS7beLDpg.jpeg.png"&gt;&lt;img src="/theme/images/1*qtdPV-XQhTYACKS7beLDpg.jpeg.png" alt="Correlation formula" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Random Forests: each tree is built using a sample of rows (with replacement) from training set. Less prone to overfitting. &lt;a href="https://scikit-learn.org/stable/modules/ensemble.html#random-forests" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Sorting &lt;a href="https://lamfo-unb.github.io/2019/04/21/Sorting-algorithms" target="_blank"&gt;tutorial&lt;/a&gt;. &lt;a href="/theme/images/rO1H18bCodMa.png"&gt;&lt;img src="/theme/images/rO1H18bCodMa.png" alt="Ridge Regression" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stochastic gradient descent &lt;a href=" https://realpython.com/gradient-descent-algorithm-python/#basic-gradient-descent-algorithm" target="_blank"&gt;tutorial&lt;/a&gt;. Calculus solution: &lt;a href="/theme/images/1*_6C1R-IamnPtIo0jLOoblw.png.png"&gt;&lt;img src="/theme/images/1*_6C1R-IamnPtIo0jLOoblw.png.png" alt="Stochastic gradient descent cost function" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SVD: Singular Value Decomposition &lt;a href="https://towardsdatascience.com/svd-8c2f72e264f" target="_blank"&gt; intuition with PCA use case&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SVM: Effective in high dimensional spaces (or when number of dimensions &amp;gt; number of examples). SVMs do not directly provide probability estimates. &lt;a href="https://scikit-learn.org/stable/modules/svm.html#svm-classification" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Transformers &lt;a href="https://www.machinecurve.com/index.php/2020/12/28/introduction-to-transformers-in-machine-learning/" target="_blank"&gt;tutorial&lt;/a&gt;&lt;a href="/theme/images/1232021073114943.png"&gt;&lt;img src="/theme/images/1232021073114943.png" alt="Original transformer architecture" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Bayes &lt;span id="bayes"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.nature.com/articles/s43586-020-00001-2" target="_blank"&gt;Nature article overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/bayesian-a-b-testing-in-pymc3-54dceb87af74" target="_blank"&gt;Bayesian A/B Testing in PyMC3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://towardsdatascience.com/bayesian-inference-intuition-and-example-148fd8fb95d6" target="_blank"&gt;Inference — Intuition and Example (Beta &amp;amp; Binomial)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Explainability &lt;span id="explainability"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Books: &lt;a href="https://christophm.github.io/interpretable-ml-book/" target="_blank"&gt;Interpretable Machine Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tutorials: &lt;a href="https://www.twosigma.com/articles/interpretability-methods-in-machine-learning-a-brief-survey/" target="_blank"&gt;twosigma: a brief survey&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EthicalML tools &lt;a href="https://github.com/EthicalML/awesome-production-machine-learning#explaining-black-box-models-and-datasets" target="_blank"&gt;EthicalML github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Partial dependence plots (PDP): x-axis = value of a single feature, y-axis = label. &lt;a href="https://scikit-learn.org/stable/modules/partial_dependence.html#partial-dependence-plots" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Individual conditional expectation (ICE): x-axis = value of a single feature, y-axis = label. &lt;a href="https://scikit-learn.org/stable/modules/partial_dependence.html#individual-conditional-expectation-ice-plot" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Permutation feature importance: Randomly shuffle features and calculate impact on model metrics such as F1. &lt;a href="https://scikit-learn.org/stable/modules/permutation_importance.html#permutation-feature-importance" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Global surrogate: train an easily interpretable model (such as liner regression) on the predictions made by a black box model&lt;/li&gt;
&lt;li&gt;Local Surrogate: LIME (for Local Interpretable Model-agnostic Explanations). Train individual models to approximate an individual prediction by removing features to learn their impact on the prediction&lt;/li&gt;
&lt;li&gt;Shapley Value (SHAP): The contribution of each feature is measured by adding and removing it from all other feature subsets. The Shapley Value for one feature is the weighted sum of all its contributions&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Infrastructure / tools &lt;span id="infrastructure"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Data &lt;a href="https://a16z.com/2020/10/15/the-emerging-architectures-for-modern-data-infrastructure/" target="_blank"&gt;(source a16z)&lt;/a&gt;
&lt;a href="/theme/images/1*LYBSxf0MPcERPzkEJlk9Cw.png.png"&gt;&lt;img src="/theme/images/1*LYBSxf0MPcERPzkEJlk9Cw.png.png" alt="A Unified Data Infra" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;ML Blueprint&lt;a href="/theme/images/1*MqMX4k5IupAK9T9vKs5h8g.png.png"&gt;&lt;img src="/theme/images/1*MqMX4k5IupAK9T9vKs5h8g.png.png" alt="AI and ML Blueprint" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Lifecycle &lt;a href="https://aws.amazon.com/blogs/machine-learning/architect-and-build-the-full-machine-learning-lifecycle-with-amazon-sagemaker/" target="_blank"&gt;AWS blog&lt;/a&gt; &lt;a href="/theme/images/mvaymymdlhxpalecniyphkibwaqhmboz.jpg"&gt;&lt;img src="/theme/images/mvaymymdlhxpalecniyphkibwaqhmboz.jpg" alt="ML lifecycle" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;EthicalML/awesome-production-machine-learning &lt;a href="https://github.com/EthicalML/awesome-production-machine-learning" target="_blank"&gt;EthicalML github&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Pipeline tools &lt;a href="https://github.com/EthicalML/awesome-production-machine-learning#data-pipeline-etl-frameworks" target="_blank"&gt;EthicalML/data-pipeline-etl-frameworks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Model evaluation &lt;span id="model_evaluation"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Classification:&lt;ul&gt;
&lt;li&gt;Recall: &lt;a href="https://en.wikipedia.org/wiki/Precision_and_recall#Recall" target="_blank"&gt;wiki&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Receiver operating characteristic (ROC): relates true positive rate (y-axis) and false positive rate (x-axis). TPR = TP / (TP + FN) and FPR = FP / (FP + TN). &lt;a href="https://scikit-learn.org/stable/modules/model_evaluation.html#receiver-operating-characteristic-roc" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Regression&lt;ul&gt;
&lt;li&gt;R2: strength of a linear relationship. Could be 0 for nonlinear relationships. Never worsens with more features. &lt;a href="https://scikit-learn.org/stable/modules/model_evaluation.html#r2-score" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Learning curves &lt;a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html#plotting-learning-curves" target="_blank"&gt;scikit tutorial&lt;/a&gt; &lt;a href="/theme/images/1*fz1sqw361u7Y_D1G-aDEmw.png.png"&gt;&lt;img src="/theme/images/1*fz1sqw361u7Y_D1G-aDEmw.png.png" alt="Learning Curve example" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Overfitting and regularization&lt;ul&gt;
&lt;li&gt;Overfitting (high variance) options: more data, increase regularization, or decrease model complexity. &lt;a href="https://rmartinshort.jimdofree.com/2019/02/17/overfitting-bias-variance-and-leaning-curves/" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Underfitting (high bias) options: decrease regularization, increase model complexity&lt;/li&gt;
&lt;li&gt;Lasso regression: linear model regularization technique with tendency to prefer solutions with fewer non-zero coefficients. &lt;a href="https://scikit-learn.org/stable/modules/linear_model.html#lasso" target="_blank"&gt;scikit tutorial&lt;/a&gt;. &lt;a href="/theme/images/1*bvk1Esh-TGPCIub2ggNzQg.png.png"&gt;&lt;img src="/theme/images/1*bvk1Esh-TGPCIub2ggNzQg.png.png" alt="Lasso equation" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Ridge regression: imposes a penalty on the size of the coefficients
&lt;a href="/theme/images/1*fekJIBmDHMoU2zQ6wVmQkA.png.png"&gt;&lt;img src="/theme/images/1*fekJIBmDHMoU2zQ6wVmQkA.png.png" alt="Ridge Regression" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;a href="https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression-and-classification" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Validation curve: &lt;a href="https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html#plotting-validation-curves" target="_blank"&gt;scikit&lt;/a&gt;&lt;a href="/theme/images/1*HVM4sFhGDTNE40xr5aVCiQ.png.png"&gt;&lt;img src="/theme/images/1*HVM4sFhGDTNE40xr5aVCiQ.png.png" alt="validation curve example" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Preprocessing &lt;span id="preprocessing"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://scikit-learn.org/stable/modules/preprocessing.html" target="_blank"&gt;scikit&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Analysis&lt;ol&gt;
&lt;li&gt;Remove duplicates&lt;/li&gt;
&lt;li&gt;SOCS of each feature: Shape (skew), Outliers, Center, Spread&lt;/li&gt;
&lt;li&gt;Feature correlation&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Production pipeline&lt;ol&gt;
&lt;li&gt;Outliers: remove or apply non-linear transformations&lt;/li&gt;
&lt;li&gt;Missing values&lt;ul&gt;
&lt;li&gt;SMOTE: Generate and place a new point on the vector between a minority class point and one of its nearest neighbors, located [0, 1] percent of the way from the original point. Algorithm is parameterized with k_neighbors. &lt;a href="https://www.kaggle.com/residentmario/oversampling-with-smote-and-adasyn" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Standardization&lt;/li&gt;
&lt;li&gt;Discretization&lt;/li&gt;
&lt;li&gt;Encoding categorical features&lt;/li&gt;
&lt;li&gt;Generating polynomial features&lt;/li&gt;
&lt;li&gt;Dimensionality reduction&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Reinforcement Learning &lt;span id="reinforcement_learning"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="/theme/images/1*TMQs5IMfL3k9OZwy1cck_A.png"&gt;&lt;img src="/theme/images/1*TMQs5IMfL3k9OZwy1cck_A.png" alt="Reinforcement learning" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;SQL &lt;span id="sql"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;window functions, row_number() and partition(): &lt;a href="https://docs.microsoft.com/en-us/sql/t-sql/functions/row-number-transact-sql?view=sql-server-ver15#d-using-row_number-with-partition" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;COALESCE(): evaluates the arguments in order and returns the current value of the first expression that initially doesn’t evaluate to NULL. &lt;a href="https://docs.microsoft.com/en-us/sql/t-sql/language-elements/coalesce-transact-sql?view=sql-server-ver15" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Statistics &lt;span id="statistics"&gt;&lt;/span&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.statology.org/tutorials/" target="_blank"&gt;Statology tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Means&lt;ul&gt;
&lt;li&gt;Arithmetic: &lt;a href="https://mathworld.wolfram.com/ArithmeticMean.html" target="_blank"&gt;wolfram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Geometric: used in finance to calculate average growth rates and is referred to as the compounded annual growth rate. &lt;a href="https://mathworld.wolfram.com/GeometricMean.html" target="_blank"&gt;wolfram&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Harmonic: used in finance to average multiples like the price-earnings ratio because it gives equal weight to each data point. Using a weighted arithmetic mean to average these ratios would give greater weight to high data points than low data points because price-earnings ratios aren't price-normalized while the earnings are equalized. &lt;a href="https://mathworld.wolfram.com/HarmonicMean.html" target="_blank"&gt;wolfram&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Probability distributions &lt;a href="https://www.statology.org/statistics-socs/" target="_blank"&gt;Description acronym SOCS&lt;/a&gt;: shape, outliers, center, spread. &lt;a href="https://medium.com/@srowen/common-probability-distributions-347e6b945ce4" target="_blank"&gt;Comparison article&lt;/a&gt;. &lt;a href="/theme/images/Bf8a4LtHWOrJ.png"&gt;&lt;img src="/theme/images/Bf8a4LtHWOrJ.png" alt="Correlation formula" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;Beta: probability distribution on probabilities bounded [0, 1]. &lt;a href="https://towardsdatascience.com/beta-distribution-intuition-examples-and-derivation-cf00f4db57af" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Binomial: probability of obtaining k successes in n binomial experiments with probability p. &lt;a href="https://www.statology.org/binomial-distribution/" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Normal: empirical rule is sometimes called the 68-95-99.7 rule&lt;/li&gt;
&lt;li&gt;Poisson: the probability of obtaining k successes during a given time interval. &lt;a href="https://www.statology.org/poisson-distribution/" target="_blank"&gt;tutorial&lt;/a&gt;. &lt;a href="https://timeseriesreasoning.com/contents/zero-inflated-poisson-regression-model/" target="_blank"&gt;Zero Inflated Poisson Regression Model&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Sample variance: divided by n-1 to achieve an unbiased estimator because 1 degree of freedom is used to estimate b0. &lt;a href="https://online.stat.psu.edu/stat500/lesson/1/1.5/1.5.3#paragraph--3051" target="_blank"&gt;tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Tests &lt;a href="/theme/images/1*ShYx679GlV5WVL8ukd2j2w.png"&gt;&lt;img src="/theme/images/1*ShYx679GlV5WVL8ukd2j2w.png" alt="Selecting statistical test. Source: Statistical Rethinking 2. Free Chapter 1" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;ANOVA: Analysis of variance compares the means of three or more independent groups to determine if there is a statistically significant difference between the corresponding population means. &lt;a href="https://www.statology.org/one-way-anova/" target="_blank"&gt;Statology tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;F-statistic: determines whether to reject a full model (F) in favor of a reduced (R) model. Reject full model if F is large — or equivalently if its associated p-value is small. &lt;a href="https://online.stat.psu.edu/stat501/lesson/6/6.2#paragraph--785" target="_blank"&gt;tutorial&lt;/a&gt;&lt;a href="/theme/images/1*7Vz6m3tqtLAvxF_Xqe2JAQ.png.png"&gt;&lt;img src="/theme/images/1*7Vz6m3tqtLAvxF_Xqe2JAQ.png.png" alt="F-statistic" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Linear regression coefficient CI: &lt;a href="https://online.stat.psu.edu/stat501/node/644" target="_blank"&gt;tutorial&lt;/a&gt;&lt;a href="/theme/images/1*hQ5pabjmSByQSC5O4r_uRw.png.png"&gt;&lt;img src="/theme/images/1*hQ5pabjmSByQSC5O4r_uRw.png.png" alt="t-interval for slope parameter beta_1" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;T-test: &lt;a href="https://online.stat.psu.edu/stat555/node/36/" target="_blank"&gt;tutorial&lt;/a&gt;&lt;a href="/theme/images/1*R1ysZ-ofSr5wXwE0_emXiQ.png.png"&gt;&lt;img src="/theme/images/1*R1ysZ-ofSr5wXwE0_emXiQ.png.png" alt="T-test formula" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;</content><category term="Machine Learning"></category></entry><entry><title>Google Colab and AutoML: Auto-sklearn Setup</title><link href="https://adamnovotny.com/blog/google-colab-and-automl-auto-sklearn-setup.html" rel="alternate"></link><published>2020-12-04T00:00:00-05:00</published><updated>2020-12-04T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-12-04:/blog/google-colab-and-automl-auto-sklearn-setup.html</id><summary type="html">&lt;p&gt;Auto ML is fast becoming a popular solution to build minimal viable models for new projects. A popular library for Python is &lt;a href="https://automl.github.io/auto-sklearn/master/#" target="_blank"&gt;Auto-sklearn&lt;/a&gt; that leverages the most popular Python ML library &lt;a href="http://sklearn.org" target="_blank"&gt;scikit-learn&lt;/a&gt;. Auto-sklearn runs a smart search over scikit-learn models and parameters to find the best performing ensemble of models …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Auto ML is fast becoming a popular solution to build minimal viable models for new projects. A popular library for Python is &lt;a href="https://automl.github.io/auto-sklearn/master/#" target="_blank"&gt;Auto-sklearn&lt;/a&gt; that leverages the most popular Python ML library &lt;a href="http://sklearn.org" target="_blank"&gt;scikit-learn&lt;/a&gt;. Auto-sklearn runs a smart search over scikit-learn models and parameters to find the best performing ensemble of models.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*n6-MAHisW5-xLrEUndHe5g.png.png"&gt;&lt;img src="/theme/images/1*n6-MAHisW5-xLrEUndHe5g.png.png" alt="Logos of Google Drive + Colab + Scikit-learn + Auto-sklearn" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This tutorial describes how to setup Auto-sklearn on &lt;a href="https://colab.research.google.com/" target="_blank"&gt;Google Colab&lt;/a&gt;. The complete &lt;a href="https://gist.github.com/adamnovotnycom/1df7ef10649d8241c389c96becb7fe37" target="_blank"&gt;notebook gist&lt;/a&gt; includes a toy project that uses an &lt;a href="https://www.kaggle.com/dgomonov/new-york-city-airbnb-open-data" target="_blank"&gt;old Airbnb dataset&lt;/a&gt; from Kaggle.&lt;/p&gt;
&lt;p&gt;The key first step is to install linux dependencies alongside Auto-sklearn:&lt;/p&gt;
&lt;pre&gt;!sudo apt-get install build-essential swig
!pip install auto-sklearn==0.11.1&lt;/pre&gt;
&lt;p&gt;After running these commands in Colab, restart the Colab runtime and run all commands again.&lt;/p&gt;
&lt;p&gt;The Airbnb dataset can be used for a regression project where price is the label. I selected a few numerical and categorical features randomly so the dataset used for modeling has the following characteristics:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*-lXTkg7Y9W-XMdNPR5KPkA.png.png"&gt;&lt;img src="/theme/images/1*-lXTkg7Y9W-XMdNPR5KPkA.png.png" alt="Airbnb dataset description" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;A more sophisticated ML project would require a detailed feature selection process and data analysis at this stage. For example, does the maximum value of 1,250 for minimum_nights make sense? In this case, I am simply showing the Auto-sklearn setup so I will skip these time consuming steps.&lt;/p&gt;
&lt;p&gt;Next, all numerical features are &lt;a href="https://en.wikipedia.org/wiki/Standard_score" target="_blank"&gt;standardized&lt;/a&gt; and missing values filled. Scikit-learn (and therefore Auto-sklearn) cannot handle string categories so categorical features are &lt;a href="https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/" target="_blank"&gt;one hot encoded&lt;/a&gt;. Also, infrequently appearing categories are combined into a single bucket to combat the &lt;a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality" target="_blank"&gt;Curse of dimensionality&lt;/a&gt;. In this case, any neighborhood that appears less than 0.5% of the time is renamed to “neighborhood_other”. Before transformations, the first 5 rows of the training dataset have the following items:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*5zbUTS8k6rTqTYUtpATzlw.png.png"&gt;&lt;img src="/theme/images/1*5zbUTS8k6rTqTYUtpATzlw.png.png" alt="Training dataset before transformations" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;After transformations, the first few columns of the 5 rows look like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*AySz4rydwvMNfOnt4v-UpA.png.png"&gt;&lt;img src="/theme/images/1*AySz4rydwvMNfOnt4v-UpA.png.png" alt="Training dataset after transformations" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I am finally ready to explore Auto-sklearn using few simple commands that fit a new model:&lt;/p&gt;
&lt;pre&gt;import autosklearn.regression
automl = autosklearn.regression.AutoSklearnRegressor(
  time_left_for_this_task=120,
  per_run_time_limit=30,
  n_jobs=1
)
automl.fit(
  X_train_transformed,
  y_train
)&lt;/pre&gt;
&lt;p&gt;Finally, here is how the model performs on a test dataset:&lt;/p&gt;
&lt;pre&gt;import sklearn.metrics
predictions = automl.predict(X_test_transformed)
sklearn.metrics.r2_score(y_test, predictions)
 output: 0.1862&lt;/pre&gt;
&lt;p&gt;An alternative approach that doesn’t use Auto-sklearn would be to manually select a model and run a grid search to find best parameters. A typical, well-performing algorithm is RandomForestRegressor so I might try the following:&lt;/p&gt;
&lt;pre&gt;from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV
model = RandomForestRegressor(max_depth=3, random_state=0)
parameters = {
  "max_depth": (2, 3, 5)
}
grid = GridSearchCV(model, parameters, cv=5, scoring="r2")
grid.fit(X_train_transformed, y_train.values.ravel())&lt;/pre&gt;
&lt;p&gt;For comparison, the performance of this model would be:&lt;/p&gt;
&lt;pre&gt;predictions = grid.predict(X_test_transformed)
sklearn.metrics.r2_score(y_test, predictions)
 output: 0.0982&lt;/pre&gt;
&lt;p&gt;Impressively, the default Auto-sklearn &lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank"&gt;R2&lt;/a&gt; performance of 0.186 is nearly twice as good as simplistic scikit-learn-only performance of 0.098. These are not intended to be absolute benchmarks because I performed no customization but the relative performance is worth noting. The results suggest that Auto-sklearn can set a very reasonable lower performance bound that no model deployed in production should underperform.&lt;/p&gt;
&lt;p&gt;More about me: &lt;a href="https://adamnovotny.com" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Google Paper: 24/7 by 2030</title><link href="https://adamnovotny.com/blog/google-paper-24-7-by-2030.html" rel="alternate"></link><published>2020-10-31T00:00:00-05:00</published><updated>2020-10-31T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-10-31:/blog/google-paper-24-7-by-2030.html</id><summary type="html">&lt;p&gt;Google released a &lt;a href="https://www.gstatic.com/gumdrop/sustainability/247-carbon-free-energy.pdf" target="_blank"&gt;white paper&lt;/a&gt; describing how the company intends to generate all of its electricity needs from renewable energy sources by 2030. Previously, Google committed to reducing emissions by buying offsets or generating renewable energy off-cycle. This new commitment goes by further: “Google intends to match its operational electricity …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Google released a &lt;a href="https://www.gstatic.com/gumdrop/sustainability/247-carbon-free-energy.pdf" target="_blank"&gt;white paper&lt;/a&gt; describing how the company intends to generate all of its electricity needs from renewable energy sources by 2030. Previously, Google committed to reducing emissions by buying offsets or generating renewable energy off-cycle. This new commitment goes by further: “Google intends to match its operational electricity use with nearby carbon-free energy sources in every hour of every year”&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*HOTcFQxbxwukF2iOdh3lkw.png.png"&gt;&lt;img src="/theme/images/1*HOTcFQxbxwukF2iOdh3lkw.png.png" alt="Google’s energy journey" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Everybody interested should read it — it’s short.&lt;/p&gt;
&lt;p&gt;Google cooperated with a &lt;a href="https://www.watttime.org/" target="_blank"&gt;Watttime&lt;/a&gt; to generate the dataset that measures the carbon emissions intensity in regions where Google’s data centers are located. Watttime has a very interesting &lt;a href="https://www.watttime.org/api-documentation/#introduction" target="_blank"&gt;API&lt;/a&gt; providing carbon intensity in real time. I collected a random set of data points over 24 hours for a selected number of regions where Google data centers are &lt;a href="https://www.google.com/about/datacenters/locations/" target="_blank"&gt;located&lt;/a&gt; in the US. All code is available in this &lt;a href="https://gist.github.com/excitedAtom/0d980a908a35732e9e55e8b1e8f27985" target="_blank"&gt;Github gist&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*U11j-6mlV0pwUsNslceXaQ.png.png"&gt;&lt;img src="/theme/images/1*U11j-6mlV0pwUsNslceXaQ.png.png" alt="Marginal Operating Emissions Rates (MOER) of Select Google Data Centers" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Marginal Operating Emissions Rates (MOER): 0 represents no emissions (clean energy generation), 100 represents highest emissions. In other words, for all regions where MOER is high, Google has a lot of work to do to replace (or store) electricity used from clean sources. In the chart above, &lt;a href="https://www.google.com/about/datacenters/locations/midlothian/" target="_blank"&gt;Midlothian, TX&lt;/a&gt; appears to be one of those challenging locations. On the other hand, Mayes County, OK is a location that Google appears to be satisfied with: “Our highest clean energy percentage is in Oklahoma (Southwest Power Pool), where our purchases of wind power helped drive carbon-free energy performance at our data center from 41% to 96%”&lt;/p&gt;
&lt;p&gt;I firmly believe that renewable energy will be widely adopted only if it is at least as cheap as alternatives. So exploring the data from a financial perspective is critical: From 2009 to 2019, costs for wind and solar power declined by 70% and 89% (&lt;a href="https://www.lazard.com/media/451086/lazards-levelized-cost-of-energy-version-130-vf.pdf" target="_blank"&gt;see page 8&lt;/a&gt;).&lt;/p&gt;</content><category term="Random"></category></entry><entry><title>Serving Dynamic Web Pages using Python and AWS Lambda</title><link href="https://adamnovotny.com/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html" rel="alternate"></link><published>2020-07-25T00:00:00-05:00</published><updated>2020-07-25T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-07-25:/blog/serving-dynamic-web-pages-using-python-and-aws-lambda.html</id><summary type="html">&lt;p&gt;While AWS Lambda functions are typically used to build API endpoints, at their core Lambda functions can return almost anything. This includes returning html markup with dynamic content.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*9bPdHLV7ghV1RuNYOGkTvA.png.png"&gt;&lt;img src="/theme/images/1*9bPdHLV7ghV1RuNYOGkTvA.png.png" alt="AWS Lambda + Python + Jinja" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will not go into details describing how to deploy AWS Lambda functions. Please see the official &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html" target="_blank"&gt;documentation&lt;/a&gt;. I will however describe …&lt;/p&gt;</summary><content type="html">&lt;p&gt;While AWS Lambda functions are typically used to build API endpoints, at their core Lambda functions can return almost anything. This includes returning html markup with dynamic content.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*9bPdHLV7ghV1RuNYOGkTvA.png.png"&gt;&lt;img src="/theme/images/1*9bPdHLV7ghV1RuNYOGkTvA.png.png" alt="AWS Lambda + Python + Jinja" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will not go into details describing how to deploy AWS Lambda functions. Please see the official &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html" target="_blank"&gt;documentation&lt;/a&gt;. I will however describe how to return dynamic html content instead of a typical &lt;a href="https://en.wikipedia.org/wiki/JSON" target="_blank"&gt;JSON&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Step 0 — Optional&lt;/h4&gt;
&lt;p&gt;If you prefer to develop and test lambda functions locally (as I do), you can use Docker to simulate the AWS lambda function environment. A sample Dockerfile I use is below.&lt;/p&gt;
&lt;pre&gt;FROM amazonlinux:latest
RUN mkdir -p /mnt/app
ADD . /mnt/app
WORKDIR /mnt/app
RUN yum update -y
RUN yum install gcc -y
RUN yum install gcc-c++ -y
RUN yum install findutils -y
RUN yum install zip -y
RUN amazon-linux-extras install python3=3.6.2
RUN pip3 install --upgrade pip
RUN pip3 install -r requirements.txt -t aws_layer/python&lt;/pre&gt;
&lt;p&gt;The requirements.txt includes just one package for simplicity. It is the common &lt;a href="https://jinja.palletsprojects.com/en/2.11.x/" target="_blank"&gt;templating for Python called Jinja2&lt;/a&gt;&lt;/p&gt;
&lt;pre&gt;Jinja2==2.11.1&lt;/pre&gt;
&lt;p&gt;You can test your Lambda function by simple calling it with sample parameters:&lt;/p&gt;
&lt;pre&gt;import lambda_function
event = {
    "queryStringParameters": {
        "param1": "value1"
    },
    "path": "/api",
    "requestContext": {
         "param2": "value2"
    }
}
res = lambda_function.lambda_handler(event=event, context={})
assert 200 == int(res["statusCode"])&lt;/pre&gt;
&lt;h4&gt;Step 1 — Write html template&lt;/h4&gt;
&lt;p&gt;In this step, we write the html template the Lambda function will return. A good default is the new &lt;a href="https://v5.getbootstrap.com/docs/5.0/getting-started/introduction/" target="_blank"&gt;Bootstrap 5&lt;/a&gt; CSS framework where the recommended starting markup looks something like this:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*b3ZXkCsw8BwLt2Fx_eu6PQ.png.png"&gt;&lt;img src="/theme/images/1*b3ZXkCsw8BwLt2Fx_eu6PQ.png.png" alt="Sample HTML page" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Saving this file in folder “templates” and naming it index.html, we are ready to write the Lambda function.&lt;/p&gt;
&lt;h4&gt;Step 2 — Write Lambda function to serve your html page&lt;/h4&gt;
&lt;p&gt;In the example below, the lambda function expects URL parameters and parses those. So when parsing a custom URL, the format would look something like this: Example.com/?my_name=somename. See step 10 in &lt;a href="https://adamnovotny.com/blog/serverless-web-apps-with-firebase-and-aws-lambda.html" target="_blank"&gt;this tutorial&lt;/a&gt; to add custom URLs to your API Gateway-triggered Lambda functions.&lt;/p&gt;
&lt;pre&gt;import os
import sys
from jinja2 import Environment, FileSystemLoader&lt;/pre&gt;
&lt;pre&gt;def lambda_handler(event, context):
    env = Environment(loader=FileSystemLoader(os.path.join(os.path.dirname(__file__), "templates"), encoding="utf8"))
    my_name = False
    if event["queryStringParameters"] and "my_name" in event["queryStringParameters"]:
        my_name_query = event["queryStringParameters"]["my_name"]
    template = env.get_template("index.html")
    html = template.render(
        my_name=my_name_query
    )
    return response(html)&lt;/pre&gt;
&lt;pre&gt;def response(myhtml):
    return {
        "statusCode": 200,
        "body": myhtml,
        "headers": {
            "Content-Type": "text/html",
        }
    }&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;jinja2 loads your previously created index.html using class “FileSystemLoader” and we store it as variable “env”&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;variable “my_name” is parsed from the URL query parameters as explained above and stored as the Python variable my_name_query&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;the jinja2 render function then passes my_name_query to the template and returns the html page&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Also published on &lt;a href="https://adamnovotny.com" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category></entry><entry><title>COVID-19 Driving Stock Market Higher Than 1 Year Ago?</title><link href="https://adamnovotny.com/blog/covid-19-driving-stock-market-higher-than-1-year-ago.html" rel="alternate"></link><published>2020-05-16T00:00:00-05:00</published><updated>2020-05-16T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-05-16:/blog/covid-19-driving-stock-market-higher-than-1-year-ago.html</id><summary type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States" target="_blank"&gt;COVID-19&lt;/a&gt; became a serious concern for wider public in the USA somewhere between February and March of 2020. Today on May 16, &lt;a href="https://finance.yahoo.com/chart/%5EGSPC#eyJpbnRlcnZhbCI6ImRheSIsInBlcmlvZGljaXR5IjoxLCJ0aW1lVW5pdCI6bnVsbCwiY2FuZGxlV2lkdGgiOjMuODg1Mzc1NDk0MDcxMTQ2NSwiZmxpcHBlZCI6ZmFsc2UsInZvbHVtZVVuZGVybGF5Ijp0cnVlLCJhZGoiOnRydWUsImNyb3NzaGFpciI6dHJ1ZSwiY2hhcnRUeXBlIjoibGluZSIsImV4dGVuZGVkIjpmYWxzZSwibWFya2V0U2Vzc2lvbnMiOnt9LCJhZ2dyZWdhdGlvblR5cGUiOiJvaGxjIiwiY2hhcnRTY2FsZSI6ImxpbmVhciIsInN0dWRpZXMiOnsi4oCMdm9sIHVuZHLigIwiOnsidHlwZSI6InZvbCB1bmRyIiwiaW5wdXRzIjp7ImlkIjoi4oCMdm9sIHVuZHLigIwiLCJkaXNwbGF5Ijoi4oCMdm9sIHVuZHLigIwifSwib3V0cHV0cyI6eyJVcCBWb2x1bWUiOiIjMDBiMDYxIiwiRG93biBWb2x1bWUiOiIjZmYzMzNhIn0sInBhbmVsIjoiY2hhcnQiLCJwYXJhbWV0ZXJzIjp7IndpZHRoRmFjdG9yIjowLjQ1LCJjaGFydE5hbWUiOiJjaGFydCJ9fX0sInBhbmVscyI6eyJjaGFydCI6eyJwZXJjZW50IjoxLCJkaXNwbGF5IjoiXkdTUEMiLCJjaGFydE5hbWUiOiJjaGFydCIsImluZGV4IjowLCJ5QXhpcyI6eyJuYW1lIjoiY2hhcnQiLCJwb3NpdGlvbiI6bnVsbH0sInlheGlzTEhTIjpbXSwieWF4aXNSSFMiOlsiY2hhcnQiLCLigIx2b2wgdW5kcuKAjCJdfX0sInNldFNwYW4iOnsibXVsdGlwbGllciI6MSwiYmFzZSI6InllYXIiLCJwZXJpb2RpY2l0eSI6eyJwZXJpb2QiOjEsImludGVydmFsIjoiZGF5In19LCJsaW5lV2lkdGgiOjIsInN0cmlwZWRCYWNrZ3JvdW5kIjp0cnVlLCJldmVudHMiOnRydWUsImNvbG9yIjoiIzAwODFmMiIsInN0cmlwZWRCYWNrZ3JvdWQiOnRydWUsImV2ZW50TWFwIjp7ImNvcnBvcmF0ZSI6eyJkaXZzIjp0cnVlLCJzcGxpdHMiOnRydWV9LCJzaWdEZXYiOnt9fSwiY3VzdG9tUmFuZ2UiOm51bGwsInN5bWJvbHMiOlt7InN5bWJvbCI6Il5HU1BDIiwic3ltYm9sT2JqZWN0Ijp7InN5bWJvbCI6Il5HU1BDIiwicXVvdGVUeXBlIjoiSU5ERVgiLCJleGNoYW5nZVRpbWVab25lIjoiQW1lcmljYS9OZXdfWW9yayJ9LCJwZXJpb2RpY2l0eSI6MSwiaW50ZXJ2YWwiOiJkYXkiLCJ0aW1lVW5pdCI6bnVsbCwic2V0U3BhbiI6eyJtdWx0aXBsaWVyIjoxLCJiYXNlIjoieWVhciIsInBlcmlvZGljaXR5Ijp7InBlcmlvZCI6MSwiaW50ZXJ2YWwiOiJkYXkifX19XX0%3D" target="_blank"&gt;stock markets&lt;/a&gt; in the USA are optimistically higher than a year ago while most of the country is still under &lt;a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Lockdowns" target="_blank"&gt;strict lockdown restrictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*yIDMVanoFc79QAurxt_pfw.png.png"&gt;&lt;img src="/theme/images/1*yIDMVanoFc79QAurxt_pfw.png.png" alt="S&amp;P 500 May 16, 2019 — May 16, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Uncertainty around the virus is …&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States" target="_blank"&gt;COVID-19&lt;/a&gt; became a serious concern for wider public in the USA somewhere between February and March of 2020. Today on May 16, &lt;a href="https://finance.yahoo.com/chart/%5EGSPC#eyJpbnRlcnZhbCI6ImRheSIsInBlcmlvZGljaXR5IjoxLCJ0aW1lVW5pdCI6bnVsbCwiY2FuZGxlV2lkdGgiOjMuODg1Mzc1NDk0MDcxMTQ2NSwiZmxpcHBlZCI6ZmFsc2UsInZvbHVtZVVuZGVybGF5Ijp0cnVlLCJhZGoiOnRydWUsImNyb3NzaGFpciI6dHJ1ZSwiY2hhcnRUeXBlIjoibGluZSIsImV4dGVuZGVkIjpmYWxzZSwibWFya2V0U2Vzc2lvbnMiOnt9LCJhZ2dyZWdhdGlvblR5cGUiOiJvaGxjIiwiY2hhcnRTY2FsZSI6ImxpbmVhciIsInN0dWRpZXMiOnsi4oCMdm9sIHVuZHLigIwiOnsidHlwZSI6InZvbCB1bmRyIiwiaW5wdXRzIjp7ImlkIjoi4oCMdm9sIHVuZHLigIwiLCJkaXNwbGF5Ijoi4oCMdm9sIHVuZHLigIwifSwib3V0cHV0cyI6eyJVcCBWb2x1bWUiOiIjMDBiMDYxIiwiRG93biBWb2x1bWUiOiIjZmYzMzNhIn0sInBhbmVsIjoiY2hhcnQiLCJwYXJhbWV0ZXJzIjp7IndpZHRoRmFjdG9yIjowLjQ1LCJjaGFydE5hbWUiOiJjaGFydCJ9fX0sInBhbmVscyI6eyJjaGFydCI6eyJwZXJjZW50IjoxLCJkaXNwbGF5IjoiXkdTUEMiLCJjaGFydE5hbWUiOiJjaGFydCIsImluZGV4IjowLCJ5QXhpcyI6eyJuYW1lIjoiY2hhcnQiLCJwb3NpdGlvbiI6bnVsbH0sInlheGlzTEhTIjpbXSwieWF4aXNSSFMiOlsiY2hhcnQiLCLigIx2b2wgdW5kcuKAjCJdfX0sInNldFNwYW4iOnsibXVsdGlwbGllciI6MSwiYmFzZSI6InllYXIiLCJwZXJpb2RpY2l0eSI6eyJwZXJpb2QiOjEsImludGVydmFsIjoiZGF5In19LCJsaW5lV2lkdGgiOjIsInN0cmlwZWRCYWNrZ3JvdW5kIjp0cnVlLCJldmVudHMiOnRydWUsImNvbG9yIjoiIzAwODFmMiIsInN0cmlwZWRCYWNrZ3JvdWQiOnRydWUsImV2ZW50TWFwIjp7ImNvcnBvcmF0ZSI6eyJkaXZzIjp0cnVlLCJzcGxpdHMiOnRydWV9LCJzaWdEZXYiOnt9fSwiY3VzdG9tUmFuZ2UiOm51bGwsInN5bWJvbHMiOlt7InN5bWJvbCI6Il5HU1BDIiwic3ltYm9sT2JqZWN0Ijp7InN5bWJvbCI6Il5HU1BDIiwicXVvdGVUeXBlIjoiSU5ERVgiLCJleGNoYW5nZVRpbWVab25lIjoiQW1lcmljYS9OZXdfWW9yayJ9LCJwZXJpb2RpY2l0eSI6MSwiaW50ZXJ2YWwiOiJkYXkiLCJ0aW1lVW5pdCI6bnVsbCwic2V0U3BhbiI6eyJtdWx0aXBsaWVyIjoxLCJiYXNlIjoieWVhciIsInBlcmlvZGljaXR5Ijp7InBlcmlvZCI6MSwiaW50ZXJ2YWwiOiJkYXkifX19XX0%3D" target="_blank"&gt;stock markets&lt;/a&gt; in the USA are optimistically higher than a year ago while most of the country is still under &lt;a href="https://en.wikipedia.org/wiki/COVID-19_pandemic_in_the_United_States#Lockdowns" target="_blank"&gt;strict lockdown restrictions&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*yIDMVanoFc79QAurxt_pfw.png.png"&gt;&lt;img src="/theme/images/1*yIDMVanoFc79QAurxt_pfw.png.png" alt="S&amp;P 500 May 16, 2019 — May 16, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Uncertainty around the virus is still high and vaccine seems to be &lt;a href="https://www.mayoclinic.org/diseases-conditions/coronavirus/in-depth/coronavirus-vaccine/art-20484859" target="_blank"&gt;12–18 months&lt;/a&gt; away. The closest region approaching herd immunity is &lt;a href="https://www.governor.ny.gov/news/amid-ongoing-covid-19-pandemic-governor-cuomo-announces-results-completed-antibody-testing" target="_blank"&gt;NYC with 20%&lt;/a&gt; spread of the virus among the population. But that is nowhere near herd immunity requirements. “&lt;a href="https://www.discovermagazine.com/health/is-herd-immunity-our-best-weapon-against-covid-19" target="_blank"&gt;Experts predict at least 70 percent of the population will need to be immune to the virus in order to achieve herd immunity.&lt;/a&gt;”&lt;/p&gt;
&lt;p&gt;Most economics charts worldwide have “broken” websites because y-axes do not reach low enough. Such is the extend of the ongoing economic pain.&lt;/p&gt;
&lt;h4&gt;Global PMI: &lt;a href="https://www.bloomberg.com/graphics/world-economic-indicators-dashboard/" target="_blank"&gt;39.8&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;JPMorgan Chase’s snapshot of the health of manufacturing around the world&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*gzR-TCrDGCkkYvx3VQBFDw.png.png"&gt;&lt;img src="/theme/images/1*gzR-TCrDGCkkYvx3VQBFDw.png.png" alt="Bloomberg Global PMI: 39.8. Updates monthly Last May 4, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;U.S. Employment: &lt;a href="https://www.bloomberg.com/graphics/world-economic-indicators-dashboard/" target="_blank"&gt;−20,537K&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*MeRsagCM6CTY8P-fzXX2kw.png.png"&gt;&lt;img src="/theme/images/1*MeRsagCM6CTY8P-fzXX2kw.png.png" alt="Bloomberg U.S. Employment: −20,537K. Updates monthly Last May 8, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;U.S. consumer spending: &lt;a href="https://www.bloomberg.com/graphics/world-economic-indicators-dashboard/" target="_blank"&gt;−7.5%&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*u-YO0zYlma3HuJ8zX3RSdA.png.png"&gt;&lt;img src="/theme/images/1*u-YO0zYlma3HuJ8zX3RSdA.png.png" alt="Bloomberg U.S. consumer spending: −7.5%. Updates monthly Last April 30, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;German Ifo: &lt;a href="https://www.bloomberg.com/graphics/world-economic-indicators-dashboard/" target="_blank"&gt;74.3&lt;/a&gt;&lt;/h4&gt;
&lt;p&gt;The leading indicator of health in the euro area’s largest economy based on a survey of ~7,000 executives in German manufacturing, services, retail, wholesale and construction companies.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*S2uDL0Mz95L0JxSt4w54DA.png.png"&gt;&lt;img src="/theme/images/1*S2uDL0Mz95L0JxSt4w54DA.png.png" alt="Bloomberg German Ifo: 74.3. Updates monthly Last April 24, 2020" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;How do we make sense of the stock market’s optimistic behavior during these strong headwinds?&lt;/h4&gt;
&lt;p&gt;There are only 2 unconvincing answers that come to mind:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Stock markets are forward-looking and all the damage of the virus is behind us.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;FED can support the markets indefinitely by buying everything publicly traded using printed money. The following &lt;a href="https://fred.stlouisfed.org/series/WALCL" target="_blank"&gt;chart&lt;/a&gt; shows the FEDs assets currently sitting at $6,934,227,000,000, or $6.9T, which have “broken” the y-axis in the opposite direction.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*NDl-1RICjp9jP-AEW6yeCg.png.png"&gt;&lt;img src="/theme/images/1*NDl-1RICjp9jP-AEW6yeCg.png.png" alt="Board of Governors of the Federal Reserve System (US). Total Assets. Retrieved from FRED, Federal Reserve Bank of St. Louis; https://fred.stlouisfed.org/series/WALCL, May 16, 2020." style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The first explanation suggests that there are many brave people betting against the virus. The second suggests that future inflation will never become a factor. Neither is convincing.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>The Real Divide: Those Who Are Not Allowed to Work</title><link href="https://adamnovotny.com/blog/the-real-divide-those-who-are-not-allowed-to-work.html" rel="alternate"></link><published>2020-05-09T00:00:00-05:00</published><updated>2020-05-09T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-05-09:/blog/the-real-divide-those-who-are-not-allowed-to-work.html</id><summary type="html">&lt;p&gt;With the employment rate in the US jumping to alarming &lt;a href="https://tradingeconomics.com/united-states/unemployment-rate" target="_blank"&gt;14.7%&lt;/a&gt; from less than 4% in just one quarter, the COVID-19 crisis has created a gap between those who are allowed to work, and those who are not.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*jIsZ9NrYpyAtH4Vy4Gl3DQ.png.png"&gt;&lt;img src="/theme/images/1*jIsZ9NrYpyAtH4Vy4Gl3DQ.png.png" alt="Industry classification by job security during the COVID crisis" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When people talk about inequality, they typically discuss absolute wealth levels …&lt;/p&gt;</summary><content type="html">&lt;p&gt;With the employment rate in the US jumping to alarming &lt;a href="https://tradingeconomics.com/united-states/unemployment-rate" target="_blank"&gt;14.7%&lt;/a&gt; from less than 4% in just one quarter, the COVID-19 crisis has created a gap between those who are allowed to work, and those who are not.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*jIsZ9NrYpyAtH4Vy4Gl3DQ.png.png"&gt;&lt;img src="/theme/images/1*jIsZ9NrYpyAtH4Vy4Gl3DQ.png.png" alt="Industry classification by job security during the COVID crisis" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;When people talk about inequality, they typically discuss absolute wealth levels as a point in time. This is similar as looking at a company’s balance sheet. However, I’ll argue that looking at a person’s freedom to continue working is more important. Not unlike looking at a company’s cash flow.&lt;/p&gt;
&lt;p&gt;I classified industries into 4 quadrants depending whether they are mission critical to maintain our standard of living and whether they require physical presence of employees:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;On the low end in terms of job security are jobs where employees cannot work remotely. Also when they do not come to work modern society remains relatively intact. These include hair salons and bars.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Next we have jobs that can be done remotely but again, the products are sometimes more of a luxury rather than a necessity. These include many types of internet businesses. Even Google with its tens of billions in cash and no debt is &lt;a href="https://www.theverge.com/2020/4/15/21222942/google-slowing-down-hiring-through-2020-covid-19-pandemic" target="_blank"&gt;“significantly slowing down hiring”&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Then we have industries such as airlines which are absolutely critical for the maintenance of our standard of living, but they require physical presence of both employees and customers. These industries are heavily impacted because many trips are not essential. &lt;a href="https://www.bloomberg.com/news/features/2020-04-24/coronavirus-travel-covid-19-will-change-airlines-and-how-we-fly" target="_blank"&gt;Airlines find themselves with extreme levels of overcapacity&lt;/a&gt; as customers reassess their travel needs.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Lastly, some jobs are essential and yet they do not require employees’ physical presence. These include analytical jobs in national security or &lt;a href="https://www.cnbc.com/2020/05/06/why-you-should-take-advantage-of-your-telemedicine-options.html" target="_blank"&gt;telemedicine&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*_7W4swGJ2COrBH2YIXk82w.png.png"&gt;&lt;img src="/theme/images/1*_7W4swGJ2COrBH2YIXk82w.png.png" alt="Parked airplanes due to COVID-19" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;People need to start thinking about inequality in terms of flow of wealth rather than accumulation. People whose flow of income is forcibly cut face life-altering events such as evictions, permanent negotiations with creditors, and limits on their &lt;a href="https://www.bankrate.com/mortgages/credit-score-gap-widens-for-mortgage-borrowers/" target="_blank"&gt;future access to resources&lt;/a&gt;. Paying my mortgage this month is more important than expensive clothes in my closet.&lt;/p&gt;
&lt;h4&gt;Update 6/13/2020&lt;/h4&gt;
&lt;p&gt;A related consequence is that working hours and wealth have become more correlated. This is well explained in the Making Sense podcast by Sam Harriss and his guest Daniel Markovits in their episode titled &lt;a href="https://samharris.org/podcasts/205-failure-meritocracy/" target="_blank"&gt;The Failure of Meritocracy&lt;/a&gt;. Wealthier people are more likely to work longer hours because their professions allow for multiple working environments, all of which further worsens inequality.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>Free Healthcare Is Free Markets</title><link href="https://adamnovotny.com/blog/free-healthcare-is-free-markets.html" rel="alternate"></link><published>2020-04-25T00:00:00-05:00</published><updated>2020-04-25T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-04-25:/blog/free-healthcare-is-free-markets.html</id><summary type="html">&lt;p&gt;Healthy people lead to more competition, not less. Free basic healthcare leads to more capitalism which some supporters of free markets seem to misunderstand.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*LsM2ZEBc5cltUyuq0pOS0w.png.png"&gt;&lt;img src="/theme/images/1*LsM2ZEBc5cltUyuq0pOS0w.png.png" alt="Waterfalls" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Entrepreneurs innovate in the economy because of their willingness to work hard AND because they are physically able. A young entreprenuer’s career in the US …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Healthy people lead to more competition, not less. Free basic healthcare leads to more capitalism which some supporters of free markets seem to misunderstand.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*LsM2ZEBc5cltUyuq0pOS0w.png.png"&gt;&lt;img src="/theme/images/1*LsM2ZEBc5cltUyuq0pOS0w.png.png" alt="Waterfalls" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Entrepreneurs innovate in the economy because of their willingness to work hard AND because they are physically able. A young entreprenuer’s career in the US could be permanently damaged if she is diagnosed with cancer. The fear of reoccurrence and financial ruin could result her giving up on her dreams andtaking a steady, but much less impactful corporate job with a healthcare plan. This behavior occurs daily and it permanently damages the economy and therefore everyone.&lt;/p&gt;
&lt;p&gt;Market-based healthcare also creates inefficiencies at the worst possible time. During the COVID-19 crisis, US states engaged in a &lt;a href="https://www.cnbc.com/2020/04/09/why-states-and-the-federal-government-are-bidding-on-ppe.html" target="_blank"&gt;bidding war against each other to obtain basic medical equipment&lt;/a&gt;. There is no reason why a resident of a wealthier US state should have access to more life-saving equipment. This ultimate hurts everyone: patients, tax payers, and most importantly the hospital workers.&lt;/p&gt;
&lt;p&gt;Of course, there needs to be a way to ensure that people who intentionally harm themselves are not subsidized to do so. Smokers who develop lung cancer need to pay for their coverage in full.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Riw51hOHOmP6efnOqXOYAg.jpeg.png"&gt;&lt;img src="/theme/images/1*Riw51hOHOmP6efnOqXOYAg.jpeg.png" alt="Ronald Reagan Speaks Out Against Socialized Medicine" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;“One of the traditional methods of imposing statism or socialism on a people has been by way of medicine. It’s very easy to disguise a medical program as a humanitarian project, most people are a little reluctant to oppose anything that suggests medical care for people who possibly can’t afford it.” ~ &lt;a href="https://en.wikipedia.org/wiki/Ronald_Reagan_Speaks_Out_Against_Socialized_Medicine" target="_blank"&gt;Ronald Reagan&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I am a strong supporter of free market economics. I believe that the economic motivation is the primary reason why people take risks, employ others, innovate, and make everyone around them wealthier. But free basic healthcare is not the same thing as a free Ferrari. It is not the same as wealthy parents giving their spoiled kids free yachts. What Reagan suggests is that free basic healthcare is a form of communist indoctrination. Others suggest that free basic healthcare would strip people of their economic motivation to work hard.&lt;/p&gt;
&lt;p&gt;Free basic healthcare doesn’t make people more or less motivated. No one has ever started a business or new careers because of their motivation to pay for healthcare.&lt;/p&gt;
&lt;p&gt;Free basic healthcare just levels the playing field so that everyone can compete, which is what free markets are all about.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>What I No Longer Believe</title><link href="https://adamnovotny.com/blog/what-i-no-longer-believe.html" rel="alternate"></link><published>2020-04-18T00:00:00-05:00</published><updated>2020-04-18T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-04-18:/blog/what-i-no-longer-believe.html</id><summary type="html">&lt;p&gt;I used to believe that optimizing for efficiency should be the main goal of modern economies. Efficiency limits corruption while driving innovation and therefore improves people’s lives. However, the unintended consequence is that resiliency is often ignored.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*JpOZ3WdW4en8iq55twe7nA.png.png"&gt;&lt;img src="/theme/images/1*JpOZ3WdW4en8iq55twe7nA.png.png" alt="Parked airplanes by Nick Oxford/Reuters" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If there is one lesson to be taken from the &lt;a href="https://en.wikipedia.org/wiki/Coronavirus_disease_2019" target="_blank"&gt;COVID-19 crisis …&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;I used to believe that optimizing for efficiency should be the main goal of modern economies. Efficiency limits corruption while driving innovation and therefore improves people’s lives. However, the unintended consequence is that resiliency is often ignored.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*JpOZ3WdW4en8iq55twe7nA.png.png"&gt;&lt;img src="/theme/images/1*JpOZ3WdW4en8iq55twe7nA.png.png" alt="Parked airplanes by Nick Oxford/Reuters" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;If there is one lesson to be taken from the &lt;a href="https://en.wikipedia.org/wiki/Coronavirus_disease_2019" target="_blank"&gt;COVID-19 crisis&lt;/a&gt;, it is the reminder that human life is an external system vulnerable to adverse events.&lt;/p&gt;
&lt;p&gt;Life is not a computer game and how things go wrong is not easy to model due to the complex correlations of the physical world. Manufacturing everything in China may be the most efficient under ideal circumstances. But when things go wrong, they go wrong at the same time. Air freight prices increased &lt;a href="https://www.marketplace.org/2020/03/24/air-freight-prices-surging-amid-covid19/" target="_blank"&gt;increased by 100+% because of no capacity of passenger flights&lt;/a&gt; which are all grounded.&lt;/p&gt;
&lt;p&gt;So efficient manufacturing works if logistics works but now logistics is failing.&lt;/p&gt;
&lt;p&gt;Countries lacking manufacturing capacity (such USA and most of Europe) need to focus on resiliency. This should be done by paying what I call “manufacturing insurance”. In other words, countries should pay slightly higher prices for &lt;a href="https://www.npr.org/sections/health-shots/2020/03/05/811387424/face-masks-not-enough-are-made-in-america-to-deal-with-coronavirus" target="_blank"&gt;essential items such as face masks manufactured locally&lt;/a&gt;. This will insure countries against expensive shocks later.&lt;/p&gt;
&lt;p&gt;In the race towards efficiency, western governments saved pennies on masks early to &lt;a href="https://www.npr.org/2020/03/26/821457551/whats-inside-the-senate-s-2-trillion-coronavirus-aid-package" target="_blank"&gt;pay trillions later&lt;/a&gt;.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>On government-run capitalism</title><link href="https://adamnovotny.com/blog/on-government-run-capitalism.html" rel="alternate"></link><published>2020-04-11T00:00:00-05:00</published><updated>2020-04-11T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2020-04-11:/blog/on-government-run-capitalism.html</id><summary type="html">&lt;p&gt;Ever since the financial crisis, it has become clear that the value of companies is linked with the willingness of governments to bail them out. The top financial gainers from the 2008 bailout program (TARP) were &lt;a href="https://en.wikipedia.org/wiki/Troubled_Asset_Relief_Program" target="_blank"&gt;Citi, BoA, AIG, JPM, Wells, GMAC, GM&lt;/a&gt;…&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*SBUVooqX6_0HDDG3sD8EqA.png.png"&gt;&lt;img src="/theme/images/1*SBUVooqX6_0HDDG3sD8EqA.png.png" alt="Largest bailouts in history in 2020 USD ($bn) vs. G7 debt to GDP. Deutsche Bank" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In 2020 the covid crisis sheds additional …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Ever since the financial crisis, it has become clear that the value of companies is linked with the willingness of governments to bail them out. The top financial gainers from the 2008 bailout program (TARP) were &lt;a href="https://en.wikipedia.org/wiki/Troubled_Asset_Relief_Program" target="_blank"&gt;Citi, BoA, AIG, JPM, Wells, GMAC, GM&lt;/a&gt;…&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*SBUVooqX6_0HDDG3sD8EqA.png.png"&gt;&lt;img src="/theme/images/1*SBUVooqX6_0HDDG3sD8EqA.png.png" alt="Largest bailouts in history in 2020 USD ($bn) vs. G7 debt to GDP. Deutsche Bank" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In 2020 the covid crisis sheds additional light on the issue. No matter how poorly run a company is, it will receive unlimited funds if it employs enough people or can be considered important to national security.&lt;/p&gt;
&lt;p&gt;What makes even less sense is that &lt;a href="https://www.npr.org/2019/12/20/789540931/2-years-later-trump-tax-cuts-have-failed-to-deliver-on-gops-promises" target="_blank"&gt;corporate taxes have been decreasing in the US&lt;/a&gt;. As long as the bailout culture remains, taxes should be high because they become a form of insurance against future bailouts. Currently, bailouts and low tax rates are the ultimate wealth transfer from younger generations to the older.&lt;/p&gt;
&lt;p&gt;The Eurozone is not sleeping behind the wheel either. The single market has strict rules against government subsidies to allow companies from all countries to compete freely. But these have been suspended as governments spray their economies with $2.2 trillion of support. Germany spent 50% of that amount propping up its businesses. This becomes absurdly anti-competitive if you consider the disadvantage of companies from poorer countries if their governments cannot afford similar subsidies. What’s the point of having rules if they are paused during every crisis?&lt;/p&gt;
&lt;p&gt;In capitalism, companies are valuable if they make efficiently something that people want.&lt;/p&gt;
&lt;p&gt;That pure equation making capitalism valuable no longer applies. Big business maintains its status by colluding with government to:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;remove competitors (example Facebook, see &lt;a href="https://www.nytimes.com/2019/05/09/opinion/sunday/chris-hughes-facebook-zuckerberg.html" target="_blank"&gt;article by co-founder Chris Hughes&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;become systemically important by claiming national security or high employment status resulting in government-financed &lt;a href="https://en.wikipedia.org/wiki/Put_option" target="_blank"&gt;puts&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;In either case, capitalism is dead. Welcome to government-run capitalism fueled by moral hazard.&lt;/p&gt;
&lt;p&gt;[updated 5/23/2020]&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>Climate Change part 1: Sea Level Change</title><link href="https://adamnovotny.com/blog/climate-change-part-1-sea-level-change.html" rel="alternate"></link><published>2019-12-15T00:00:00-05:00</published><updated>2019-12-15T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2019-12-15:/blog/climate-change-part-1-sea-level-change.html</id><summary type="html">&lt;p&gt;After reading many articles about climate change, I decided to start doing a technical deep dive and study the data. In the first part, I will just locate and visualize sea level data.&lt;/p&gt;
&lt;p&gt;To begin with, I looked at data from NOAA focused on sea level rise calculated from satellite …&lt;/p&gt;</summary><content type="html">&lt;p&gt;After reading many articles about climate change, I decided to start doing a technical deep dive and study the data. In the first part, I will just locate and visualize sea level data.&lt;/p&gt;
&lt;p&gt;To begin with, I looked at data from NOAA focused on sea level rise calculated from satellite images. The data is &lt;a href="https://www.star.nesdis.noaa.gov/sod/lsa/SeaLevelRise/LSA_SLR_timeseries.php" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Zm7zAFn44DDqv3tz-1MvhQ.jpeg.png"&gt;&lt;img src="/theme/images/1*Zm7zAFn44DDqv3tz-1MvhQ.jpeg.png" alt="Source: https://en.wikipedia.org/wiki/TOPEX/Poseidon" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/TOPEX/Poseidon" target="_blank"&gt;satellite missions collecting the data&lt;/a&gt; are no less interesting. The initial mission TOPEX/Poseidon was followed up by Jason-1, Jason-2, and Jason-3.&lt;/p&gt;
&lt;p&gt;The data appears convincing:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*eM-1SqQmZKBhenCTqEtAVQ.png.png"&gt;&lt;img src="/theme/images/1*eM-1SqQmZKBhenCTqEtAVQ.png.png" alt="Mean sea level anomaly of missions" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;and descriptive stats:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*LGgz3fwWycKVOJxZuVhcyA.png.png"&gt;&lt;img src="/theme/images/1*LGgz3fwWycKVOJxZuVhcyA.png.png" alt="Descriptive statistics of missions" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Random"></category></entry><entry><title>Custom VPN using PiVPN and public cloud</title><link href="https://adamnovotny.com/blog/custom-vpn-using-pivpn-and-public-cloud.html" rel="alternate"></link><published>2018-12-30T00:00:00-05:00</published><updated>2018-12-30T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-12-30:/blog/custom-vpn-using-pivpn-and-public-cloud.html</id><summary type="html">&lt;h4&gt;Motivation: Many public Wi-Fi networks block certain internet ports and protocols. For example, a public library might only allow ports 80 and 443 and the TCP protocol. Leaving aside the logic of such decisions by network owners, they prevent users from taking advantage of many commercial VPN products that rely …&lt;/h4&gt;</summary><content type="html">&lt;h4&gt;Motivation: Many public Wi-Fi networks block certain internet ports and protocols. For example, a public library might only allow ports 80 and 443 and the TCP protocol. Leaving aside the logic of such decisions by network owners, they prevent users from taking advantage of many commercial VPN products that rely on other ports. The goal of this article is to create a custom VPN solution to improve privacy even on such restricted public networks.&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*hI_JbE-q2OQpXW1tEB46Cg.png.png"&gt;&lt;img src="/theme/images/1*hI_JbE-q2OQpXW1tEB46Cg.png.png" alt="AWS, Google Cloud, Microsoft Azure" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;
&lt;a href="/theme/images/1*7C1odbX4Kk_yxToAaKnlwA.png.png"&gt;&lt;img src="/theme/images/1*7C1odbX4Kk_yxToAaKnlwA.png.png" alt="PiVPN" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;All step are outlined in more detail in &lt;a href="https://github.com/adam5ny/vpn-gcp-pivpn" target="_blank"&gt;this Github repo&lt;/a&gt;. The tutorial is written for Python 3 and &lt;a href="https://cloud.google.com/compute/" target="_blank"&gt;Google Cloud Compute&lt;/a&gt;. However, all public clouds can be used including AWS or Azure.&lt;/p&gt;
&lt;h4&gt;Create public cloud compute instance&lt;/h4&gt;
&lt;p&gt;Login to &lt;a href="https://console.cloud.google.com/" target="_blank"&gt;GCP console&lt;/a&gt;. Create an Ubuntu machine and make sure to allow https traffic. Then locate your public IP which is where your traffic will be routed. In GCP, you can find it using the following steps (as of Dec 2018): VPC network &gt; External IP addresses &gt; switch the type of your instance IP from “Ephemeral” to “Static”. This will be your public IP.&lt;/p&gt;
&lt;h4&gt;Create PiVPN instance&lt;/h4&gt;
&lt;p&gt;Login to your compute instance and download PiVPN using the following command:&lt;/p&gt;
&lt;pre&gt;curl -L &lt;a href="https://install.pivpn.io" target="_blank"&gt;https://install.pivpn.io&lt;/a&gt; | bash&lt;/pre&gt;
&lt;p&gt;Follow all setup steps using default values except for port and protocol. Select port 443 and protocol TCP. Select reboot at the end of the installation.&lt;/p&gt;
&lt;h4&gt;Create VPN credentials&lt;/h4&gt;
&lt;pre&gt;pivpn add&lt;/pre&gt;
&lt;p&gt;Enter your custom username and password. Download credentials to your computer from your newly create cloud computer instance. Credentials are typically located at ~/ovpns on your Ubuntu instance.&lt;/p&gt;
&lt;h4&gt;Download a VPN client for your platform&lt;/h4&gt;
&lt;p&gt;For MacOS you may use &lt;a href="https://tunnelblick.net/" target="_blank"&gt;Tunnelblick&lt;/a&gt;. Then drag credentials (.ovpn) from the previous step to the Tunneblick app icon. Click on Tunneblick icon to connect to your VPN with your custom username and password.&lt;/p&gt;
&lt;h4&gt;Final notes:&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;use port 80 instead of 443 above if necessary.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;to minimize cost, remember to shut down your compute instance when you are not using the VPN. The typical cost is &lt; $20 for 100GB of traffic and 24/7 usage which is in line with respectable third-party VPN providers. However, your cost may be significantly lower if you shut down unused instances and use pre-emptible instances (GCP-specific).&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category></entry><entry><title>Lessons from the 2008 Financial Crisis (t+10 edition)</title><link href="https://adamnovotny.com/blog/lessons-from-the-2008-financial-crisis-t-10-edition.html" rel="alternate"></link><published>2018-09-23T00:00:00-05:00</published><updated>2018-09-23T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-09-23:/blog/lessons-from-the-2008-financial-crisis-t-10-edition.html</id><summary type="html">&lt;h4&gt;Contents: Too Big To Fail, Low Interest Rates &amp; Mortgages, Accountability and Ethics&lt;/h4&gt;
&lt;p&gt;This month has marked the 10th anniversary of the global financial crisis of 2008 and media raced to discuss the &lt;a href="https://www.economist.com/leaders/2018/09/06/the-world-has-not-learned-the-lessons-of-the-financial-crisis" target="_blank"&gt;lessons (not) learned&lt;/a&gt;. In reality, the only lesson learned is that the largest financial institutions must be bailed …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Contents: Too Big To Fail, Low Interest Rates &amp; Mortgages, Accountability and Ethics&lt;/h4&gt;
&lt;p&gt;This month has marked the 10th anniversary of the global financial crisis of 2008 and media raced to discuss the &lt;a href="https://www.economist.com/leaders/2018/09/06/the-world-has-not-learned-the-lessons-of-the-financial-crisis" target="_blank"&gt;lessons (not) learned&lt;/a&gt;. In reality, the only lesson learned is that the largest financial institutions must be bailed out at all costs to keep the current financial system afloat.&lt;/p&gt;
&lt;p&gt;Below I review some of the main causes of the 2008 financial crisis and comment on what we have learned in the past 10 years.&lt;/p&gt;
&lt;h4&gt;Too Big To Fail&lt;/h4&gt;
&lt;p&gt;In an ideal world, every sector of the economy would be subject to Joseph Schumpeter’s &lt;a href="https://en.wikipedia.org/wiki/Creative_destruction" target="_blank"&gt;Creative Destruction&lt;/a&gt;. Economies improve the lives of citizens when businesses can be readily replaced by more efficient and innovative competitors. Unfortunately, this rule not apply to the financial system today.&lt;/p&gt;
&lt;p&gt;Creative destruction cannot happen as long the financial system remains as interconnected as it has been. Just once did regulators in the US attempt to use tough love and punish bankers in 2008. Debating whether or not large banks should be allowed to go bankrupt, &lt;a href="https://en.wikipedia.org/wiki/Bankruptcy_of_Lehman_Brothers" target="_blank"&gt;Lehman was allowed to declare bankruptcy&lt;/a&gt; with no clear plans for its future. This was a miserable failure. The wave of uncertainty in the next few days resulted the need for the &lt;a href="https://en.wikipedia.org/wiki/American_Recovery_and_Reinvestment_Act_of_2009" target="_blank"&gt;largest bailout&lt;/a&gt; ever.&lt;/p&gt;
&lt;p&gt;The financial system is not just &lt;a href="https://sloanreview.mit.edu/article/the-critical-difference-between-complex-and-complicated/" target="_blank"&gt;complicated, it is complex&lt;/a&gt;. Ultimately, the regulators understood that the complexity of the system means that no stone should be left unturned when saving it. The confusing part of this lesson is that the key regulators in the 2008 financial crisis now argue that some tools they used to save the financial system would &lt;a href="https://www.nytimes.com/2018/09/07/opinion/sunday/bernanke-lehman-anniversary-oped.html" target="_blank"&gt;no longer be available to them today&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;It seems clear that the system would have to change completely before the 2008 tools are taken away from regulators. However, the system has not been changed. Let’s compare the 2007 and 2018 market capitalization of the 10 largest US financial institutions according to &lt;a href="https://us.spindices.com/indices/equity/sp-500-financials-sector" target="_blank"&gt;S&amp;P 500 Financials&lt;/a&gt; sector index:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Dx18Ux6VgT-1yuDXRK-kKg.png.png"&gt;&lt;img src="/theme/images/1*Dx18Ux6VgT-1yuDXRK-kKg.png.png" alt="Market capitalization of the top 10 S&amp;P 500 Financials" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Admittedly, the table is only a very rough estimate of the impact of the largest financial institutions on the economy. Market cap does not directly relate to the impact on the economy but it’s a good rough estimate. Some companies were removed from the index so their impact diminished (Lehman’s market cap in 2007 reached $60bn but today it’s 0). General inflation also causes some of the growth. However, considering the significant growth of the value of these companies from 2007 to 2018, it’s impossible to argue that large financials are less important to the state of the economy today than they were in 2007.&lt;/p&gt;
&lt;h4&gt;Low Interest Rates &amp; Mortgages&lt;/h4&gt;
&lt;p&gt;Another logical cause of the financial crisis were low interest rates between 2002–2006. Ever since Paul Volcker, FED Chairman, used interest rates to &lt;a href="https://www.thebalance.com/who-is-paul-volcker-3306157" target="_blank"&gt;eliminate double digit inflation in early 1980s&lt;/a&gt;, interest rates have become the most favorite tool of central bankers to influence the economy. High interest rates lead to slow downs of economies and inflation, and low interest rates have the opposite effect. The consensus about the significant impact of central bankers is so strong that their &lt;a href="https://www.wsj.com/articles/central-bankers-jackson-hole-gathering-a-cheat-sheet-1535068873" target="_blank"&gt;annual gathering in Jackson Hole&lt;/a&gt; has become legendary.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*OFnvoEtJeI7TCwlOVntIWQ.png.png"&gt;&lt;img src="/theme/images/1*OFnvoEtJeI7TCwlOVntIWQ.png.png" alt="Effective Federal Funds Rate" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Low interest rates incentivize companies and individuals to borrow (for example in the form of mortgages) which stimulates economic growth. After 9/11, interest rates were lowered to around 1% to stimulate growth and resulted in the excessive, irresponsible borrowing &lt;a href="http://uk.businessinsider.com/central-banks-mistakes-low-interest-rates-hurting-the-global-economy-2015-9" target="_blank"&gt;many argue became one of the primary causes of the 2008 financial crisis&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Whether this argument is true is still open to debate. What is not open to debate is that if extremely low interest rate kept for too long are harmful, then we have not learned the lesson.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*g4kSFkFGtk7dcyCWBXIEsg.png.png"&gt;&lt;img src="/theme/images/1*g4kSFkFGtk7dcyCWBXIEsg.png.png" alt="30-Year Fixed Rate Mortgage Average in the United States" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Mortgage rates reflect general interest rates so rates between 2001–2006 were at historically lowest levels at the time. This led people to borrow too much for properties they couldn’t afford. And that is how the &lt;a href="https://en.wikipedia.org/wiki/Subprime_mortgage_crisis_solutions_debate" target="_blank"&gt;subprime mortgage crisis&lt;/a&gt; unfolded. However, mortgage rates have now been lower for over a decade compared to those supposedly causing the crisis. Financial institutions presumably increased lending standards to ensure that only people in good financial standings can borrow. However, even this obvious lesson is &lt;a href="https://www.marketwatch.com/story/ex-citimortgage-ceo-3-things-i-learned-from-the-2008-great-financial-crisis-2018-09-17" target="_blank"&gt;now being tested&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Accountability and Ethics&lt;/h4&gt;
&lt;p&gt;Fractional banking means that confidence of the population in the financial system is paramount. It’s not just paramount, it’s the only thing there is.&lt;/p&gt;
&lt;p&gt;The public perception is that there were no people made responsible for 2008 crisis. When the music stopped for&lt;a href="https://en.wikipedia.org/wiki/Enron_scandal" target="_blank"&gt; Enron&lt;/a&gt; or later &lt;a href="https://en.wikipedia.org/wiki/Madoff_investment_scandal" target="_blank"&gt;Madoff&lt;/a&gt;, people responsible landed jail sentences totaling hundreds of years. However, no such cases took place following the 2008 financial crisis even though many &lt;a href="https://www.standard.co.uk/business/german-bank-sues-goldman-sachs-over-toxic-mortgage-losses-6521591.html" target="_blank"&gt;lawsuits&lt;/a&gt; were filled.&lt;/p&gt;
&lt;p&gt;Moreover, education has not kept up either because some students are concerned that &lt;a href="https://www.nature.com/articles/d41586-018-06608-6" target="_blank"&gt;economics is being taught as if no financial crisis&lt;/a&gt; happened. Imagine an architect designing a bridge which subsequently falls down but students continue being taught to build the same bridges in schools.&lt;/p&gt;
&lt;p&gt;Criminal cases must be decided beyond any reasonable doubt so it is conceivable that the complexity of the financial system means that no people can be made directly responsible for it. However, not changing how we think about and teach economics leads us directly to Albert Einstein:&lt;/p&gt;
&lt;p&gt;The definition of insanity is doing the same thing over and over again, but expecting different results&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>Serverless web apps with Firebase and AWS Lambda</title><link href="https://adamnovotny.com/blog/serverless-web-apps-with-firebase-and-aws-lambda.html" rel="alternate"></link><published>2018-09-09T00:00:00-05:00</published><updated>2018-09-09T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-09-09:/blog/serverless-web-apps-with-firebase-and-aws-lambda.html</id><summary type="html">&lt;h4&gt;Topics: Firebase Hosting, AWS Lambda&lt;/h4&gt;
&lt;p&gt;Serverless has become a popular solution for small to medium-sized projects. The downside is a technology stack lock-in which forces developers to use technologies that might not be optimal for their projects. For example, people using &lt;a href="https://firebase.google.com/" target="_blank"&gt;Google’s Firebase&lt;/a&gt; to host their static resources have …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Topics: Firebase Hosting, AWS Lambda&lt;/h4&gt;
&lt;p&gt;Serverless has become a popular solution for small to medium-sized projects. The downside is a technology stack lock-in which forces developers to use technologies that might not be optimal for their projects. For example, people using &lt;a href="https://firebase.google.com/" target="_blank"&gt;Google’s Firebase&lt;/a&gt; to host their static resources have to write custom endpoint &lt;a href="https://firebase.google.com/docs/functions/" target="_blank"&gt;functions&lt;/a&gt; in JavaScript or TypeScript (as of August 2018). Developers typically use custom backend functions to hide business logic or proprietary data operations from users because anything that runs in the browsers front end as JavaScript is ultimately an open book from the users perspective.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*6QfJob8HhDsYsGjzYSb3eA.jpeg.png"&gt;&lt;img src="/theme/images/1*6QfJob8HhDsYsGjzYSb3eA.jpeg.png" alt="Firebase + AWS Lambda" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One simple solution is to combine Firebase with custom functions using a different platform. I will outline the steps to create a Firebase-hosted web app, setup DNS for subdomain, and create AWS Lambda functions to serve custom business logic as APIs. This is just an example setup and all major cloud players provide solutions that can be combined in other ways such as using &lt;a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html" target="_blank"&gt;AWS S3 to host statics resources&lt;/a&gt; and &lt;a href="https://cloud.google.com/functions/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=emea-emea-all-en-dr-bkws-all-all-trial-e-gcp-1003963&amp;utm_content=text-ad-none-any-DEV_c-CRE_253480695966-ADGP_Hybrid%20%7C%20AW%20SEM%20%7C%20BKWS%20~%20EXA_M:1_EMEA_EN_General_Cloud%20Functions_ETL%20Warehouse-KWID_43700019207153401-kwd-34012173938-userloc_1003837&amp;utm_term=KW_function%20google-ST_function%20google&amp;ds_rl=1245734&amp;gclid=EAIaIQobChMInM_0gaH43AIVpp3tCh1CTAHNEAAYASAAEgJ1iPD_BwE&amp;dclid=COGAr4Oh-NwCFQ6ZdwodUTIBBQ" target="_blank"&gt;Google’s Cloud Function&lt;/a&gt; to serve business logic API.&lt;/p&gt;
&lt;p&gt;At the end we will have:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;www.example.com (&lt;a href="https://en.wikipedia.org/wiki/Single-page_application" target="_blank"&gt;single page app&lt;/a&gt; served by Firebase)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;api.example.com (AWS lambda function serving custom business logic used by www.example.com)&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;I will not go into specific details on each platform because their UIs constantly change. Instead, I will highlight the sequence of steps I typically take to setup the services quickly.&lt;/p&gt;
&lt;h4&gt;Firebase hosting&lt;/h4&gt;
&lt;ul&gt;&lt;li&gt;1) Deploy a static web app to &lt;a href="https://firebase.google.com/products/hosting/" target="_blank"&gt;Firebase&lt;/a&gt; by following &lt;a href="https://firebase.google.com/docs/hosting/deploying" target="_blank"&gt;this part&lt;/a&gt; of the Firebase documentation. The end result will be a public web app. Its URL will look something like this: my-project-name.firebaseapp.com&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*gH7ELyB6td5o2A1bY4drGw.png.png"&gt;&lt;img src="/theme/images/1*gH7ELyB6td5o2A1bY4drGw.png.png" alt="Firebase hosting setup" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;2) Let’s assume we purchased the custom domain example.com. We now need to update the DNS records so that example.com and www.example.com point to our static web app.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;3) Go to your Firebase project dashboard and in the hosting section initiate the steps to connect to a custom domain. You will need to verify ownership of the domain by adding a DNS TXT record to your registrar’s DNS settings. As always, the &lt;a href="https://firebase.google.com/docs/hosting/custom-domain" target="_blank"&gt;documentation&lt;/a&gt; is useful.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Kd7lW2alk8UA4cL8kkvxxA.png.png"&gt;&lt;img src="/theme/images/1*Kd7lW2alk8UA4cL8kkvxxA.png.png" alt="Connect custom domain to Firebase app" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;4) Go to your domain registrar’s DNS settings, and create a DNS A record for subdomain www pointing to the IP address of the Firebase servers obtained in the previous step. After SSL certificates are automatically provisioned by Firebase, users can go to https://www.example.com to locate your Firebase app.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;5) We also need to make sure that users entering just example.com are also pointed to https://www.example.com. To accomplish this, return to your registrar’s DNS settings and a setup subdomain forwarding. The exact steps vary for each registrar but the end result will be example.com -&gt; https://www.example.com. If possible, set the redirect as permanent 301, forward path, and enable SSL.&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;AWS Lambda&lt;/h4&gt;
&lt;p&gt;At this point we have a web app deployed and using our custom URL. The app however uses the subdomain api.example.com to obtain proprietary data. In Angular, the code requesting data from the subdomain may look something like this:&lt;/p&gt;
&lt;pre&gt;const headers = {
    headers: new HttpHeaders({
      'Content-Type':  'application/json',
      'x-api-key': 'some-api-key'
    })
};
this.http.get('&lt;a href="https://api.examle.com/get-data'" target="_blank"&gt;https://api.example.com'&lt;/a&gt;, headers)
.subscribe((data: string) =&gt; {
    const dataJson = JSON.parse(data);
    // some data operations
    }
);&lt;/pre&gt;
&lt;p&gt;If our backend is relatively simple (doesn’t require large third party packages) and runs fast, the easiest solution is to deploy cloud functions at one of the largest providers. AWS limits Lambda deployments to 50MB and the default timeout is 3 seconds which are reasonable guidelines to determine whether your custom API backend is suitable for serverless functions.&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;6) We need to create a Lambda function. I like to test my lambda functions locally and then deploy them as zip files to AWS. For Python, follow &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-python-how-to-create-deployment-package.html" target="_blank"&gt;this&lt;/a&gt; tutorial. Lambda supports all major languages and similar tutorials exists for at least Node.js, C#, Go, Java.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*ktkhPraczTuDeRUkbkhUUA.png.png"&gt;&lt;img src="/theme/images/1*ktkhPraczTuDeRUkbkhUUA.png.png" alt="AWS Lambda zip upload" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;7) Next we need to make the function publicly available so we will use API Gateway to create a public endpoint. Make sure to check that API key is required and then go to Actions and Deploy API.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*HgGVxaoWfxJo9jM3WSs-3Q.png.png"&gt;&lt;img src="/theme/images/1*HgGVxaoWfxJo9jM3WSs-3Q.png.png" alt="API Gateway endpoint creation" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;8) Secure the endpoint with at least an API key which can be created in the API Gateway as well.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*UQQ7OzNmF9iNxeWxlIXYDQ.png.png"&gt;&lt;img src="/theme/images/1*UQQ7OzNmF9iNxeWxlIXYDQ.png.png" alt="API key generation example" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;9) Create a Usage Plan that will limit how often your API can be used. This will prevent your Lambda function from being overused. While AWS Lambda has a very generous free tier, security is paramount for peace of mind. A Usage Plan basically connects the API key (step 8) to the endpoint deployment (step 7). At this point, you should be able to use your Lambda function by going to a URL that looks something like this https://xyz1234567.execute-api.us-east-1.amazonaws.com/stage. Remember that an API key is required as a header so tools such as &lt;a href="https://www.getpostman.com/" target="_blank"&gt;Postman&lt;/a&gt; are useful to customize the API requests easily.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*xbehZsrAZpOs_zB8QDD6Wg.png.png"&gt;&lt;img src="/theme/images/1*xbehZsrAZpOs_zB8QDD6Wg.png.png" alt="Usage Plan example" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;10) Ultimately, we want to have a nice-looking URL such as api.example.com instead of the long random URL above. First, we need to create a certificate for our subdomain to so that our connection supports &lt;a href="https://en.wikipedia.org/wiki/Transport_Layer_Security" target="_blank"&gt;SSL&lt;/a&gt; (https). Go to Certificate Manager and follow the steps to create a certificate managed by AWS:&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*FBijvl_qHBaCx2ue46fSzg.png.png"&gt;&lt;img src="/theme/images/1*FBijvl_qHBaCx2ue46fSzg.png.png" alt="AWS Certificate Manager" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;11) Now that we have a certificate available, return to API Gateway and go to Custom Domain Names and create the API name (such as api.example.com) and select the ACM certificate created in previous step. Map it to your API deployment. This will generate a Target domain name of the form xyz1234567899.cloudfront.net.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*et1Xy6T2IAhI2O475rC4Kw.png.png"&gt;&lt;img src="/theme/images/1*et1Xy6T2IAhI2O475rC4Kw.png.png" alt="Custom Domain Name" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;12) Return to your domain registrar’s DNS records, and create a CNAME record pointing to the target domain name above (such as xyz1234567899.cloudfront.net). Now once DNS records propagate, requesting api.example.com is going to terminate at your Lambda function and will be accessible by your Firebase frontend.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;That’s it! Now you can deploy a fully featured web app with a custom backend, URL and generous free tiers (as of August 2018). With a little bit of practice the process takes about an hour subject to DNS propagation and requires virtually no backend deployment knowledge. It scales well for most small to medium-sized apps that do not require specialized compute-intensive workloads such as Machine Learning (see my ML deployment article &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category></entry><entry><title>Machine Learning Tutorial #4: Deployment</title><link href="https://adamnovotny.com/blog/machine-learning-tutorial-4-deployment.html" rel="alternate"></link><published>2018-09-02T00:00:00-05:00</published><updated>2018-09-02T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-09-02:/blog/machine-learning-tutorial-4-deployment.html</id><summary type="html">&lt;h4&gt;Topics: Stack Selection, Heroku, Testing&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*T_-rIQ8yUgPba_ezxt6ogg.png.png"&gt;&lt;img src="/theme/images/1*T_-rIQ8yUgPba_ezxt6ogg.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this final phase of the series, I will suggest a few options ML engineers have to deploy their code. In large organizations, this part of the project will be handled by a specialized team which is especially important when scaling is a concern. Other …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Topics: Stack Selection, Heroku, Testing&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*T_-rIQ8yUgPba_ezxt6ogg.png.png"&gt;&lt;img src="/theme/images/1*T_-rIQ8yUgPba_ezxt6ogg.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this final phase of the series, I will suggest a few options ML engineers have to deploy their code. In large organizations, this part of the project will be handled by a specialized team which is especially important when scaling is a concern. Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , #4 Deployment (this article). &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-deployment" target="_blank"&gt;Github code&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Stack Selection&lt;/h4&gt;
&lt;p&gt;The number of options to deploy ML code is numerous but I typically decide between at least the three general buckets:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Solution provided as-a-service (e.g. Microsoft Azure Machine Learning Studio)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Serverless function (e.g. &lt;a href="https://docs.aws.amazon.com/lambda/latest/dg/python-programming-model.html" target="_blank"&gt;AWS Lambda&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Custom backend code (e.g. &lt;a href="http://flask.pocoo.org/docs/0.12/" target="_blank"&gt;Python Flask&lt;/a&gt; served by &lt;a href="https://devcenter.heroku.com/articles/getting-started-with-python" target="_blank"&gt;Heroku&lt;/a&gt;)&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;As-a-service solution&lt;/h4&gt;
&lt;p&gt;Platforms such as Microsoft Azure Machine Learning Studio offer the full suite of tools for the entire project including preprocessing and training. Custom API endpoints are usually easy to generate and writing code is often not necessary thanks to drag-and-drop interfaces. The solutions are often well optimized for &lt;a href="https://en.wikipedia.org/wiki/Lazy_learning" target="_blank"&gt;lazy learners&lt;/a&gt; where evaluation is the most expensive computational step. The downside is that it is sometimes more challenging to bring in custom code (such as the final model) without going through all the project steps on the platform.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*4F3z9NovnqtOtIRWRCJn_Q.jpeg.png"&gt;&lt;img src="/theme/images/1*4F3z9NovnqtOtIRWRCJn_Q.jpeg.png" alt="As-a-service deployment example: Microsoft Azure" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Serverless function&lt;/h4&gt;
&lt;p&gt;Serverless functions are a good solution for inexpensive computations. AWS uses default timeout of 3 seconds for a function to complete. While timeouts can be extended, the default value is often a good general guideline when deciding about suitability. Lambda only allows 50MB of custom code to be uploaded which is generally not enough for most machine learning purposes. However, functions are well suited for fast computations such as linear regression models. Another downside is that platforms support only specific languages. In terms of Python solutions, AWS Lambda supports versions 2.7 and 3.6 only at the time of writing this article.&lt;/p&gt;
&lt;h4&gt;Custom backend code&lt;/h4&gt;
&lt;p&gt;Writing a custom backend code on platform such as Heroku or Amazon’s EC2 allows us to replicate fully the code we write on local machines. The code and server deployment can be fully customized for the type of ML algorithm we are deploying. The downside of such solutions is their operational complexity because we need to focus on many steps unrelated to ML such as security.&lt;/p&gt;
&lt;p&gt;I will deploy the code on &lt;a href="https://devcenter.heroku.com/articles/getting-started-with-python" target="_blank"&gt;Heroku&lt;/a&gt; which offers a free tier for testing purposes. The lightweight &lt;a href="http://flask.pocoo.org/" target="_blank"&gt;Flask framework&lt;/a&gt; will drive the backend. The primary reason for this choice is that it allows us to reuse essentially all the code written in previous tutorials for the backend. We can install Flask with Python 3.6 and all machine learning libraries we use previously side by side.&lt;/p&gt;
&lt;p&gt;The entire backend code to run the app is literally a few lines long with Flask:&lt;/p&gt;
&lt;pre&gt;import pickle
import pandas as pd
from flask import Flask, jsonify, request, make_response&lt;/pre&gt;
&lt;pre&gt;app = Flask(__name__)&lt;/pre&gt;
&lt;pre&gt;&lt;a href="http://twitter.com/app" target="_blank"&gt;@app&lt;/a&gt;.route('/forecast', methods=["POST"])
def forecast_post():
    """
    Args:
        request.data: json pandas dataframe
            example: {
                "columns": ["date", "open", "high", "low", "close",
                   "volume"],
                "index":[1, 0],
                "data": [
                   [1532390400000, 108, 108, 107, 107, 26316],
                   [1532476800000, 107, 111, 107, 110, 30702]]
            }
    """
    if request.data:
        df = pd.read_json(request.data, orient='split')
        X = preprocess(df)
        model = pickle.load(open("dtree_model.pkl", "rb"))
        y_pred = run_model(X, model)
        resp = make_response(jsonify({
           "y_pred": json.dumps(y_pred.tolist())
        }), 200)
        return resp
    else:
        return make_response(jsonify({"message": "no data"}), 400)&lt;/pre&gt;
&lt;ul&gt;&lt;li&gt;pd.read_json(…): reads data from &lt;a href="https://en.wikipedia.org/wiki/POST_(HTTP)" target="_blank"&gt;POST request&lt;/a&gt; which is a json object corresponding to price data formatted the same way as Yahoo finance prices (our original data source)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;preprocess(…): copy of our code from the &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing&lt;/a&gt; tutorial that manipulates raw price data into features. Importantly, the scaler used must be the exact same we used in Preprocessing so it has to be saved to pickle file first during Preprocessing and loaded from pickle now&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;run_model(…): loads and runs our saved final model from the &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;Training&lt;/a&gt; tutorial&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;make_response(…): returns forecasts&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;Heroku&lt;/h4&gt;
&lt;p&gt;Deploying our prediction code to Heroku will require that we collect at least two necessary pieces of our code from previous tutorials: the final model (saved as a pickle file) and the code from the &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing&lt;/a&gt; tutorial that transforms the original features we collected from the real world to features our model can handle.&lt;/p&gt;
&lt;p&gt;I will not go into details about how to deploy a Docker app on Heroku. There are plenty of good materials including Heroku’s documentation, which is excellent. All the necessary code to run and deploy the Docker app on Heroku is also in the &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-deployment" target="_blank"&gt;Github &lt;/a&gt;repo. There are a few key steps to remember:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Save Dockerfile as Dockerfile.web which is a container of all code necessary to run the app&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Deploy container using command &lt;a href="https://devcenter.heroku.com/articles/container-registry-and-runtime" target="_blank"&gt;heroku container:push&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Release container using command &lt;a href="https://devcenter.heroku.com/articles/container-registry-and-runtime" target="_blank"&gt;heroku container:release&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;At this point our code is deployed which we can test using &lt;a href="https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwjmut-U1JvdAhVKsqQKHaQUBg0QFjAAegQIBRAC&amp;url=https%3A%2F%2Fwww.getpostman.com%2F&amp;usg=AOvVaw1vWzpwzQOHi5ErKZnywLDR" target="_blank"&gt;Postman&lt;/a&gt; to make a manual forecast request:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*5kvKnVEez88tZ96uTtOqjg.png.png"&gt;&lt;img src="/theme/images/1*5kvKnVEez88tZ96uTtOqjg.png.png" alt="Postman sample request" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The date is represented by Unix timestamp. The first Body window consists of inputs we provide to the endpoint in the form of prices. The second window returns forecasts from the app.&lt;/p&gt;
&lt;h4&gt;Testing&lt;/h4&gt;
&lt;p&gt;To test the implementation, I will reuse the code from the Evaluation step. However, instead of making predictions locally using our sklearn model, I will use the Heroku app to predict the 691 samples from Evaluation as a batch. The goal is for our predictions we made on a local machine to perfectly match those made using our deployment stack.&lt;/p&gt;
&lt;p&gt;This step is critical to ensure that we can replicate our results remotely using a pre-trained model. The testing code is also available on &lt;a href="https://github.com/adam5ny/blogs/blob/master/ml-deployment/backend/tests/test_app.py" target="_blank"&gt;Github&lt;/a&gt;. We confirm that the performance of our Heroku app matches the performance generated locally in the Evaluation tutorial:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Oewaabcu926MZpC-zFFpHQ.png.png"&gt;&lt;img src="/theme/images/1*Oewaabcu926MZpC-zFFpHQ.png.png" alt="Tested deployment performance matches evaluation results" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To conclude, the project is intended to provide an overview of the kind of thinking a data science project entails. The code should not be used in production and is provided solely for illustrative purposes. As always, I welcome all constructive feedback (positive or negative) on &lt;a href="https://twitter.com/adam5ny" target="_blank"&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt;, #4 Deployment (this article). &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-deployment" target="_blank"&gt;Github code&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Machine Learning Tutorial #3: Evaluation</title><link href="https://adamnovotny.com/blog/machine-learning-tutorial-3-evaluation.html" rel="alternate"></link><published>2018-08-19T00:00:00-05:00</published><updated>2018-08-19T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-08-19:/blog/machine-learning-tutorial-3-evaluation.html</id><summary type="html">&lt;h4&gt;Topics: Performance Metrics, Commentary&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png"&gt;&lt;img src="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this third phase of the series, I will explore the Evaluation part of the ML project. I will reuse some of the code and solutions from the second Training phase. However, it is important to note that the Evaluation phase should be completely separate from …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Topics: Performance Metrics, Commentary&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png"&gt;&lt;img src="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this third phase of the series, I will explore the Evaluation part of the ML project. I will reuse some of the code and solutions from the second Training phase. However, it is important to note that the Evaluation phase should be completely separate from training except for using the final model produced in the Training step. Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, #3 Evaluation (this article), &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;. &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-evaluation" target="_blank"&gt;Github code&lt;/a&gt;.&lt;/p&gt;
&lt;h4&gt;Performace Metrics&lt;/h4&gt;
&lt;p&gt;The goal of this section is to determine how our model from the Training step performs on real life data it has not learned from. First, we have to load the model we saved as the Final model:&lt;/p&gt;
&lt;pre&gt;model = pickle.load(open("dtree_model.pkl", "rb"))
&gt;&gt;&gt; model
DecisionTreeRegressor(criterion='mse', max_depth=3, max_features=None, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=1, min_samples_split=5, min_weight_fraction_leaf=0.0, presort=False, random_state=1, splitter='best')&lt;/pre&gt;
&lt;p&gt;Next, we will load the testing data we created in the Preprocessing part of this tutorial. The primary reason why I keep the Evaluation section separate from Training is precisely this step. I keep the code separate as well to ensure that no information from training leaks into evaluation. To restate, we should have not seen the data used in this section at any point until now.&lt;/p&gt;
&lt;pre&gt;X = pd.read_csv("X_test.csv", header=0)
y = pd.read_csv("y_test.csv", header=0)&lt;/pre&gt;
&lt;p&gt;At this stage, we may perform additional performance evaluation on top of the Training step. However, I will stick to the metrics used previously: MAE, MSE, R2.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*5Cjov6KncJ3qLJ-fHi6MTg.png.png"&gt;&lt;img src="/theme/images/1*5Cjov6KncJ3qLJ-fHi6MTg.png.png" alt="Decision tree MAE, MSE, R2" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Commentary&lt;/h4&gt;
&lt;p&gt;We have known that our model does not perform well enough in practice from the previous tutorial already. However, as I mentioned before, I went ahead and used it for illustrative purposes here in order to complete the tutorial and to explain the kind of thinking involved in real life projects where performance is not always ideal out of the box as many toy datasets would make one think.&lt;/p&gt;
&lt;p&gt;The key comparison is how well does our model evaluate relative to the training phase. In the case of models ready for production, I would expect the performance in the Evaluation step to be comparable to those of testing folds in the Training phase.&lt;/p&gt;
&lt;p&gt;Comparing the last training test fold &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;here&lt;/a&gt; (5249 datapoints used to train) and the Evaluation results above:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;MAE: final Training phase ~10^-2. Evaluation phase ~10^-2&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;MSE: final Training phase ~10^-4. Evaluation phase ~10^-3&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;R²: final Training phase ~0. Evaluation phase ~0&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;The performance on dataset the model has never seen before is reasonably similar. Nonetheless, overfitting is still something to potentially address. If we had a model ready for production from the Training phase, we would be reasonably confident at this stage that it would perform as we expect on out of sample data.&lt;/p&gt;
&lt;p&gt;Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, #3 Evaluation (this article), &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Machine Learning Tutorial #2: Training</title><link href="https://adamnovotny.com/blog/machine-learning-tutorial-2-training.html" rel="alternate"></link><published>2018-08-12T00:00:00-05:00</published><updated>2018-08-12T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-08-12:/blog/machine-learning-tutorial-2-training.html</id><summary type="html">&lt;h4&gt;Topics: Performance Metrics, Cross Validation, Model Selection, Hyperparameter Optimization, Project Reflection, Tools&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png"&gt;&lt;img src="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This second part of the ML Tutorial follows up on the first &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing&lt;/a&gt; part. All code is available in this &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-training" target="_blank"&gt;Github repo&lt;/a&gt;. Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, #2 Training (this article), &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Topics: Performance Metrics, Cross Validation, Model Selection, Hyperparameter Optimization, Project Reflection, Tools&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png"&gt;&lt;img src="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This second part of the ML Tutorial follows up on the first &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing&lt;/a&gt; part. All code is available in this &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-training" target="_blank"&gt;Github repo&lt;/a&gt;. Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, #2 Training (this article), &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I concluded Tutorial #1 with 4 datasets: training features, testing features, training target variables, and testing target variables. Only training features and and training target variables will be used in this Tutorial #2. The testing data will be used for evaluation purposes in Tutorial #3.&lt;/p&gt;
&lt;h4&gt;Performance Metrics&lt;/h4&gt;
&lt;p&gt;We are focused on regression algorithms so I will consider 3 most often used performance metrics&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Mean_absolute_error" target="_blank"&gt;Mean Absolute Error&lt;/a&gt; (MAE)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Mean_squared_error" target="_blank"&gt;Mean Squared Error&lt;/a&gt; (MSE)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://en.wikipedia.org/wiki/Coefficient_of_determination" target="_blank"&gt;R²&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;In practice, a domain-specific decision could be made to supplement the standard metrics above. For example, investors are typically more concerned about significant downside errors rather than upside errors. As a result, a metric could be derived that overemphasizes downside errors corresponding to financial losses.&lt;/p&gt;
&lt;h4&gt;Cross Validation&lt;/h4&gt;
&lt;p&gt;I will return to the same topic I addressed in &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing&lt;/a&gt;. Due to the nature of time series data, standard randomized K-fold validation produces forward looking bias and should not be used. To illustrate the issue here, let’s assume that we split 8 years of data into 8 folds, each representing one year. The first training cycle will use folds #1–7 for training and fold #8 for testing. The next training cycle may use folds #2–8 for training and fold #1 for testing. This is of course unacceptable because we are using data from years 2–7 to forecast year 1.&lt;/p&gt;
&lt;p&gt;Our cross validation must respect the temporal sequence of the data. We can use Walk Forward Validation or simply multiple Train-Test Splits. For illustration, I will use 3 Train-Test splits. For example, let’s assume we have 2000 samples sorted by timestamp from the earliest. Our 3 segments would look as follows:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*cFti5rqcbFrE5p_4My4eww.png.png"&gt;&lt;img src="/theme/images/1*cFti5rqcbFrE5p_4My4eww.png.png" alt="Train-Test splits. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Model Selection&lt;/h4&gt;
&lt;p&gt;&lt;a href="/theme/images/1*M2clZWay68ODL2jEB-J5Dw.png.png"&gt;&lt;img src="/theme/images/1*M2clZWay68ODL2jEB-J5Dw.png.png" alt="ML Model Selection. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this section, I will select the models to train. The “Supervised” algorithms section (red section in the image above) is relevant because the dataset contains both features and labels (target variables). I like to follow &lt;a href="https://en.wikipedia.org/wiki/Occam%27s_razor" target="_blank"&gt;Occam’s razor&lt;/a&gt; when it comes to algorithms selection. In other words, start with the algorithm that exhibits the fastest times to train and the greatest interpretability. Then we can increase complexity.&lt;/p&gt;
&lt;p&gt;I will explore the following algorithms in this section:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Linear Regression: fast to learn, easy to interpret&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Decision Trees: fast to learn (requires pruning), easy to interpret&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Neural Networks: slow to learn, hard to interpret&lt;/li&gt;&lt;/ul&gt;
&lt;h4&gt;Linear Regression&lt;/h4&gt;
&lt;p&gt;Starting with linear regression is useful to see if we can “get away” with simple statistics to achieve our goal before diving into complex machine learning algorithms. House price forecasting with clearly defined features is an example where linear regression often works well and using more complex algorithms is unnecessary.&lt;/p&gt;
&lt;p&gt;Training a linear regression model using sklearn is simple:&lt;/p&gt;
&lt;pre&gt;from sklearn import linear_model
model = linear_model.LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)&lt;/pre&gt;
&lt;p&gt;Initial results yielded nothing remotely promising so I took another step and transformed features further. I created polynomial and nonlinear features to account for nonlinear relationships. For example, features [a, b] become [1, a, b, a², ab, b²] in the case of degree-2 polynomial.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*c9-D9EJoTwKsRlJuK_WQaQ.png.png"&gt;&lt;img src="/theme/images/1*c9-D9EJoTwKsRlJuK_WQaQ.png.png" alt="Linear Regression results. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The x-axis represents 3 cross validation segments (the fold 1st uses 1749 samples for training and 1749 for testing, the 2nd uses 3499 for training and 1749 for testing, and the last uses 5249 for training and 1749 for testing). Clearly, the results suggest that the linear model is not useful in practice. At this stage I have at least the following options:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Ridge regression: addresses overfitting (if any)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Lasso linear: reduces model complexity&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;At this point, I don’t believe that any of the options above will meaningfully impact the outcome. I will move on to other algorithms to see how they compare.&lt;/p&gt;
&lt;p&gt;Before moving on, however, I need to set expectations. There is a saying in finance that successful forecasters only need to be correct 51% of the time. Financial leverage can be used to magnify results so being just a little correct produces impactful outcomes. This sets expectations because we will never find algorithms that are constantly 60% correct or better in this domain. As a result, we expect low R² values. This needs to be said because many sample projects in machine learning are designed to look good, which we can never match in real-life price forecasting.&lt;/p&gt;
&lt;h4&gt;Decision Tree&lt;/h4&gt;
&lt;p&gt;Training a decision tree regressor model using sklearn is equally simple:&lt;/p&gt;
&lt;pre&gt;from sklearn import tree
model = tree.DecisionTreeRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)&lt;/pre&gt;
&lt;p&gt;The default results for the fit function above almost always &lt;a href="https://en.wikipedia.org/wiki/Overfitting" target="_blank"&gt;overfit&lt;/a&gt;. Decision trees have a very expressive hypothesis space so they can represent almost any function when not pruned. R² for training data can easily become perfect 1.0 while for testing data the result will be 0. We therefore need to use the max_depth argument of scikit-learn &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" target="_blank"&gt;DecisionTreeRegressor&lt;/a&gt; to enforce that the tree generalizes well for test data.&lt;/p&gt;
&lt;p&gt;One of the biggest advantages of decision trees is their interpretability: see many useful &lt;a href="https://medium.com/@rnbrown/creating-and-visualizing-decision-trees-with-python-f8e8fa394176" target="_blank"&gt;visualization articles&lt;/a&gt; using standard illustrative datasets.&lt;/p&gt;
&lt;h4&gt;Neural Networks&lt;/h4&gt;
&lt;p&gt;Scikit-learn makes &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor" target="_blank"&gt;simple neural network&lt;/a&gt; training just as simple as building a decision tree:&lt;/p&gt;
&lt;pre&gt;from sklearn.neural_network import MLPRegressor
model = MLPRegressor(hidden_layer_sizes=(200, 200), solver="lbfgs", activation="relu")
model.fit(X_train, y_train)
y_pred = model.predict(X_test)&lt;/pre&gt;
&lt;p&gt;Training a neural net with 2 hidden layers (of 200 units each) and polynomial features starts taking tens of seconds on an average laptop. To speed up the training process in the next section, I will step away from scikit-learn and use &lt;a href="https://keras.io/" target="_blank"&gt;Keras&lt;/a&gt; with TensorFlow backend.&lt;/p&gt;
&lt;p&gt;Keras API is equally simple. The project even includes &lt;a href="https://keras.io/scikit-learn-api/#wrappers-for-the-scikit-learn-api" target="_blank"&gt;wrappers for scikit-learn&lt;/a&gt; to take advantage of scikit’s research libraries.&lt;/p&gt;
&lt;pre&gt;from keras.models import Sequential
from keras.layers import Dense
model = Sequential()
input_size = len(X[0])
model.add(Dense(200, activation="relu", input_dim=input_size))
model.add(Dense(200, activation="relu"))
model.add(Dense(1, activation="linear"))
model.compile(optimizer="adam", loss="mse")
model.fit(X_train, y_train, epochs=25, verbose=1)
y_pred = model.predict(X_test)&lt;/pre&gt;
&lt;h4&gt;Hyperparameter Optimization&lt;/h4&gt;
&lt;p&gt;The trick to doing hyperparameter optimization is to understand that parameters should not be treated independently. Many parameters interact with each other which is why exhaustive &lt;a href="https://en.wikipedia.org/wiki/Hyperparameter_optimization#Grid_search" target="_blank"&gt;grid search&lt;/a&gt; is often performed. However, grid search is that it becomes expensive very quickly.&lt;/p&gt;
&lt;h4&gt;Decision Tree&lt;/h4&gt;
&lt;p&gt;Our decision tree grid search will iterate over the following inputs:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;splitter: strategy used to split nodes (best or random)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;max depth of the tree&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;min samples per split: the minimum number of samples required to split an internal node&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;max leaf nodes: number or None (allow unlimited number of leaf nodes)&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Illustrative grid search results are below:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*CQHDbOr3_ZO7oWkdOYdKQw.png.png"&gt;&lt;img src="/theme/images/1*CQHDbOr3_ZO7oWkdOYdKQw.png.png" alt="Grid Search Decision Tree — first rows" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;
&lt;a href="/theme/images/1*DQHZ9reWIOMF6fq9eKA8IQ.png.png"&gt;&lt;img src="/theme/images/1*DQHZ9reWIOMF6fq9eKA8IQ.png.png" alt="Grid Search Decision Tree — last rows" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Performance using the best parameters:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*2qHu4Z1DiJGmx440QCyTAA.png.png"&gt;&lt;img src="/theme/images/1*2qHu4Z1DiJGmx440QCyTAA.png.png" alt="Decision Tree results" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Again, the results do not seem to be very promising. They appear to be better than linear regression (lower MAE and MSE) but R² is still too low to be useful. I would conclude, however, that the greater expressiveness of decision trees is useful and I would discard the linear regression model at this stage.&lt;/p&gt;
&lt;h4&gt;Neural Networks&lt;/h4&gt;
&lt;p&gt;Exploring the hyperparameters of the neural net build by Keras, we can alter at least the following parameters:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;number of hidden layers and/or units in each layer&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;model &lt;a href="https://keras.io/optimizers/" target="_blank"&gt;optimizer&lt;/a&gt; (SGD, Adam, etc)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://keras.io/activations/" target="_blank"&gt;activation function&lt;/a&gt; in each layer (relu, tanh)&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;batch size: the number of samples per gradient update&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;epochs to train: the number of iterations over the entire training dataset&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Illustrative grid search results are below:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*c_fhFAu5NkphQXM6Do8QXQ.png.png"&gt;&lt;img src="/theme/images/1*c_fhFAu5NkphQXM6Do8QXQ.png.png" alt="Grid Search Neural Net — first rows" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;
&lt;a href="/theme/images/1*CZIYuZ9UrEdoVkWhOtfIWQ.png.png"&gt;&lt;img src="/theme/images/1*CZIYuZ9UrEdoVkWhOtfIWQ.png.png" alt="Grid Search Neural Net — last rows" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Using the best parameters, we obtain the following performance metrics:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*auWhs2uGbBher9adskreog.png.png"&gt;&lt;img src="/theme/images/1*auWhs2uGbBher9adskreog.png.png" alt="Keras MAE, MSE, R2" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Neural net and decision tree results are similar which is common. Both algorithms have very expressive hypothesis spaces and often produce comparable results. If I achieve comparable results, I tend to use the decision tree model for its faster training times and greater interpretability.&lt;/p&gt;
&lt;h4&gt;Project Reflection&lt;/h4&gt;
&lt;p&gt;At this stage it becomes clear that no model can be used in production. While the decision tree model appears to perform the best, its performance on testing data is still unreliable. At this stage, it would be time to go back and find additional features and/or data sources.&lt;/p&gt;
&lt;p&gt;As I mentioned in the first &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;Preprocessing Tutorial&lt;/a&gt;, finance practitioners might spend months sourcing data and building features. Domain-specific knowledge is crucial and I would argue that financial markets exhibit at least the &lt;a href="https://www.investopedia.com/exam-guide/cfa-level-1/securities-markets/weak-semistrong-strong-emh-efficient-market-hypothesis.asp" target="_blank"&gt;Weak-Form of Efficient Market Hypothesis&lt;/a&gt;. This implies that future stock returns cannot be predicted from past price movements. I have used only past price movements to develop the models above so practitioners would notice already in the first tutorial that results would not be promising.&lt;/p&gt;
&lt;p&gt;For the sake of completing this tutorial, I will go ahead and save the decision tree model and use it for illustrative purposes in the next sections of this tutorial (as if it were the Final production model):&lt;/p&gt;
&lt;pre&gt;pickle.dump(model, open("dtree_model.pkl", "wb"))&lt;/pre&gt;
&lt;p&gt;Important: there are &lt;a href="https://www.cs.uic.edu/~s/musings/pickle/" target="_blank"&gt;known security vulnerabilities&lt;/a&gt; in the Python pickle library. To stay on the safe side, the key takeaway is to never unpickle data you did not create.&lt;/p&gt;
&lt;h4&gt;Tools&lt;/h4&gt;
&lt;p&gt;Tooling is a common question but often not critical until the project is composed of tens of thousands of examples and at least hundreds of features. I typically start with scikit-learn and move elsewhere when performance becomes the bottleneck. &lt;a href="https://www.tensorflow.org/" target="_blank"&gt;TensorFlow&lt;/a&gt;, for example, is not just a deep learning framework but also contains other algorithms such as &lt;a href="https://www.tensorflow.org/api_docs/python/tf/estimator/LinearRegressor" target="_blank"&gt;LinearRegressor&lt;/a&gt;. We could train Linear Regression above with TensorFlow and GPUs if scikit-learn does not perform well enough.&lt;/p&gt;
&lt;p&gt;Other tutorials in this series: &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-1-preprocessing-d90198e37577" target="_blank"&gt;#1 Preprocessing&lt;/a&gt;, #2 Training (this article), &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Machine Learning Tutorial #1: Preprocessing</title><link href="https://adamnovotny.com/blog/machine-learning-tutorial-1-preprocessing.html" rel="alternate"></link><published>2018-08-05T00:00:00-05:00</published><updated>2018-08-05T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-08-05:/blog/machine-learning-tutorial-1-preprocessing.html</id><summary type="html">&lt;h4&gt;Topics: Data Cleaning, Target Variable Selection, Feature Extraction, Scaling, Dimensionality Reduction&lt;/h4&gt;
&lt;p&gt;In this machine learning tutorial, I will explore 4 steps that define a typical machine learning project: Preprocessing, Learning, Evaluation, and Prediction (deployment). In this first part, I will complete the Preprocessing step. Other tutorials in this series: #1 …&lt;/p&gt;</summary><content type="html">&lt;h4&gt;Topics: Data Cleaning, Target Variable Selection, Feature Extraction, Scaling, Dimensionality Reduction&lt;/h4&gt;
&lt;p&gt;In this machine learning tutorial, I will explore 4 steps that define a typical machine learning project: Preprocessing, Learning, Evaluation, and Prediction (deployment). In this first part, I will complete the Preprocessing step. Other tutorials in this series: #1 Preprocessing (this article), &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png"&gt;&lt;img src="/theme/images/1*iPgIcpnc-nzkigs6RaTZBw.png.png" alt="Machine Learning project overview. Author: Adam Novotny" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I will use stock price data as the main dataset. There are a few reasons why this is a good choice for the tutorial:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;The dataset is public by definition and can be easily downloaded from multiple sources so anyone can replicate the work.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Not all features are immediately available from the source and need to be extracted using domain knowledge, resembling real life.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;The outcome of the project is highly uncertain which again simulates real life. Billions of dollars are thrown at the stock price prediction problem every year and the vast majority of projects fail. This tutorial is therefore not about creating a magical money-printing machine; it is about replicating the experience a machine learning engineer might have with a project.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;All code is located at the following &lt;a href="https://github.com/adam5ny/blogs/tree/master/ml-preprocessing" target="_blank"&gt;Github repo&lt;/a&gt;. The file “preprocessing.py” drives the analysis. Python 3.6 is recommended and the file includes directions to setup all necessary dependencies.&lt;/p&gt;
&lt;p&gt;First we need to download the dataset. I will somewhat arbitrarily choose the Microsoft stock data (source: &lt;a href="https://finance.yahoo.com/quote/MSFT/history?p=MSFT" target="_blank"&gt;Yahoo Finance&lt;/a&gt;). I will use the entire available history which at the time of writing includes 3/13/1986 — 7/30/2018. The share price performed as follows during this period:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*lR8eaHKYLjtKsZY_J19pog.png.png"&gt;&lt;img src="/theme/images/1*lR8eaHKYLjtKsZY_J19pog.png.png" alt="MSFT stock price. Source https://finance.yahoo.com/chart/MSFT" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The price movement is interesting because it exhibits at least two modes of behavior:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;the steep rise until the year 2000 when tech stocks crashed&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;the sideways movement since 2000&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;This makes for a number of interesting machine learning complexities such as the sampling of training and testing data.&lt;/p&gt;
&lt;h4&gt;Data Cleaning&lt;/h4&gt;
&lt;p&gt;After some simple manipulations and loading of the csv data into pandas DataFrame, we have the following dataset where open, high, low and close represent prices on each date and volume the total number of shares traded.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*psQ_9EoBHpiN78QgreAVOQ.png.png"&gt;&lt;img src="/theme/images/1*psQ_9EoBHpiN78QgreAVOQ.png.png" alt="Raw dataset includes columns: date, prices (open, high, low, close), trading volume" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;
&lt;a href="/theme/images/1*pF4V5GC6b2vfC-koQkAynw.png.png"&gt;&lt;img src="/theme/images/1*pF4V5GC6b2vfC-koQkAynw.png.png" alt="Raw dataset includes columns: date, prices (open, high, low, close), trading volume" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Missing values are not present which I confirmed by running the following command:&lt;/p&gt;
&lt;pre&gt;missing_values_count = df.isnull().sum()&lt;/pre&gt;
&lt;p&gt;&lt;a href="/theme/images/1*RxQtAFDbviXDbYcxU8j02Q.png.png"&gt;&lt;img src="/theme/images/1*RxQtAFDbviXDbYcxU8j02Q.png.png" alt="No missing values in dataset" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Outliers are the next topic I need to address. The key point to understand here is that our dataset now includes prices but prices are not the metric I will attempt to forecast because they are measured in absolute terms and therefore harder to compare across time and other assets. In the tables above, the first price available is ~$0.07 while the last is $105.37.&lt;/p&gt;
&lt;p&gt;Instead, I will attempt to forecast daily returns. For example, at the end of the second trading day the return was +3.6% (0.073673/0.071132). I will therefore create a return column and use it to analyze possible outliers.&lt;/p&gt;
&lt;p&gt;The 5 smallest daily returns present in the dataset are the following:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*-FluO_dSIB7Gc8Rlvgry_w.png.png"&gt;&lt;img src="/theme/images/1*-FluO_dSIB7Gc8Rlvgry_w.png.png" alt="5 smallest daily returns" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And 5 largest daily returns:&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*CvJuQYGojLfLlxpnm9Ut1w.png.png"&gt;&lt;img src="/theme/images/1*CvJuQYGojLfLlxpnm9Ut1w.png.png" alt="5 largest daily returns" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The most negative return is -30% (index 405) and the largest is 20% (index 3692). Normally, a further domain-specific analysis of the outliers is necessary here. I will skip it for now and assume this tutorial outlines the process for illustrative purposes only. Generally, the data appears to make sense given that in 1987 and 2000 market crashes took place associated with extremely volatility.&lt;/p&gt;
&lt;p&gt;The same analysis would be required for open, high, low and volume columns. Admittedly, data cleaning was somewhat academic because Yahoo Finance is a very widely used and reliable source. It is still a useful exercise to understand the data.&lt;/p&gt;
&lt;h4&gt;Target Variable Selection&lt;/h4&gt;
&lt;p&gt;We need to define what our ML algorithms will attempt to forecast. Specifically, we will forecast next day’s return. The timing of returns is important here so we are not mistakenly forecasting today’s or yesterday’s return. The formula to define tomorrow’s return as our target variable is as follows:&lt;/p&gt;
&lt;pre&gt;df["y"] = df["return"].shift(-1)&lt;/pre&gt;
&lt;h4&gt;Feature Extraction&lt;/h4&gt;
&lt;p&gt;Now I will turn to some simple transformations of the prices, returns and volume to &lt;a href="https://en.wikipedia.org/wiki/Feature_extraction" target="_blank"&gt;extract features&lt;/a&gt; ML algorithms can consume. Finance practitioners have developed 100s of such features but I will only show a few. Hedge funds spent the vast majority of time on this step because ML algorithms are generally only as useful as the data available, aka. “garbage in, garbage out”.&lt;/p&gt;
&lt;p&gt;One feature we might consider is how today’s closing price relates to that of 5 trading days ago (one calendar week). I call this feature “5d_momentum”:&lt;/p&gt;
&lt;pre&gt;df[“5d_momentum”] = df[“close”] / df[“close”].shift(5)&lt;/pre&gt;
&lt;p&gt;&lt;a href="/theme/images/1*4dWC4F1sqjmpW5dohmF-Mg.png.png"&gt;&lt;img src="/theme/images/1*4dWC4F1sqjmpW5dohmF-Mg.png.png" alt="New 5d_momentum feature" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;One typical trend following feature is &lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:moving_average_convergence_divergence_macd" target="_blank"&gt;MACD&lt;/a&gt; (Moving Average Convergence/Divergence Oscillator). The strengths of pandas shine here because MACD can be created in only 4 lines of code. The chart of the MACD indicator is below. On the lower graph, a typical buy signal would be the blue “macd_line” crossing above the orange line representing a 9-day exponential moving average of the “macd_line”. The inverse would represent a sell signal.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/1*cA6MrDLu1Fuwd4pIDoS0fQ.png.png"&gt;&lt;img src="/theme/images/1*cA6MrDLu1Fuwd4pIDoS0fQ.png.png" alt="MACD of stock price" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The python code “generate_features.py” located in the Github repo mentioned above includes additional features we might consider. For example:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://www.investopedia.com/articles/active-trading/052014/how-use-moving-average-buy-stocks.asp" target="_blank"&gt;Trend: Moving Average&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*2ip_ErJJ73742mxoNoGknA.png.png"&gt;&lt;img src="/theme/images/1*2ip_ErJJ73742mxoNoGknA.png.png" alt="MSFT Moving Average 50 day — 200 day" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:parabolic_sar" target="_blank"&gt;Trend: Parabolic SAR&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*uRA6nhA4QpXoqhl6dfECrg.png.png"&gt;&lt;img src="/theme/images/1*uRA6nhA4QpXoqhl6dfECrg.png.png" alt="MSFT SAR" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:stochastic_oscillator_fast_slow_and_full" target="_blank"&gt;Momentum: Stochastic Oscillator&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/1*Qt0JrOJuvdUBelJ_ddGO1g.png.png"&gt;&lt;img src="/theme/images/1*Qt0JrOJuvdUBelJ_ddGO1g.png.png" alt="MSFT Stochastic Oscillator" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:commodity_channel_index_cci" target="_blank"&gt;Momentum: Commodity Channel Index (CCI)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:relative_strength_index_rsi" target="_blank"&gt;Momentum: Relative Strength Index (RSI)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:bollinger_bands" target="_blank"&gt;Volatility: Bollinger Bands&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:average_true_range_atr" target="_blank"&gt;Volatility: Average True Range&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:on_balance_volume_obv" target="_blank"&gt;Volume: On Balance Volume (OBV)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://stockcharts.com/school/doku.php?id=chart_school:technical_indicators:chaikin_oscillator" target="_blank"&gt;Volume: Chaikin Oscillator&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;At the end of the feature extraction process, we have the following features:&lt;/p&gt;
&lt;pre&gt;['return', 'close_to_open', 'close_to_high', 'close_to_low', 'macd_diff', 'ma_50_200', 'sar', 'stochastic_oscillator', 'cci', 'rsi', '5d_volatility', '21d_volatility', '60d_volatility', 'bollinger', 'atr', 'on_balance_volume', 'chaikin_oscillator']&lt;/pre&gt;
&lt;h4&gt;Sampling&lt;/h4&gt;
&lt;p&gt;We need to split the data into training and testing buckets. I cannot stress enough that the testing dataset should never be used in the Learning step. It will be used only in the Evaluation step so that performance metrics are completely independent of training and represent an unbiased estimate of actual performance.&lt;/p&gt;
&lt;p&gt;Normally, we could randomize the sampling of testing data but time series data is often not well suited for randomized sampling. The reason being that would would bias the learning process. For example, randomization could produce a situation where the data point from 1/1/2005 is used in the Learning step to later forecast a return from 1/1/2003.&lt;/p&gt;
&lt;p&gt;I will therefore choose a much simpler way to sample the data and use the first 7000 samples as training dataset for Learning and the remaining 962 as testing dataset for Evaluation.&lt;/p&gt;
&lt;p&gt;Both datasets will be saved as csv files so we conclude this part of the ML tutorial by storing 4 files (MSFT_X_learn.csv, MSFT_y_learn.csv, MSFT_X_test.csv, MSFT_y_test.csv). These will be consumed by the next steps of this tutorial.&lt;/p&gt;
&lt;h4&gt;Scaling&lt;/h4&gt;
&lt;p&gt;Feature scaling is used to reduce the time to Learn. This typically applies to &lt;a href="https://en.wikipedia.org/wiki/Feature_scaling#Application" target="_blank"&gt;stochastic gradient descent and SMV&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The open source&lt;a href="http://scikit-learn.org/stable/index.html" target="_blank"&gt; sklearn&lt;/a&gt; package will be used for most additional ML application so I will start using it here to &lt;a href="http://scikit-learn.org/stable/modules/preprocessing.html#standardization-or-mean-removal-and-variance-scaling" target="_blank"&gt;scale all features&lt;/a&gt; to have zero mean and unit variance:&lt;/p&gt;
&lt;pre&gt;from sklearn import preprocessing
scaler_model = preprocessing.StandardScaler().fit(X_train)
X_train_scaled = scaler_model.transform(X_train)
X_test_scaled = scaler_model.transform(X_test)&lt;/pre&gt;
&lt;p&gt;It is important that data sampling takes place before features are modified to avoid any training to testing data leakage.&lt;/p&gt;
&lt;h4&gt;Dimensionality Reduction&lt;/h4&gt;
&lt;p&gt;At this stage, our dataset 17 features. The number of features has a significant impact on the speed of learning. We could use a number of techniques to try to reduce the number of features so that only the most “useful” features remain.&lt;/p&gt;
&lt;p&gt;Many hedge funds would be working with 100s of features at this stage so dimensional reduction would be critical. In our case, we only have 17 illustrative features so I will keep them all in the dataset until I explore the learning times of different algorithms.&lt;/p&gt;
&lt;p&gt;Out of curiosity however, I will perform Principal Component Analysis &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html" target="_blank"&gt;(PCA) &lt;/a&gt;to get an idea of how many features we could create from our dataset without losing meaningful explanatory power.&lt;/p&gt;
&lt;pre&gt;from sklearn.decomposition import PCA
sk_model = PCA(n_components=10)
sk_model.fit_transform(features_ndarray)
print(sk_model.explained_variance_ratio_.cumsum())
 [0.30661571 0.48477408 0.61031358 0.71853895 0.78043556 0.83205298
 0.8764804  0.91533986 0.94022672 0.96216244]&lt;/pre&gt;
&lt;p&gt;The first 8 features explain 91.5% of data variance. The downside of PCA is that new features are located in a lower dimensional space so they no longer correspond the real-life concepts. For example, the first original feature could be “macd_line” I derived above. After PCA, the first feature explains 31% of variance but we not longer have any logical description for what the feature represents in real life.&lt;/p&gt;
&lt;p&gt;For now, I will keep all features 17 original features but note that if the learning time of algorithms is too slow, PCA will be helpful.&lt;/p&gt;
&lt;p&gt;Other tutorials in this series: #1 Preprocessing (this article), &lt;a href="https://medium.com/coinmonks/machine-learning-tutorial-2-training-f6f735830838" target="_blank"&gt;#2 Training&lt;/a&gt;, &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-3-evaluation-a157f90914c9" target="_blank"&gt;#3 Evaluation&lt;/a&gt; , &lt;a href="https://medium.com/@adam5ny/machine-learning-tutorial-4-deployment-79764123e9e1" target="_blank"&gt;#4 Prediction&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Machine Learning"></category></entry><entry><title>Limited Competition in Sports</title><link href="https://adamnovotny.com/blog/limited-competition-in-sports.html" rel="alternate"></link><published>2018-07-15T00:00:00-05:00</published><updated>2018-07-15T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-07-15:/blog/limited-competition-in-sports.html</id><summary type="html">&lt;p&gt;Freakonomics podcast titled &lt;a href="http://freakonomics.com/podcast/leicester-city/" target="_blank"&gt;The Longest Long Shot&lt;/a&gt; made me think about the benefits of competition. The core of the podcast discussed the unbelievable title run by the Leicester City Football Club in the English Premier League in 2016. According to bookmakers who set the team’s odds to 5000–1 …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Freakonomics podcast titled &lt;a href="http://freakonomics.com/podcast/leicester-city/" target="_blank"&gt;The Longest Long Shot&lt;/a&gt; made me think about the benefits of competition. The core of the podcast discussed the unbelievable title run by the Leicester City Football Club in the English Premier League in 2016. According to bookmakers who set the team’s odds to 5000–1 at the beginning of the season, it was the greatest underdog story in sports history. However, I came away thinking about the difference between “full competition” sports leagues such as the Premier League and what I call “limited competition” leagues such as the NBA.&lt;/p&gt;
&lt;p&gt;Full competition leagues such as the Premier League have 3 main characteristics:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Struggling teams get relegated to lower leagues&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;No draft picks are given; players are bought and sold in open market&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;No salary cap is imposed allowing teams to spend almost unlimited amounts&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/0*kUuFLf4l1vTfAs79.png"&gt;&lt;img src="/theme/images/0*kUuFLf4l1vTfAs79.png" alt="“Night view of crowded Volksparkstadion soccer stadium before the match at night” by Mario Klassen on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Limited competition leagues typically have the opposite characteristics:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Guaranteed spot in the league next season regardless of performance&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Draft picks to help poorly performing teams get the best youngest players for free&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Salary caps helping teams from smaller markets to compete&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;&lt;a href="/theme/images/0*zVrqAokLjb0ZE1__.png"&gt;&lt;img src="/theme/images/0*zVrqAokLjb0ZE1__.png" alt="Photo by JC Gellidon on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;An excellent example of the behavior induced by limited competition is the long-term &lt;a href="https://www.forbes.com/sites/leighsteinberg/2018/04/13/was-tanking-the-key-to-philadelphia-76ers-success/#122b47876d0a" target="_blank"&gt;tanking of NBA’s Philadelphia 76ers&lt;/a&gt;. Between 2012 and 2015 the team was managed to lose games to get valuable draft picks. Higher draft picks are given to struggling teams in the NBA. The league is effectively trying to raise the quality of the bottom teams while preventing the top teams from improving further.&lt;/p&gt;
&lt;p&gt;What’s worse, &lt;a href="http://www.espn.com/nba/story/_/id/23435394/philadelphia-76ers-future-bright-was-process-worth-nba" target="_blank"&gt;tanking seems to be working for the 76ers&lt;/a&gt;, which is likely to encourage other teams to follow the same strategy. Some fans will therefore watch terrible games of their home teams for years.&lt;/p&gt;
&lt;p&gt;Full competition in the Premier League encourages different behavior. In the season prior to winning the title, Leicester City was almost relegated and had to win 7 out of the 8 remaining games just to stay in the Premier League. I would argue that this improved the teams spirits and built a base for next year’s title success. In a limited competition league, the team would likely be losing games on purpose waiting for draft picks.&lt;/p&gt;
&lt;p&gt;The interesting thing at the moment is that both models of competition seem to work. The popularity of full competition leagues represented by European soccer is increasing, as is that of limited competition leagues in the US such as NFL or NBA.&lt;/p&gt;
&lt;p&gt;However, the rules likely financially benefit different stakeholders. The list of the &lt;a href="https://en.wikipedia.org/wiki/Forbes%27_list_of_the_most_valuable_sports_teams" target="_blank"&gt;most valuable sports franchises in 2017&lt;/a&gt; is dominated by US limited competition teams. Only Manchester United, FC Barcelona, Real Madrid, and Bayern Munich representing full competition leagues crack the top 30. The value of teams from full competition leagues is lower due to no safety nets and the risk of relegation.&lt;/p&gt;
&lt;p&gt;The limited competition rules likely cause a wealth transfer from the athletes to the owners. If US leagues changed their rules to full competition, some of the team owners’ wealth would likely land in the pockets of the athletes due to their better negotiating position.&lt;/p&gt;
&lt;p&gt;The best examples are the recent transfers of the best athletes in basketball and soccer. The total cost of LeBron James from the perspective of the Los Angeles Lakers is &lt;a href="http://www.espn.com/nba/story/_/id/24052129/official-lakers-announce-lebron-james-signed" target="_blank"&gt;$153M&lt;/a&gt; over the next 4 seasons (or $38M per season). The cost of Christiano Ronaldo from the perspective of Juventus is &lt;a href="https://www.theguardian.com/football/2018/jul/10/cristiano-ronaldo-juventus-real-madrid" target="_blank"&gt;$400M&lt;/a&gt; (or $100 per season). To be fair, taxes and transfers fees are included so not all the money lands in Ronaldo’s pockets. However, Messi and Ronaldo from full competition leagues were among the &lt;a href="https://en.wikipedia.org/wiki/Forbes%27_list_of_the_world%27s_highest-paid_athletes" target="_blank"&gt;top 3 highest paid athletes in 2016 and 2017&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>Inflation Is Back</title><link href="https://adamnovotny.com/blog/inflation-is-back.html" rel="alternate"></link><published>2018-06-23T00:00:00-05:00</published><updated>2018-06-23T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-06-23:/blog/inflation-is-back.html</id><summary type="html">&lt;p&gt;Most economic variables of western economies are reaching record highs in at least a decade. However, rising inflation makes information blurry or even meaningless.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*SixrIdH-CSSIWyKh.png"&gt;&lt;img src="/theme/images/0*SixrIdH-CSSIWyKh.png" alt="“Colorful lights in bokeh against a black background” by Alice Wu on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At least since the &lt;a href="https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008" target="_blank"&gt;2008 financial crisis&lt;/a&gt;, the top line performance of economies and companies had been assumed to be real because inflation had been close …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Most economic variables of western economies are reaching record highs in at least a decade. However, rising inflation makes information blurry or even meaningless.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*SixrIdH-CSSIWyKh.png"&gt;&lt;img src="/theme/images/0*SixrIdH-CSSIWyKh.png" alt="“Colorful lights in bokeh against a black background” by Alice Wu on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At least since the &lt;a href="https://en.wikipedia.org/wiki/Financial_crisis_of_2007%E2%80%932008" target="_blank"&gt;2008 financial crisis&lt;/a&gt;, the top line performance of economies and companies had been assumed to be real because inflation had been close to zero. Real was true both in &lt;a href="https://en.wikipedia.org/wiki/Real_gross_domestic_product" target="_blank"&gt;terms of economics&lt;/a&gt; and colloquially. People could depend on the figures because inflation was not blurring the results.&lt;/p&gt;
&lt;p&gt;However, the tide is changing based on individual accounts of businesses even though government statistics are slow to reflect it. This makes all government and private figures difficult to digest and the markets will need to adjust.&lt;/p&gt;
&lt;p&gt;As an example let’s take a world without inflation. If people are told that they are receiving a 10% wage raise, they have a reason to be happy and celebrate. However, in a world where inflation is a problem but its value is uncertain, a 10% wage rise is meaningless. A 5% inflation still implies people’s financial situation improves after the wage rise and they should celebrate, but a 20% inflation means people are worse off and the wage rise just lessens the pain. We just don’t know which inflationary world we are in.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*rW-82t_ucnT8St4U.png"&gt;&lt;img src="/theme/images/0*rW-82t_ucnT8St4U.png" alt="Photo by sergio souza on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Non-aggregate individual accounts of people and companies are painting an inflationary picture.&lt;/p&gt;
&lt;p&gt;Anything close to 3 percent is likely dangerous territory for central banks. I would argue that given the evidence below, government data have plenty of headroom to climb in order to reflect the experiences of private businesses.&lt;/p&gt;
&lt;p&gt;Data pointing to low or target inflationary environment (yoy):&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=481797&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;“Policy makers expect inflation to run near its symmetric target of 2 percent” (FOMC meeting, USA, 6/13/2018)&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=487946&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Consumer Price Index (Italy, May 2018)&lt;/a&gt;: 1.0%&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=490920&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Consumer Price Index (Japan, May 2018)&lt;/a&gt;: 0.7%&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Data pointing to high or unexpected inflationary environment (yoy):&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=486997&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Producer Price Index (UK, May 2018)&lt;/a&gt;: 2.9%&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=486996&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Consumer Price Index (UK, May 2018)&lt;/a&gt;: 2.8%&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=485740&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Producer Price Index (US, &lt;/a&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=486997&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;May&lt;/a&gt; &lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=485740&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;2018)&lt;/a&gt;: 3.1%&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=485931&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Existing Home Sales (US, May 2018)&lt;/a&gt;: 4.9%&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byshoweventfull.asp?fid=487535&amp;cust=wsj-global&amp;year=2018&amp;lid=0&amp;prev=/byweek.asp#top" target="_blank"&gt;Consumer Price Index (Canada, May 2018)&lt;/a&gt;: 2.2%&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Earnings commentary with data pointing to high or unexpected inflationary environment:&lt;/p&gt;
&lt;p&gt;&lt;q&gt;Casey General Store (CASY): The average retail price of fuel during this period increased over 13%.&lt;/q&gt;
&lt;q&gt;Darden Restaurants (DRI): Restaurant labor was 130 basis points unfavorable last year due to several factors…we continue to see elevated wage inflation of approximately 4%.&lt;/q&gt;
&lt;q&gt;Kirby (KEX): In the inland marine transportation business, we saw a positive change in market dynamics during the quarter. Spot market pricing increased approximately 10% to 15%.&lt;/q&gt;
&lt;q&gt;Patterson UTI (PTEN): Average rig operating costs per day were also higher-than-expected at $13,970, due primarily to higher-than-expected labor cost.&lt;/q&gt;
&lt;q&gt;Chipotle (CMG): The price increases averaged about 5% across the menu. Labor costs for the quarter were 27.8%, 90 basis points higher than last year. Wage inflation of 5%.&lt;/q&gt;
&lt;q&gt;Union Pacific (UNP): 5% improvement in average revenue per car drove a 7% increase in freight revenue.&lt;/q&gt;
&lt;q&gt;Domino’s Pizza (DPZ): … on food basket inflation, we’re still anchoring to the 2% to 4% for 2018.&lt;/q&gt;
&lt;q&gt;CH Robinson (CHRW): First, we are in a unprecedented freight environment. The healthy economy and rapid growth in e-commerce is driving a significant increase in the demand for freight. To illustrate this point, costs in our North America truckload business increased 21.5% this quarter, the largest single quarterly increase in our 21-year history as publicly traded company.&lt;/q&gt;
&lt;q&gt;KB Home (KBH): 7% increase in our overall average selling price.&lt;/q&gt;
&lt;q&gt;AO Smith (AOS): As a result of significantly higher steel prices and inflation in freight and other costs, we announced a price increase up to 12% on U.S. water heater products effective in early June.&lt;/q&gt;
&lt;q&gt;H.B. Fuller (FUL): We have three areas of focus as we enter the second quarter: first will be to realize over $50 million in annualized pricing to offset last year’s raw material inflation&lt;/q&gt;
&lt;q&gt;PotlatchDeltic (PCH): In general, I’d say costs are going up in trucking about 10% per year.&lt;/q&gt;
&lt;q&gt;Pulte Group (PHM): Our revenue growth for the period reflects the combination of a 10% or $38,000 increase in average sales price of $413,000.&lt;/q&gt;
&lt;q&gt;Lowe’s (LOW): We recently announced plans to expand our employee benefits and a one-time bonus of up to $1,000 for our more than 260,000 hourly employees in the U.S&lt;/q&gt;
&lt;q&gt;World Fuel Service (INT): Consolidated revenue is up 12% compared to the first quarter of 2017. This increase was principally due to a 22% year-over-year increase in oil prices.&lt;/q&gt;
&lt;q&gt;Brinker (EAT): … ongoing market-driven wage rate pressures which continued in the 3% to 4% range&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;The only data pointing to normal, expected inflation are based on lagging government data or structurally CPI-challenged countries such as Japan and Italy. Slower inflation data are perhaps calming the market now so its response remains relatively subdued. As soon as private business data starts leaking into government numbers significantly, I would expect a meaningful market response similar to that in February.&lt;/p&gt;
&lt;p&gt;Credits&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="http://wsj-global.econoday.com/byweek.asp" target="_blank"&gt;WSJ Economic Calendar&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Eric Cinnamond: &lt;a href="http://www.ericcinnamond.com/inflation-subsiding-or-accelerating/" target="_blank"&gt;Inflation — Subsiding or Accelerating?&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>50 Percent of 2017 ICOs Have Failed Already</title><link href="https://adamnovotny.com/blog/50-percent-of-2017-icos-have-failed-already.html" rel="alternate"></link><published>2018-06-10T00:00:00-05:00</published><updated>2018-06-10T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-06-10:/blog/50-percent-of-2017-icos-have-failed-already.html</id><summary type="html">&lt;p&gt;In my &lt;a href="https://medium.com/@adamnovo/why-most-icos-are-going-to-0-and-its-not-a-big-deal-88486b2d0988" target="_blank"&gt;previous article&lt;/a&gt; about ICOs I suggested that most ICOs would go nowhere. Fortune has since &lt;a href="http://fortune.com/2018/02/25/cryptocurrency-ico-collapse/" target="_blank"&gt;reported&lt;/a&gt; that a half of 2017 ICOs are dead already.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*5Id4QitfO4uRaPKP.png"&gt;&lt;img src="/theme/images/0*5Id4QitfO4uRaPKP.png" alt="Photo by Estée Janssens on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It comes as no surprise given that many projects were attempted money grabs with white papers written over a weekend. Proven scams should …&lt;/p&gt;</summary><content type="html">&lt;p&gt;In my &lt;a href="https://medium.com/@adamnovo/why-most-icos-are-going-to-0-and-its-not-a-big-deal-88486b2d0988" target="_blank"&gt;previous article&lt;/a&gt; about ICOs I suggested that most ICOs would go nowhere. Fortune has since &lt;a href="http://fortune.com/2018/02/25/cryptocurrency-ico-collapse/" target="_blank"&gt;reported&lt;/a&gt; that a half of 2017 ICOs are dead already.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*5Id4QitfO4uRaPKP.png"&gt;&lt;img src="/theme/images/0*5Id4QitfO4uRaPKP.png" alt="Photo by Estée Janssens on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;It comes as no surprise given that many projects were attempted money grabs with white papers written over a weekend. Proven scams should be punished by authorities after the SEC clarifies which ICOs are securities.&lt;/p&gt;
&lt;p&gt;&lt;q&gt;There are [no ICOs] that I’ve seen that aren’t securities. &lt;a href="https://finance.yahoo.com/news/sec-ico-tokens-regulated-securities-205650102.html?guccounter=1" target="_blank"&gt;SEC Chairman Clayton, April 26&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;The SEC even set up a &lt;a href="https://www.bloomberg.com/news/articles/2018-05-16/sec-tries-to-scam-ico-investors-to-show-them-how-easy-it-is" target="_blank"&gt;false ICO site&lt;/a&gt; asking investors for funds to prove how easy it is to scam investors.&lt;/p&gt;
&lt;p&gt;However, many ICO founders are just unrealistic about their ideas. Entrepreneurship requires a certain amount of unrealistic thinking to start a business and ignore all the risks and unknown unknowns. As long as they do not step over any securities bright lines, they should be supported and treated just like other entrepreneurs hopeful that their business succeeds.&lt;/p&gt;
&lt;p&gt;We need to distinguish between ICO founders being delusional and fraudulent.&lt;/p&gt;
&lt;p&gt;Not everything needs to have a blockchain. Most things don’t. Some projects will be transformative but building everything on top of a decentralized network makes no sense. Blockchain for streaming movies? Probably not necessary, Netflix seems to work just fine. But most ICO are likely not fraudulent either. They might just be a small part of the &lt;a href="https://www.fundera.com/blog/what-percentage-of-small-businesses-fail" target="_blank"&gt;70% of small businesses that fail within 10 years&lt;/a&gt;.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>I Finally Read Facebook’s Terms of Service</title><link href="https://adamnovotny.com/blog/i-finally-read-facebooks-terms-of-service.html" rel="alternate"></link><published>2018-06-03T00:00:00-05:00</published><updated>2018-06-03T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2018-06-03:/blog/i-finally-read-facebooks-terms-of-service.html</id><summary type="html">&lt;p&gt;Just like many people using free online services, I used to ignore many Terms of Service notices and just scrolled to the bottom to confirm them. I believed that the best mindset is just acknowledging the Terms and behaving defensively as if free online services had no Terms protecting their …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Just like many people using free online services, I used to ignore many Terms of Service notices and just scrolled to the bottom to confirm them. I believed that the best mindset is just acknowledging the Terms and behaving defensively as if free online services had no Terms protecting their users. In other words, view them as postcards that anyone can read.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*nKjH5jX987I6yGQj.png"&gt;&lt;img src="/theme/images/0*nKjH5jX987I6yGQj.png" alt="Photo of postcards by Annie Spratt on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;However, the recent &lt;a href="https://en.wikipedia.org/wiki/Facebook%E2%80%93Cambridge_Analytica_data_scandal" target="_blank"&gt;Facebook and Cambridge Analytica scandal&lt;/a&gt; made me think about what services like Facebook actually promise. The scandal also serves as a reminder that some uses of private data are hard to imagine until you see them. I never considered how a British firm could impact US elections by using Facebook data available to developers, a lot of online surveys, and machine learning.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*-1uj_gI3Jf10eFR3.png"&gt;&lt;img src="/theme/images/0*-1uj_gI3Jf10eFR3.png" alt="“A large number of shipping containers” by chuttersnap on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Just like free shipping, free online services are not really free. The money necessary to offer them must to come from somewhere.&lt;/p&gt;
&lt;p&gt;Now, I don’t think that there is anything wrong with people willingly exchanging their personal data for services they enjoy. &lt;a href="http://www.cbc.ca/news/canada/nova-scotia/intact-insurance-mobile-phone-tracking-app-driving-habits-1.4247082" target="_blank"&gt;Insurance companies offer tracking devices to discount some drivers&lt;/a&gt;, for example. If people value their privacy less that an endless scrolling of Facebook’s timeline, they should be free to make that exchange. As long as such an exchange is made with full knowledge of the facts. And that’s where Terms of Services come in.&lt;/p&gt;
&lt;p&gt;Below I will refer to Facebook’s Terms of Service as of June 2, 2018.&lt;/p&gt;
&lt;p&gt;&lt;q&gt;We use the data we have — for example, about the connections you make, the choices and settings you select, and what you share and do on and off our Products — to personalize your experience. &lt;a href="https://www.facebook.com/terms.php" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;I will ignore large funnel statements such as the one above because they not very informative. They could be theoretically used to encompass and justify every use of personal data. I will also ignore statements that should be obvious to users such as we collect information about how you use our Products.&lt;/p&gt;
&lt;p&gt;&lt;q&gt;We also have developed, and continue to explore, new ways for people to use technology, such as augmented reality and 360 video to create and share more expressive and engaging content on Facebook. &lt;a href="https://www.facebook.com/terms.php" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;This statement nicely couches all new technology as if it were a birthday gift to its users. However, Facebook probably already has all the &lt;a href="https://www.theverge.com/2018/3/27/17165150/facebook-face-recognition-how-to-turn-off-disable" target="_blank"&gt;face recognition technology&lt;/a&gt; it needs to create user graphs across posts, pictures, and videos even if users do not actively tag them. Such technology has some good uses such eradicating hate or adult content.&lt;/p&gt;
&lt;p&gt;However, it could also be used for dangerous purposes such as the building of profiles even of people who do not have Facebook accounts. Google search reveals pictures and names of people. If a complete stranger posts a picture of somebody who doesn’t even have a Facebook profile (let’s call him John Smith), Facebook could pattern match the picture with other public data to build a profile. It can learn that John Smith attended a concert in New York on May 5, 2015 from a picture with tens of faces posted by someone else. All of this without John Smith’s knowledge.&lt;/p&gt;
&lt;p&gt;&lt;a href="/theme/images/0*5Vgl_myk6hUfRjzQ.png"&gt;&lt;img src="/theme/images/0*5Vgl_myk6hUfRjzQ.png" alt="“A black sign with white text on a wall that reads "Please respect our neighbors' privacy".” by Kai Brame on Unsplash" style="width: 100%" loading="lazy"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;To be fair, government agencies are likely already building such profiles, but those should be governed by strict privacy laws. The Economist claims that the size of the &lt;a href="https://www.economist.com/technology-quarterly/2018-05-02/justice" target="_blank"&gt;surveillance tech market in China is now $120bn&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;However, it appears that the only thing preventing Facebook from building such dangerous graphs of people’s lives is integrity.&lt;/p&gt;
&lt;p&gt;&lt;q&gt;You should know that, for technical reasons, content you delete may persist for a limited period of time in backup copies (though it will not be visible to other users). In addition, content you delete may continue to appear if you have shared it with others and they have not deleted it. &lt;a href="https://www.facebook.com/terms.php" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;This is a more problematic statement and it will be interesting to see how &lt;a href="https://en.wikipedia.org/wiki/General_Data_Protection_Regulation" target="_blank"&gt;GDPR&lt;/a&gt; impacts it. One of the goals of GDPR is to ensure that users have unconditional ability to delete their content. However, Facebook will correctly argue that due to the nature of content being shared around the globe and cached in computers for better user experience, it may be technically difficult to guarantee deletion. &lt;a href="https://en.wikipedia.org/wiki/Content_delivery_network" target="_blank"&gt;CDN&lt;/a&gt;s, caching, and other forms of partial data retention in many locations near users are fundamental to the user experience internet services are able to provide today.&lt;/p&gt;
&lt;p&gt;One of the first GDPR-related statements is already visible in Facebook’s Data Policy:&lt;/p&gt;
&lt;p&gt;&lt;q&gt;You can choose to provide information in your Facebook profile fields or Life Events about your religious views, political views, who you are “interested in,” or your health. This and other information (such as racial or ethnic origin, philosophical beliefs or trade union membership) is subject to special protections under EU law. &lt;a href="https://www.facebook.com/about/privacy/update" target="_blank"&gt;Source, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;Deleting content is so fundamental to user privacy that some compromise guaranteeing when user data is deleted entirely should to reached.&lt;/p&gt;
&lt;p&gt;&lt;q&gt;We collect the content, communications and other information you provide when you use our Products … [including] what you see through features we provide, such as our camera. &lt;a href="https://www.facebook.com/about/privacy/update" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;It is unclear how user privacy is affected when using Facebook’s camera and other internal services compared to the standard camera software on smartphones. The statement could mean that Facebook performs greater data analyses on pictures taken by its in-app camera.&lt;/p&gt;
&lt;p&gt;The Data Policy statement is a treasure of information. Just about devices Facebook collects:&lt;/p&gt;
&lt;p&gt;&lt;q&gt;Device attributes: information such as the operating system, hardware and software versions, battery level, …&lt;/q&gt;
&lt;q&gt;Device operations: information about operations and behaviors performed on the device, such as whether a window is foregrounded or backgrounded …&lt;/q&gt;
&lt;q&gt;Identifiers: unique identifiers, device IDs, and other identifiers, such as from games, apps or accounts you use, and Family Device IDs …&lt;/q&gt;
&lt;q&gt;Device signals: Bluetooth signals, and information about nearby Wi-Fi access points, beacons, and cell towers …&lt;/q&gt;
&lt;q&gt;Data from device settings: information you allow us to receive through device settings you turn on, such as access to your GPS location, camera or photos.&lt;/q&gt;
&lt;q&gt;Network and connections: information such as the name of your mobile operator or ISP, language, time zone, mobile phone number, IP address, connection speed and, in some cases, information about other devices that are nearby or on your network …&lt;/q&gt;
&lt;q&gt;Cookie data: data from cookies stored on your device, including cookie IDs and settings … &lt;a href="https://www.facebook.com/about/privacy/update" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;I intentionally included most of the details above to highlight how overwhelming the data collection is. It is unclear to me why all this information is necessary to run a social network if user privacy is one of the core values of Facebook as Mark Zuckerberg claims:&lt;/p&gt;
&lt;p&gt;&lt;q&gt;Our goal is to make it so that people can share with exactly the people they want to — &lt;a href="https://www.cnbc.com/video/2018/05/16/mark-zuckerberg-privacy-facebook-2010.html" target="_blank"&gt;Mark Zuckerberg, 2010&lt;/a&gt;&lt;/q&gt;
&lt;q&gt;Advertisers, app developers, and publishers can send us information … These partners provide information about your activities off Facebook — including information about your device, websites you visit, purchases you make, the ads you see, and how you use their services — whether or not you have a Facebook account or are logged into Facebook. &lt;a href="https://www.facebook.com/about/privacy/update" target="_blank"&gt;Facebook, June 2, 2018&lt;/a&gt;&lt;/q&gt;&lt;/p&gt;
&lt;p&gt;Facebook collects data from other sites as well, that should be obvious to users. Especially so if people use the ubiquitous Facebook login feature to sign into other sites. The key here is the phrase whether or not you have a Facebook account or are logged into Facebook. So Facebook at least attempts to track everybody online with of without a Facebook account. Again, it is unclear to me how this is necessary to run a social network that values peoples privacy.&lt;/p&gt;
&lt;p&gt;I see at least two outcomes that can pressure Facebook to alter its privacy behavior: regulators and lower usage if its products. I am skeptical on both fronts. The &lt;a href="https://www.theguardian.com/commentisfree/2018/apr/12/mark-zuckerberg-facebook-congressional-hearing-information-warfare-normal" target="_blank"&gt;Facebook hearings&lt;/a&gt; did not produce much more than a respectable performance by Mark Zuckerberg using the opportunity to market his company in the face of &lt;a href="https://www.inc.com/minda-zetlin/mark-zuckerberg-congress-hearings-funny-stupid-questions.html" target="_blank"&gt;sometimes clueless US lawmakers&lt;/a&gt;. Regarding the second point, Facebook users somehow &lt;a href="http://www.businessinsider.com/people-increased-facebook-usage-after-cambridge-analytica-scandal-2018-5" target="_blank"&gt;increased the usage of the product after the Cambridge Analytica&lt;/a&gt; scandal.&lt;/p&gt;
&lt;p&gt;If Facebook wants keep its users’ trust (which it appears not have lost so far considering the post-Cabridge Analytica usage), it should offer a paid service. This will have two benefits:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Paid accounts will offer a completely ad-free network with no user privacy leakage or unecessary internet tracking.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Free accounts will be clearly distinguished for what they are not: a charity. Free accounts exist for Facebook to run a profit-maximizing business. Users will knowingly exchange their private information for a service they enjoy. As long as both parties are aware of the consequences, they should be free to do so.&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Author website: &lt;a href="https://www.adamnovotny.com/" target="_blank"&gt;adamnovotny.com&lt;/a&gt;&lt;/p&gt;</content><category term="Random"></category></entry><entry><title>Why most ICOs are going to 0, and it’s not a big deal</title><link href="https://adamnovotny.com/blog/why-most-icos-are-going-to-0-and-its-not-a-big-deal.html" rel="alternate"></link><published>2017-09-03T00:00:00-05:00</published><updated>2017-09-03T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2017-09-03:/blog/why-most-icos-are-going-to-0-and-its-not-a-big-deal.html</id><summary type="html">&lt;p&gt;as long as people understand what they are buying.&lt;/p&gt;
&lt;p&gt;Raising money using cryptocurrency issuance has become the new gold rush. Tezos has raised $200M+ and Bancor $100M+. There are hundreds of other completed raises and even more on the horizon.&lt;/p&gt;
&lt;p&gt;The most important thing to realize is that these are …&lt;/p&gt;</summary><content type="html">&lt;p&gt;as long as people understand what they are buying.&lt;/p&gt;
&lt;p&gt;Raising money using cryptocurrency issuance has become the new gold rush. Tezos has raised $200M+ and Bancor $100M+. There are hundreds of other completed raises and even more on the horizon.&lt;/p&gt;
&lt;p&gt;The most important thing to realize is that these are startups. And most startups fail. ICOs may be even riskier than your typical startup because they often can’t clear the conventional financing bar with angels and VCs.&lt;/p&gt;
&lt;p&gt;But even if I give ICOs the benefit of the doubt and call them conventional startup investments, most conventional startups fail. Angels and VCs know that most (say 90%) of early stage startups return nothing at all or nothing meaningful. And that’s not a problem as long as 10% of their investments are high flyers (the Amazons, Facebooks, and Googles of the world).&lt;/p&gt;
&lt;p&gt;The conventional startup market operates efficiently because investors understand the risk profile of their investments. Bad companies fail without affecting the rest so the winners can shine.&lt;/p&gt;
&lt;p&gt;The danger with ICOs is that most buyers don’t realize what they are buying and as soon as 90% of investments fail, they will panic which may wipe out the 10% of potential successes as lawsuits and regulators join the party.&lt;/p&gt;
&lt;p&gt;Companies raising money using ICOs should go above and beyond explaining to buyers the expected risk / reward profile of these investments.&lt;/p&gt;</content><category term="Finance"></category></entry><entry><title>Linear programming in Python: CVXOPT and game theory</title><link href="https://adamnovotny.com/blog/linear-programming-in-python-cvxopt-and-game-theory.html" rel="alternate"></link><published>2017-08-16T00:00:00-05:00</published><updated>2017-08-16T00:00:00-05:00</updated><author><name>Adam Novotny</name></author><id>tag:adamnovotny.com,2017-08-16:/blog/linear-programming-in-python-cvxopt-and-game-theory.html</id><summary type="html">&lt;p&gt;CVXOPT is an excellent Python package for linear programming. However, when I was getting started with it, I spent way too much time getting it to work with simple game theory example problems. This tutorial aims to shorten the startup time for everyone trying to use CVXOPT for more advanced …&lt;/p&gt;</summary><content type="html">&lt;p&gt;CVXOPT is an excellent Python package for linear programming. However, when I was getting started with it, I spent way too much time getting it to work with simple game theory example problems. This tutorial aims to shorten the startup time for everyone trying to use CVXOPT for more advanced problems.&lt;/p&gt;
&lt;p&gt;All code is available &lt;a href="http://github.com/adam5ny/blogs/tree/master/cvxopt" target="_blank"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Installation of dependencies:&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;Using Docker is the fastest way to run the code. In only 5 commands you can replicate my environment and run the code.&lt;/li&gt;&lt;/ul&gt;
&lt;ul&gt;&lt;li&gt;Alternatively, the code has the following dependencies: Python (3.5.3), numpy (1.12.1), cvxopt (1.1.9), glpk optimizer (but you can use the default optimizer, glpk is better for some more advanced problems)&lt;/li&gt;&lt;/ul&gt;
&lt;p&gt;Please review &lt;a href="http://cvxopt.org/examples/tutorial/lp.html" target="_blank"&gt;how CVXOPT solves simple maximization problems&lt;/a&gt;. While this article focuses on game theory problems, it is critical to understand how CVXOPT defines optimization problems in general.&lt;/p&gt;
&lt;p&gt;The first problem we will solve is a &lt;a href="http://en.wikipedia.org/wiki/Minimax#Example" target="_blank"&gt;2-player zero-sum game&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The constraints matrix A is defined as&lt;/p&gt;
&lt;pre&gt;A = [[3, -2, 2], [-1, 0, 4] ,[-4, -3, 1]]&lt;/pre&gt;
&lt;p&gt;Next, we define a maxmin helper function&lt;/p&gt;
&lt;pre&gt;def maxmin(self, A, solver="glpk"):
    num_vars = len(A)
     minimize matrix c
    c = [-1] + [0 for i in range(num_vars)]
    c = np.array(c, dtype="float")
    c = matrix(c)
     constraints G*x &lt;= h
    G = np.matrix(A, dtype="float").T  reformat each variable is in a row
    G *= -1  minimization constraint
    G = np.vstack([G, np.eye(num_vars) * -1])  &gt; 0 constraint for all vars
    new_col = [1 for i in range(num_vars)] + [0 for i in range(num_vars)]
    G = np.insert(G, 0, new_col, axis=1)  insert utility column
    G = matrix(G)
    h = ([0 for i in range(num_vars)] + 
         [0 for i in range(num_vars)])
    h = np.array(h, dtype="float")
    h = matrix(h)
     contraints Ax = b
    A = [0] + [1 for i in range(num_vars)]
    A = np.matrix(A, dtype="float")
    A = matrix(A)
    b = np.matrix(1, dtype="float")
    b = matrix(b)
    sol = solvers.lp(c=c, G=G, h=h, A=A, b=b, solver=solver)
    return sol&lt;/pre&gt;
&lt;p&gt;Last, we use the maxmin helper function to solve our example problem:&lt;/p&gt;
&lt;pre&gt;sol = maxmin(A=A, solver=”glpk”)
probs = sol[“x”]
print(probs)
 [ 1.67e-01]
 [ 8.33e-01]
 [ 0.00e+00]&lt;/pre&gt;
&lt;p&gt;In other words, player A chooses action 1 with probility 1/6 and action 2 with probability 5/6.&lt;/p&gt;
&lt;p&gt;Next we will solve a Correlated Equilibrium problem called Game of Chicken as defined on page 3 of &lt;a href="http://www.cs.rutgers.edu/~mlittman/topics/nips02/nips02/greenwald.ps" target="_blank"&gt;this document&lt;/a&gt;. The constraints matrix A is defined as&lt;/p&gt;
&lt;pre&gt;A = [[6, 6], [2, 7], [7, 2], [0, 0]]&lt;/pre&gt;
&lt;p&gt;Next, we define a ce and build_ce_constraints helper functions:&lt;/p&gt;
&lt;pre&gt;def ce(self, A, solver=None):
    num_vars = len(A)
     maximize matrix c
    c = [sum(i) for i in A]  sum of payoffs for both players
    c = np.array(c, dtype="float")
    c = matrix(c)
    c *= -1  cvxopt minimizes so *-1 to maximize
     constraints G*x &lt;= h
    G = self.build_ce_constraints(A=A)
    G = np.vstack([G, np.eye(num_vars) * -1])  &gt; 0 constraint for all vars
    h_size = len(G)
    G = matrix(G)
    h = [0 for i in range(h_size)]
    h = np.array(h, dtype="float")
    h = matrix(h)
     contraints Ax = b
    A = [1 for i in range(num_vars)]
    A = np.matrix(A, dtype="float")
    A = matrix(A)
    b = np.matrix(1, dtype="float")
    b = matrix(b)
    sol = solvers.lp(c=c, G=G, h=h, A=A, b=b, solver=solver)
    return sol&lt;/pre&gt;
&lt;pre&gt;def build_ce_constraints(self, A):
    num_vars = int(len(A) ** (1/2))
    G = []
     row player
    for i in range(num_vars):  action row i
        for j in range(num_vars):  action row j
            if i != j:
                constraints = [0 for i in A]
                base_idx = i * num_vars
                comp_idx = j * num_vars
                for k in range(num_vars):
                    constraints[base_idx+k] = (- A[base_idx+k][0]
                                               + A[comp_idx+k][0])
                G += [constraints]
     col player
    for i in range(num_vars):  action column i
        for j in range(num_vars):  action column j
            if i != j:
                constraints = [0 for i in A]
                for k in range(num_vars):
                    constraints[i + (k * num_vars)] = (
                        - A[i + (k * num_vars)][1]
                        + A[j + (k * num_vars)][1])
                G += [constraints]
    return np.matrix(G, dtype="float")&lt;/pre&gt;
&lt;p&gt;Using the helper functions, we solve the Game of Chicken&lt;/p&gt;
&lt;pre&gt;sol = ce(A=A, solver="glpk")
probs = sol["x"]
print(probs)
 [ 5.00e-01]
 [ 2.50e-01]
 [ 2.50e-01]
 [ 0.00e+00]&lt;/pre&gt;
&lt;p&gt;In other words, the optimal strategy is for both players to select actions [6, 6] 50% of the time, actions [2, 7] 25% of the time, and action [7, 2] also 25% of the time.&lt;/p&gt;
&lt;p&gt;Hopefully this overview helps in getting you started with linear programming and game theory in Python.&lt;/p&gt;
&lt;p&gt;Credits: &lt;a href="http://cvxopt.org/examples/tutorial/lp.html" target="_blank"&gt;cvxopt.org/examples/tutorial/lp.html&lt;/a&gt;&lt;a href="https://www.cs.duke.edu/courses/fall12/cps270/lpandgames.pdf" target="_blank"&gt;, cs.duke.edu/courses/fall12/cps270/lpandgames.pdf&lt;/a&gt;&lt;a href="https://en.wikipedia.org/wiki/Minimax#Example" target="_blank"&gt;, en.wikipedia.org/wiki/Minimax#Example&lt;/a&gt;&lt;a href="https://www3.ul.ie/ramsey/Lectures/Operations_Research_2/gametheory4.pdf" target="_blank"&gt;, https://www3.ul.ie/ramsey/Lectures/Operations_Research_2/gametheory4.pdf&lt;/a&gt;&lt;a href="https://www.cs.rutgers.edu/~mlittman/topics/nips02/nips02/greenwald.ps" target="_blank"&gt;, cs.rutgers.edu/~mlittman/topics/nips02/nips02/greenwald.ps&lt;/a&gt;&lt;a href="https://www.cs.duke.edu/courses/fall16/compsci570/LPandGames.pdf" target="_blank"&gt;, cs.duke.edu/courses/fall16/compsci570/LPandGames.pdf&lt;/a&gt;&lt;/p&gt;</content><category term="Programming"></category></entry></feed>